{"title": "Proteomics Analysis of FLT3-ITD Mutation in Acute Myeloid Leukemia Using  Deep Learning Neural Network", "tag": "q-bio", "abstract": " Deep Learning can significantly benefit cancer proteomics and genomics. In this study, we attempt to determine a set of critical proteins that are associated with the FLT3-ITD mutation in newly-diagnosed acute myeloid leukemia patients. A Deep Learning network consisting of autoencoders forming a hierarchical model from which high-level features are extracted without labeled training data. Dimensional reduction reduced the number of critical proteins from 231 to 20. Deep Learning found an excellent correlation between FLT3-ITD mutation with the levels of these 20 critical proteins (accuracy 97%, sensitivity 90%, specificity 100%). Our Deep Learning network could hone in on 20 proteins with the strongest association with FLT3-ITD. The results of this study allow a novel approach to determine critical protein pathways in the FLT3-ITD mutation, and provide proof-of-concept for an accurate approach to model big data in cancer proteomics and genomics. ", "text": "deep learning significantly benefit cancer proteomics genomics. study attempt determine critical proteins associated flt-itd mutation newly-diagnosed acute myeloid leukemia patients. deep learning network consisting autoencoders forming hierarchical model high-level features extracted without labeled training data. dimensional reduction reduced number critical proteins deep learning found excellent correlation flt-itd mutation levels critical proteins deep learning network could hone proteins strongest association flt-itd. results study allow novel approach determine critical protein pathways flt-itd mutation provide proof-of-concept accurate approach model data cancer proteomics genomics. acute myeloid leukemia neoplasm bone marrow caused mutations myeloid stem cells leading formation aberrant myeloblasts. highly proliferative cancer cells impede formation normal blood cells eventually causing death patients left untreated. cases deaths disease quarter patients diagnosed survive beyond years urgent need find better treatments type leukemia. includes many subtypes share common clinical presentation despite arising diverse mutations genetic events. variety technologies targeting gene mrna microrna protein level helped andy n.d. nguyen department pathology laboratory medicine university texas health science center houston fannin street houston nghia.d.nguyenuth.tmc.edu gene mutations prognosis patients quite varied. possible explanation diversity differences protein signaling. genetic aberrations mutations myeloid leukemic cells often cause profound impact cellular protein networks. proteomics include vast collection techniques allowing analysis proteins cellular level. therefore proteomics could ideal tool predicting responses well monitoring targeted therapy. much work remains determine critical proteins involved particular mutation pathogenesis elucidated. like tyrosine kinase gene mutation. protein member class receptor-tyrosine kinase family encoded gene located chromosome and. shares high degree structural homology pdgfr receptors. plays critical role normal growth differentiation precursor cells bone marrow. upon binding ligand receptor dimerizes plasma membrane leading autophosphorylation activation several downstream effector signaling cascades. cascades include ras/mek pik/akt/mtor stat- pathways important cell cycle progression inhibition apoptosis activation differentiation. mutant expressed higher levels demonstrates ligand-independence causing constitutive autophosphorylation activation downstream signaling. common type mutations internal tandem duplication mutation. activates signal transduction pathways juxta-membranous region typically activated ligand-stimulated wild-type mutation occurs patients de-novo aml. patients flt-itd potential achieve initial complete remission wild-type higher relapse rate poorer response salvage therapy. presence inhibitors shown improve overall survival. platform collaborative community studies focus developing computational tools solve biomedical problems. dream challenges crowd-source non-profit studies supported contributors universities computer technology companies like research non-profit organizations like sage bionetworks biotechnology pharmaceutical companies. dream sage bionetworks offer open data access participants wish solve complex problems. since first dream challenge participants presented numerous findings leading biomedical journals. dream challenges leverage wisdom crowd develop innovative computational models making methods available public. insight gained challenge stored synapse site shared research community. dream challenges offer wonderful source various types cancer scientific research. dream challenge hosted rice university houston texas provides unique source data patients utilized study. important role clinical applications. protein expression profiles samples cancer patients compared normal samples allowing studying disease pathology. machine learning classification techniques used classify tissue samples mutated type versus normal type. however high dimensions protein expression data availability relatively small number samples given mutation analysis presents significant challenges process data. first challenge reduce number proteins ensures sufficient information perform accurate classification time eliminates superfluous information several solutions made available address high dimension problem perform feature space reduction constructing features either manually supervised ways. feature space reduction however leads methods typically scalable. second challenge involves small sample sets making problem difficult solve increasing risk over-fitting. propose paper deep learning methods based unsupervised feature extraction address challenges described above. successful deep learning methods involve artificial neural networks family models inspired biological neural networks artificial neural network artificial nodes connected together form network mimicking biological neural network. warren mcculloch walter pitts created computational model neural networks based algorithm called threshold logic neural networks shown superior performance compared machine learning methods introduction deep learning core concept deep learning involves learning hierarchical structure data initially extracting simple low-level features progressively used build complex features capturing underlying features data. simple example demonstrated facial recognition task pixel image represented input layer. input data compressed hidden layer features large small nose. words input data face described using learned features less information given original image. compressed data used represent input data output layer allowing facial image reconstructed entirely learned features. stacked autoencoders form deep network capable achieving unsupervised learning type machine-learning algorithm draws inferences input data labeled training examples. contrast previous methods conventional neural network data must strictly categorized provide appropriate label supervised learning unlabeled data deep learning used unsupervised training phase. resulting features training sets used basis construction classifier. study attempt deep learning incorporates unsupervised feature training find correlation flt-itd mutation levels critical proteins. best knowledge unsupervised feature learning methods materials data study obtained dream challenge includes patients’ demographics cytogenetics selected gene mutation status proteomic data patients diagnosed aml. data de-identified exclude personal information prior release dream challenge site. patients newly-diagnosed treated blood sample drawing. proteomics data include serum level proteins obtained reverse phase protein array method. testing procedure described details elsewhere illustrated fig. exclude factors confound analysis study included patients normal cytogenetics flt-itd sole mutation. restrictions number cases study reduced main analysis method deep learning neural network stacked auto-encoder. training mostly based unsupervised feature learning used successfully image audio recognition. deep learning neural network designed language. programming language statistical computing graphics supported foundation statistical computing. derived language originally developed bell notes cells standardized concentration cells microliter samples came either blood bone marrow. statistical difference determined protein expression blood bone marrow recent years advances machine learning. source code software environment written primarily java fortran also itself. freely available general public license pre-compiled binary versions provided various operating systems including unix windows macos. study many deep learning functions obtained various packages available comprehensive archive network. compare performance deep learning neural network conventional neural network also include study conventional neural network supervised training back-propagation sigmoid activation function. learning strategy conventional neural network starts randomly initializing weights network followed supervised backpropagation gradient descent. method shown find suboptimal solutions networks multiple hidden layers. suggested randomly initialized weights gradient-based training supervised neural networks stuck local minima plateaus difficult find solution layers. stacked autoencoder neural network illustrated fig. incorporates training phases pretraining unsupervised learning method fine-tuning similar supervised back-propagation conventional neural network. pre-training phase output layer subsequently used input next output layer. output layer essence represents approximation input data constructed limited number features represented hidden units legends step input training attributes feed forward step actual output compared desired output step connecting weights adjusted minimize error sigmoid function used activation function hidden layers. fine-tuning phase back-propagation method minimizes error additional sparsity penalty. features learned pretraining phase subsequently used labeled data specific mutation status train classifier. classifier defined function receives values various features training examples provides output predicts category training example belongs fine-tuning phase used linear function classifier. networks original training including proteins compared networks terms accuracy predicting flt-itd mutation status crossvalidation sets. high dimensionality protein expression data likely introduce background noise addition relevant proteins training set. addressed dimensionality problem protein expression data reducing dimensionality feature space relevant number proteins based ranking proteins training. ranking protein based absolute weights connections input node nodes first hidden layer. performance neural networks terms accuracy predicting mutation status using scaled-down protein compared. cross-validation method used obtain comprehensive validation results small number samples. validation method small subset data excluded training; resultant trained network used predict mutation status case excluded subset. process repeated cases data validated. overall accuracy neural network mean validated subsets. initial full attribute proteins yields accuracy conventional network. deep learning network performs better accuracy. using proteins ranked initial trial conventional network achieves better accuracy best accuracy obtained deep learning network proteins remarkable accuracy corresponds sensitivity specificity predicting positive flt-itd status case using level proteins. using smaller larger number proteins yield better accuracy indicating optimal number proteins study. appears fewer proteins contain insufficient data prediction. conversely proteins would introduce background noise compromising accuracy. scaling number proteins training significantly reduces number data points analysis accuracy predicting fltitd status different protein data sets conventional neural networks deep learning networks summarized table compression original features training illustrated fig. original features show spread-out pattern whereas extracted features compact indicating higher level representation. neural networks achieve optimal accuracy following important observations conventional neural performs best hidden layer fact well known type neural network relies strictly supervised learning multiple hidden layers present difficulty training often leading convergence training reason hidden layer conventional neural network study. despite limitation validation subsets still show convergence proteins. deep learning network performs well hidden layers consisting nodes respectively -protein set. however suboptimal results obtained -protein set. reason hidden layers contain nodes respectively proteins achieve better performance. parameters model parameters. tuning often requires experience sometimes brute-force search. parameters optimal configurations used neural networks obtained trial error follows conventional neural network learning rate momentum deep learning network learning rate momentum legends -column position protein dataset -input name name protein -importance absolute weights connections input node nodes first hidden layer. deep learning algorithms innovative tools research machine learning extract complex data representations high levels abstraction. fact deep learning cited breakthrough technologies technology review. important contribution deep learning algorithms develop hierarchical architecture data higher-level features defined terms lower-level features. hierarchical learning architecture deep learning algorithms motivated biological structure primary sensorial areas neocortex human brain automatically extracts abstract features underlying data-. deep learning algorithms rely large amounts unsupervised data typically learn data representations greedy layer-wise fashion. studies shown data representations obtained stacking nonlinear feature extractors often yield better machine classification results-. including speech recognition- computer vision natural language processing-. recent challenge hosted international symposium biomedical imaging lead successful deep learning system automated detection metastatic cancer whole slide images sentinel lymph nodes. data-intensive technologies proteomics genomics well improved computational data storage resources contributed data science technology-based companies microsoft google yahoo amazon maintained databases measured exabyte proportions larger. various private public organizations invested data analytics address needs business research making exciting area data science research. present study used deep learning proteomics analysis acute myeloid leukemia specifically determined critical proteins associated flt-itd mutation proteins available newly-diagnosed patients. implemented deep learning network consisting autoencoders stacked form hierarchical deep models high-level features compressed organized extracted without labeled training data. dimensional reduction initially performed reduce number critical proteins showed deep learning incorporates unsupervised feature training used find excellent correlation positive flt-itd mutation status levels proteins study also showed deep learning network outperforms conventional neural network task note objective determine critical proteins detect flt-itd mutation since existing testing technology polymerase chain reaction much better purpose. instead goal determine proteins involved flt-itd mutation. results study yield critical dataset proteins flt-itd mutation potential research determine important protein pathways mutation explore pathogenesis involving mutation monitor chemotherapy response design personalized treatment. best knowledge deep learning unsupervised feature learning methods applied protein expression analysis aml. amount data used relatively modest study provides proof-of-concept using deep learning neural network accurate approach modeling data cancer genomics proteomics. sanz burnett lo-coco lowenberg inhibition targeted therapy acute myeloid leukemia. curr opin oncol verstovsek therapeutic potential inhibitors. hematology hematol educ program dream challenge https//www.synapse.org/synapsesyn/wiki/ synapse site http//www.synapse.org/ c.c. gilbert ensemble machine learning gene expression data cancer classification. applied bioinformatics issn mcculloch warren; walter pitts. logical calculus ideas immanent nervous activity. bulletin mathematical biophysics doi. steven kornblau raoul tibes wenjing chen hagop kantarjian michael andreeff kevin coombes gordon mills. functional proteomic largman pham a.y. unsupervised feature learning audio classification using convolutional deep belief networks. advances neural information processing systems huang learned-miller learning hierarchical representations face verification convolutional deep belief networks. ieee conf. computer vision pattern recognition core team language environment statistical computing. foundation statistical computing vienna austria. http//www.r-project.org/ tippmann sylvia \"programming tools adventures nature –.doi./a comprehensive archive network https//cran.r-project.org/ rumelhart david hinton geoffrey williams ronald learning representations back-propagating errors. nature larochelle bengio louradour lamblin exploring strategies training deep neural networks. journal machine learning research bengio lamblin popovici larochelle greedy layer-wise training deep networks. advances neural information processing systems coates analysis single-layer networks unsupervised feature learning. aistats bengio yoshua lamblin pascal popovici larochelle hugo. greedy layer-wise training deep networks. raina battle packer a.y. self-taught learning transfer learning unlabeled data. proc. int'l conf. machine learning isbn ----. pereira mitchell botvi machine learning classifiers fmri tutorial overview. neuroimage easynn help user guide cheshire neural planner software; last updated january available from http//www.easynn.com/ snoek jasper hugo larochelle ryan prescott adams. practical bayesian optimization machine learning algorithms. advances neural information processing systems package deepnet. cran project https//cran.r-project.org/web/packages/deepnet/index.html technology review https//www.technologyreview.com/s//deep-learning/ bengio lecun scaling learning algorithms towards bottou chapelle decoste weston large scale kernel machines. press cambridge vol. http//www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter.pdf bengio courville vincent representation learning review perspectives. pattern analysis machine intelligence ieee transactions doi./tpami.. arel rose karnowski deep machine learning-a frontier artificial intelligence research ieee comput intell nets. neural comput bengio lamblin popovici larochelle greedy layer-wise training deep networks vol. larochelle bengio louradour lamblin exploring strategies training deep neural networks. mach learn salakhutdinov hinton deep boltzmann machines. international conference artificial intelligence statistics. jmlr.org. goodfellow saxe measuring invariances deep networks. advances neural information processing systems. curran associates inc. dahl ranzato mohamed hinton phone recognition mean-covariance restricted boltzmann machine. advances neural information processing systems. curran associates inc. hinton deng mohamed jaitly senior vanhoucke nguyen sainath dahl kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups. signal process ieee seide conversational speech transcription using contextdependent deep neural networks. interspeech. isca. dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition. audio speech lang process ieee trans mohamed dahl hinton acoustic modeling using deep belief networks. audio speech lang process ieee trans krizhevsky sutskever hinton imagenet classification deep convolutional neural networks. advances neural information processing systems. curran associates inc. vol. mikolov deoras kombrink burget cernock`y empirical evaluation combination advanced language modeling techniques. interspeech. isca. socher huang pennin manning dynamic pooling unfolding recursive autoencoders paraphrase detection. advances neural information processing systems. curran associates inc. bordes glorot weston bengio joint learning words meaning representations open-text semantic parsing. international conference artificial intelligence statistics. jmlr.org. dayong wang aditya khosla rishab gargeya humayun irshad andrew beck. deep learning identifying metastatic breast cancer. arxiv.v june national research council frontiers massive data analysis. national academies press washington http//www.nap.edu/openbook.php?record_id= dumbill data? introduction data landscape. strata making data work. o’reilly santa clara o’reilly", "year": "2017"}