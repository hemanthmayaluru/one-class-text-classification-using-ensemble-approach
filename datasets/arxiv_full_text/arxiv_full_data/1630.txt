{"title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword  Spotting", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.", "text": "keyword spotting constitutes major component human-technology detection accuracy false alarm rate minimizing footprint size latency complexity goals kws. towards achieving them study convolutional recurrent neural networks inspired large-scale state-ofthe-art speech recognition systems combine strengths convolutional layers recurrent layers exploit local structure long-range context. analyze effect architecture parameters propose training strategies improve performance. parameters crnn model yields acceptably latency achieves accuracy fa/hour signal-to-noise ratio. index terms keyword spotting speech convolutional neural networks recurrent neural networks. motivated common humans interact other conversational human-technology interfaces becoming increasingly popular numerous applications. highperformance speech-to-text conversion text-to-speech conversion constitute important aspects interfaces computational algorithms developed text inputs outputs. another crucial aspect conversational interfaces keyword spotting also known wakeword detection transitioning different computational states based voice input provided users. systems detect particular keyword continuous stream audio. output determines different states device high detection accuracy false alarm rate critical enable satisfactory user experience. typical applications exist environments interference background audio reverberation distortion sounds generated speaker device embedded. system demonstrate robust performance range situations. furthermore computational complexity model size important concerns systems typically embedded consumer devices limited memory computational resources smartphones smart-home sensors. already millions devices embedded systems. traditional approaches based hidden markov models sequence search algorithms advances deep learning increase amount available data state-of-the-art replaced deep learning-based approaches superior performance deep learning-based systems commonly deep neural networks combined compression techniques multi-style training approaches potential drawback dnns ignore structure context input audio input strong dependencies time frequency domains. goal exploiting local connectivity patterns shared weights convolutional neural networks explored potential drawback cnns cannot model context entire frame without wide filters great depth. recurrent neural networks also studied connectionist temporal classification loss unlike aforementioned models cross-entropy loss. high accuracy rate could obtained given ambitious targets applications systems. similar dnns potential limitation rnns modeling done input features without structure successive time frequency steps. recently proposed convolutional recurrent neural network architecture loss. however despite large model size similar rnns high accuracy rate could obtained. paper focus developing production-quality system using crnns loss small-footprint model applied single keyword. goal combine strengths cnns rnns additional strategies applied training improve overall performance keeping small-footprint size. rest paper organized follows. section describe end-to-end architecture training methodologies small-footprint kws. section explain experiments corresponding results. section present conclusions. focus canonical crnn architecture inspired successful large-scale speech recognition systems adapt architectures small-footprint model size needs shrunk three orders magnitude. analyze impact different parameters performance shrinking size model. fig. shows crnn architecture corresponding parameters. time-domain inputs converted perchannel energy normalized spectrograms succinct representation efficient training. choose frame length seconds sufficiently long capture reasonable pronunciation talktype. using sampling rate frame contains time-domain samples. corresponding pcen spectrograms obtained stride channels yielding input dimensionality entire data consists different samples collected speakers. dataset split training development test sets ratio. training samples augmented applying additive noise power determined signal-to-noise ratio sampled interval. additive noise sampled data representative background noise speech total length exceeding hours. provide robustness alignment errors training samples also augmented introducing random timing jitter. adam optimization algorithm training batch size learning rate initially chosen later dropped evaluation considers streaming scenario inference performed overlapping frames duration shift frames chosen metrics focus false rejection rate false alarms hour typically fixing latter desired value fa/hr noise added development test sets magnitude depending value. note collected samples already noisy actual lower defined precisely ratio powers information-bearing signal noise. similar augmentation training sets negative samples noise datasets sampled representative background noise speech. performance various crnn architectures development snr. note models trained convergence even though requires different number epochs. observe general trend larger model size typically yields better performance. increasing number convolution filters increasing number recurrent hidden units effective approaches improve performance. increasing number recurrent layers limited impact preferred lstm better performance obtained lower complexity. performance model architectures comparable size.) pcen features given inputs convolutional layer employ filtering along time frequency dimensions. outputs convolutional layer bidirectional recurrent layers might include gated recurrent units long short-term memory units process entire frame. outputs recurrent layers given fully connected layer. lastly softmax decoding applied neurons obtain corresponding scalar score. rectified linear units activation function layers. require keyword characters smoothed character occupancy scores ğ‘ğ‘\"ğ‘¡ decay rate initialize ğ‘-.ğ‘\"ğ‘¡ ğ‘ğ‘\"ğ‘¡ ğ‘.-ğ‘\"ğ‘¡ ğ‘ğ‘\"ğ‘¡ ğ‘‡.-=argmax ğ‘.-ğ‘\"ağ‘¡ =ğ›¼â‹…ğ‘.-ğ‘\"ağ‘¡ ğ‘¡â‰¥ğ‘‡. ğ‘˜âˆ¶=ğ¾âˆ’ ğ‘‡-.=argmax ğ‘-.ğ‘\"dağ‘¡ =ğ›¼â‹…ğ‘-.ğ‘\"dağ‘¡ ğ‘¡â‰¤ğ‘‡-. return implemented processors modern consumer devices implement nonlinear operations). even implemented modern smartphones without approximations special function units model achieve inference time much faster time scale reactive time humans auditory stimuli desired limit model size given resource constraints latency memory power consumption. following choose size limit rest paper default architecture parameters highlighted bold also corresponds fairly optimal point given model size performance trade-off. compare performance architecture based given discrepancy input dimensionality training data reoptimize model hyperparameters best performance upper-bounding number parameters fair comparison. development best architecture achieves fa/hour fa/hour. metrics higher compared values chosen crnn model parameters. interestingly performance lower higher values. elaborate section recall model bidirectional runs overlapping second windows stride. however thanks small model size large time stride initial convolution layer able inference comfortably faster real time. inference computational complexity chosen crnn-based model parameters roughly floating point operations given representation capacity limit imposed architecture size increasing amount positive samples training data limited effect performance. fig. shows fa/hour number unique talktype samples used training. saturation performance occurs faster applications similar type data large-scale models e.g. besides increasing amount positive samples observe performance improvement increasing diversity relevant negative samples obtained hard mining. mine negative samples using pre-converged model large public videos dataset then training continued using mined negative samples convergence. shown fig. hard negative mining yields decrease test set. test various values fig. shows hour. higher lower obtained stable performance starts lower rate. note values augmented training samples sampled distribution mean deterioration performance observed beyond value. performance lower values improved augmenting lower comes expense decreased performance higher attributed limited learning capacity model. observe benefit recurrent layers especially lower values. performance crnn architectures architectures explained section reduces increases. hypothesize recurrent layers better able adapt noise signature individual samples since layer processes information entire frame. cnns contrast require wide filters and/or great depth level information propagation. dataset already consists samples recorded varying distance values representative applications smartphone systems. applications smart-home systems require high performance far-field conditions. fig. shows performance degradation additional distance. far-field test sets constructed augmenting original test impulse responses corresponding variety configurations given distance significant deterioration conjunction higher noise also explained provide robustness deterioration consider training far-field-augmented training samples using variety impulse responses different ones test set. augmentation achieves significantly less degradation performance farther distances. yields worse performance original data training/testing mismatch. studied crnns small-footprint systems. presented trade-off model size performance demonstrated optimal choice parameters given tradeoff. capacity limitation model various implications. performance gain limited merely increasing number positive samples hard negative mining improves performance. training sets carefully chosen reflect application environment noise level far-field conditions. overall fa/hour model achieves accuracy test values respectively. numerical performance results seem better models literature. however direct comparison meaningful difference datasets actual keywords i.e. inference task. given human performance excellent task still believe room improvement terms performance. discussions andrew sanjeev satheesh jiaji huang bing jiang gratefully acknowledged. thank song impulse response measurements used farfield augmentation. j.r. rohlicek russell roukos gish continuous hidden markov modeling speaker-independent wordspotting ieee proceedings international conference acoustics speech signal processing chen parada heigold small-footprint keyword spotting using deep neural networks proceedings international conference acoustics speech signal processing tucker panchapagesan vitaladevuni model compression applied small-footprint keyword spotting proceedings interspeech vikas sindhwani tara sainath sanjiv kumar structured transforms small-footprint deep learning neural information processing systems prabhavalkar alvarez parada nakkiran sainath automatic gain control multi-style training robust small-footprint keyword spotting deep neural networks ieee proceedings international conference acoustics speech signal processing panchapagesan khare matsoukas mandal hoffmeister vitaladevuni multi-task learning weighted cross-entropy dnn-based keyword spotting proceedings interspeech sainath parada convolutional neural networks small-footprint keyword spotting proceedings interspeech wang getreuer hughes lyon saurous trainable frontend robust far-field keyword spotting arxiv preprint arxiv. hwang sung online keyword spotting character-level preprint arxiv. fernandez graves schmidhuber application recurrent neural networks discriminative keyword spotting artificial neural networks. springer lengerich hannun end-to-end architecture keyword spotting voice activity detection arxiv preprint arxiv. deng platt ensemble deep learning speech recognition proceedings interspeech sainath vinyals senior convolutional long short-term memory fully connected deep neural networks ieee proceedings international conference acoustics speech signal processing amodei deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. merrienboer bahdanau bengio properties translation encoder-decoder approaches arxiv preprint arxiv. hochreiter schmidhuber long short-term memory neural computation vol. kingma adam method stochastic optimization arxiv preprint arxiv. shelton kumar comparison auditory visual simple reaction times neuroscience medicine vol. kumatani microphone array processing distant speech recognition towards real-world deployment ieee asiapacific signal information processing association annual summit conference", "year": 2017}