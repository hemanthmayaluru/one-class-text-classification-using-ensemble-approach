{"title": "Structured Pruning of Deep Convolutional Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Real time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses. Network pruning is a promising technique to solve this problem. However, pruning usually results in irregular network connections that not only demand extra representation efforts but also do not fit well on parallel computation. We introduce structured sparsity at various scales for convolutional neural networks, which are channel wise, kernel wise and intra kernel strided sparsity. This structured sparsity is very advantageous for direct computational resource savings on embedded computers, parallel computing environments and hardware based systems. To decide the importance of network connections and paths, the proposed method uses a particle filtering approach. The importance weight of each particle is assigned by computing the misclassification rate with corresponding connectivity pattern. The pruned network is re-trained to compensate for the losses due to pruning. While implementing convolutions as matrix products, we particularly show that intra kernel strided sparsity with a simple constraint can significantly reduce the size of kernel and feature map matrices. The pruned network is finally fixed point optimized with reduced word length precision. This results in significant reduction in the total storage size providing advantages for on-chip memory based implementations of deep neural networks.", "text": "real time application deep learning algorithms often hindered high computational complexity frequent memory accesses. network pruning promising technique solve problem. however pruning usually results irregular network connections demand extra representation efforts also well parallel computation. introduce structured sparsity various scales convolutional neural networks channel wise kernel wise intra kernel strided sparsity. structured sparsity direct computational resource savings embedded computers parallel computing environments hardware based systems. decide importance network connections paths proposed method uses particle filtering approach. importance weight particle assigned computing misclassification rate corresponding connectivity pattern. pruned network re-trained compensate losses pruning. implementing convolutions matrix products particularly show intra kernel strided sparsity simple constraint significantly reduce size kernel feature tensors. pruned network finally quantized reduced word length precision. results significant reduction total storage size providing advantages on-chip memory based implementations deep neural networks. introduction onvolutional neural networks successfully applied several diverse classification problems including speech image recognition design need decide optimum network architecture parameters count specific task. large networks capacity learn difficult functions cost increased computational complexity. however current parameters count greater unknown optimum number overfitting occur. hand parameters limit network’s learning capability. efficient approach training learn task large sized network prune removing redundant duplicate connections. results comparable level performance fewer parameters better generalization another important problem porting deep learning algorithms resource limited portable devices. advances deep learning make smartphones machines even smarter. purpose researchers proposed ideas designing best performing improved generalization parameters saved chip memory. results energy savings frequent dram accesses consume much energy. since deep neural networks conduct many multiply accumulate operations sparsity helps reducing computations. however computational complexity neural network depends number parameters arithmetic operations architecture layer types connectivity patterns. irregular sparsity difficult exploit efficient computation. proposed work investigates proposes ideas solve problems structured pruning fixed point optimization. pruning useful several ways. first gradual network pruning inherits knowledge bigger cumbersome network. secondly directly learning complex function lightweight network yield acceptable results. moreover practice optimum network architecture unknown. literature large sized deep networks achieved state-of-the-art performance various challenging tasks. therefore appropriate first learn task many parameters followed pruning redundant less important connections. pruning techniques broadly categorized structured unstructured. unstructured pruning follow specific geometry constraint. cases technique needs extra information represent sparse locations. depends sparse representation computational benefits. hand structured sparsity places non-zero parameters well-defined locations. kind constraint enables modern cpus graphics processing units easily exploit computational savings. work explore channel kernel intra-kernel sparsity means structured pruning. channel level pruning incoming outgoing weights to/from feature pruned. channel level pruning directly produce lightweight network. kernel level pruning drops full kernel whereas intra-kernel sparsity prunes weights kernel. proposed work shows combined convolution lowering intra kernel strided sparsity significantly speedup convolution layer processing. kernel level pruning special case intra-kernel sparsity pruning. granularities applied various combinations different orders. network pruning studied several researches works shown much bigger portion weights zero minimum loss performance. train network norm augmented loss functions gradually prune value connection less threshold connection dropped. authors extend work quantizing finally pruned network however sparse representation benefit induced sparsity. units hidden layers pruned feed-forward deep neural network. computational complexity reduced sparse connectivity convolution fully connected layers prunes multi-layered feed forward networks genetic algorithm simulated annealing. survey pruning techniques reported works utilize unstructured sparsity feed forward neural networks. recently published work induces channel wise sparsity network compared proposed work explores sparsity multiple levels using efficient search followed fixed point optimization. dropout dropconnect zeroes neuron outputs weights training network architecture change evaluation time. techniques train different subsets network parameters training results better generalization. work drops parameters permanently yields network fewer parameters test time. convolutions converted matrix-matrix multiplication follows logic bigger sized matrix multiplications better several small sized ones fixed-point optimization used reduce memory computational requirements deep networks another contribution proposed work particle filter locate pruning candidates. search likely connection combinations prune rest. particle simulates connections masks. also apply fixed point optimization reduce word length network parameters. represent weights signals bits precision maintaining level performance. reduces storage requirement advantageous on-chip based implementation deep learning algorithms. rest paper organized follows. section briefly introduces context pruning. pruning criterion outlined section section also discusses pruning granularities particle filter genetic algorithm hybrid evolutionary particle filter. section explains iterative pruning process fixed point optimization. experimental results discussions provided section finally section introduces future work concludes. section briefly introduces context pruning. first proposed applied handwritten digit recognition lecun et.al. diverse layer types deep neural network convolution pooling fully connected layers. convolution pooling layers feature extractors. rear near output layer employs fully connected layers classification. sample network shown fig. computationally expensive layers convolution layers. fully connected layers implemented matrix multiplications. convolution layer convolves kernels feature maps previous layer. next layer feature maps convolutions performed operations performed represents feature width height next layer. convolution layers default memory access pattern cache friendly. therefore highly desirable reduce complexity convolution layers. literature various attempts reduce computational complexity convolution layers. work converts convolutions matrix-matrix multiplication. avoids usage levels nested loops speeds computation times however redundant data kernels storage cost extra memory usage. work proposes reduce complexity structured pruning fixed point optimization. show strided sparsity helpful reducing size matrices convolution lowering next section discusses detail. pruning permanently drops less important connections network computational benefits. unstructured pruning requires sparse representation schemes reducing computations however demands addressing overhead computing addresses non-zero elements. structured pruning little extra cost easily exploited efficient implementation. introduce structured pruning various granularities. section present granularities selection pruning candidates evolutionary particle filter figure convolutional neural network eight layers. layers convolution layers constitute pooling layers. fully connected layers. network represented string ------- number denotes count feature maps layer taking computational advantages using randomly scattered unstructured sparsity network difficult. demands many conditional operations extra representation denote location zero non-zero parameters. generally convolution layers non-pruned network fully connected convolution connections. fig. layer contain feature maps respectively. channel feature similar concepts therefore used interchangeably throughout article. number convolution connections feature convolution connection channel thus pruning exploiting largest granularity deleting feature feature maps. feature layer removed incoming outgoing kernels pruned. fig. shows pruned kernels dashed line. considering configuration fig. architecture reduced next level pruning deleting kernels kernel represents whole convolution. kernel level sparsity depicted blue dotted lines fig.. lowest level pruning using intra-kernel sparsity forces weights zero valued ones. previous works intra-kernel level pruning usually conducted zeroing small valued weights particularly explore intra-kernel level pruning using sparsity well-defined locations called intra kernel strided sparsity. fig. depicts idea. starting index first non-zero element randomly assigned. therefore strided sparsity associates offset starting index stride size kernel. figure shows strided sparsity simple constraint reduce size feature kernel matrices. intra kernel strided pruning potential bridge pruning computational advantages. combined convolution lowering significantly reduce computational cost. igure shows channel filter wise pruning. dashed line shows channel level pruning. prune incoming filters feature outgoing kernels also pruned. blue dotted line depicts pruning kernels. shows intra kernel level sparsity structured unstructured cases. kernel level pruning special case intra-kernel pruning sparsity rate figure figure provides example convolution lowering idea introduced proposed idea constrains outgoing convolution connection source feature similar stride offset. offset shows index first pruned weight. constraint shown similar colored background squares. significantly reduces size features matrix kernel matrix. first columns input feature matrix changes underlined elements pruned. colored elements feature maps kernels survive rest pruned. example size feature matrix reduced kernel matrix size reduced figure shows dotted matrices pruning masks weights layers. state vector represents mask. example state vector provided. circles represent neurons shows weight going neuron pruning process needs select less important connection combinations pruning candidates. connections pruned least adversary network performance compensated re-training. highly likely connections survive iterative pruning. work propose locate pruning candidate connections sequential monte carlo method also known particle filters particle filters finds applications several fields -.with weighted particles represents filtering distribution particle filter usually applied system model shown equation state vector represented shows time step observation vector observation noise process noise. observation function represented whereas represents transition function. pruning connections possible combinations order means exhaustive search feasible. particles simulate several possible connection combinations. fig. shows example particle’s state vector. trained network used observation function noisy classification error rate greater evaluate misclassification rate evaluation particle. importance weight computed mcr. connections high importance survive several iterations rest pruned. compares network assigned label true label guides network learn true labels. particles assigned probabilities construct cumulative distribution function resample sequential importance resampling transition function simulated perturbing pruning mask. finite number samples suffer degeneracy impoverishment problems less likely particles replaced highly likely particles evolutionary particle filter proposes hybrid approach genetic algorithm combined particles similar chromosomes survival fittest equivalence resampling algorithm. hybrid approach increase fitness whole population. augmented approach re-supplies re-defines particles less likely regions maintaining highly likely genes chromosome. reduces computational cost requires fewer particles conventional particle filters help reducing cost finding pruning candidates importance weight assignment uses small sized evaluation set. also consider techniques potential usages exploring network parameters space. proposed work first trains network baseline. followed pruning reduces effective number network parameters. pruning degrades network performance compensated re-training pruned network. pruning limit layer depending parameter count learning capacity. usually first convolution layer fewer parameters following layers. secondly directly performs input layer therefore sensitive pruning therefore lower limit first convolution layer. whole process aside validation network convergence. particularly show plot sparsities various levels applied different combinations. work reduce memory requirement quantizing pruned network fixed-point optimization. quantization techniques orthogonal network pruning techniques hence supplement fixed-point optimization algorithm outlined fixed-point optimization network keeps high precision weights. algorithm obtains quantized weights floating points using error minimization. grouping weights quantization convolution kernel quantization step size. quantized weights used feed forward path whereas corresponding floating point weights updated backward path. algorithm also evaluates layer wise sensitivity analysis signal quantization. apply techniques pruned network squeeze memory computational requirements. next section provides initial experimental results discussions. first show pruning enables network learn better fewer parameters resources. fig. shows results first experiment. experiment architectures ------ ------. half convolution connection train cifar dataset achieve baseline result shown black solid line fig. trained network pruned obtain shown green line fig. find performance quite close baseline. second case randomly initialize train cifar-. shown dashed line fig. observe network cannot reach close baseline performance pruned network. indicates pruning helps network learn better. transfer learning enables lightweight network inherit useful knowledge bigger sized predecessor. experiment conduct channel kernel level pruning second third convolution layers. fig. shows mask matrix intermediate stage pruning. vertical black lines show pruning feature maps destination layer horizontal black lines show pruning source feature maps. also kernel level pruning appears black squares fig. observed sparsity well structured. present experimental results datasets mnist cifar- observe pruned networks show better performance non-pruned generalization. however drop many parameters network capability reduced that’s error rate starts rising strided pruning useful introduces intra kernel structured sparsity. conducting experiments evaluating effectiveness convolution lowering .the proposed work unique sense friendlier computational benefits. training pruning stochastic gradient descent mini-batch size rmsprop cifar- dataset consists class classification problem dataset includes samples classes airplane automobile bird deer frog horse ship truck. training consists samples. test contains samples. sample resolution. first conduct experiment network ------ architecture. divide training validation training train network rectified linear units sgd. network total parameters. channel wise sparsity reduces network size ------. first convolution layer pruned. convolution layer numbers convolution connections reduced reduced result convolution connections dropped less increase mcr. first stage total sparsity. increasing channel level sparsity beyond point causes increase next experiments evaluated sparsities various combinations. fig.. summarizes results. find channel level pruning destructive affects large number parameters. intra kernel sparsities induced various proportions. example kernel weights pruned ratio etc. experimental results show reduce size network minimum loss accuracy. achieved macro pruning followed intra-kernel strided sparsity emphasize kind sparsity well structured directly encoded representing convolutions matrix-matrix multiplication. also apply fixed point optimization finally pruned network. results shown table observe pruned network represented precision. here precision -levels denotes representation. reduces network size significant amount on-chip memory based vlsi implementations. figure plot shows pruning applied various combinations. observed feature map/channel level pruning followed intra kernel pruning provides best result. figure shows channel kernel level sparsity. entry matrix represents kernel shows channel level sparsity layer. layer total convolutions. figure shows number reduced also causes kernel level pruning next convolution layer shown horizontal black lines. black squares show kernel level pruning. mnist handwritten digit recognition dataset consisting grey scale images resolutions images divided training test sets. first evaluate effectiveness structured sparsity network architecture ------ architecture convolution pooling layers. first convolution layer weights whereas second convolution layer weights. first train network using training samples validation samples sgd. obtain test error rate training. followed pruning network. prune first layer twenty convolution connections. feature map/channel level pruning applied second convolution layer. pruning iteration network size reduced ------ test set. number convolution connections layer reduced reduction second convolution layer. indirect impact first fully connected layer reduces important recall kind sparsity demand extra representational efforts sparsity well structured directly reduces computational cost. work explored structured sparsity deep convolutional neural networks. sparsity channel kernel level explained along intra-kernel strided one. found channel level sparsity cannot applied higher proportion beyond limit affects network’s representational capacity. found intra-kernel strided sparsity along convolution lowering significantly reduce computational complexity convolutions. quantization pruning orthogonal techniques augment savings. future work exploring related aspects work profiling execution time effects reduced size convolution lowering.", "year": 2015}