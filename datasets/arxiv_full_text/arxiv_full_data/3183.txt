{"title": "Feature Selection Using Classifier in High Dimensional Data", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Feature selection is frequently used as a pre-processing step to machine learning. It is a process of choosing a subset of original features so that the feature space is optimally reduced according to a certain evaluation criterion. The central objective of this paper is to reduce the dimension of the data by finding a small set of important features which can give good classification performance. We have applied filter and wrapper approach with different classifiers QDA and LDA respectively. A widely-used filter method is used for bioinformatics data i.e. a univariate criterion separately on each feature, assuming that there is no interaction between features and then applied Sequential Feature Selection method. Experimental results show that filter approach gives better performance in respect of Misclassification Error Rate.", "text": "feature selection frequently used pre-processing step machine learning. process choosing subset original features feature space optimally reduced according certain evaluation criterion. central objective paper reduce dimension data finding small important features give good classification performance. applied filter wrapper approach different classifiers respectively. widely-used filter method used bioinformatics data i.e. univariate criterion separately feature assuming interaction features applied sequential feature selection method. experimental results show filter approach gives better performance respect misclassification error rate. input variables modeling problem) grows exponentially. cutting input variable's scale into segments implies single-input model require examples model construction model input variables require forth assuming inputs completely uncorrelated input variables criterion would require million examples. real problems input variables usually somewhat correlated reducing number needed examples problem still explodes rapidly estimates considered somewhat conservative perhaps cell. given issue data miners often faced task selecting predictor variables keep model. process goes several names common subset selection attribute selection feature selection. many solutions proposed task though none perfect except small problems. solutions attack problem directly experimenting predictors kept. applied biology domain technique also called discriminative gene selection detects influential genes based microarray experiments. removing irrelevant redundant features data feature selection helps improve performance learning models filter method filter approach requires statistical analysis feature solving task without utilizing learning model filters work fast using simple measurement result always satisfactory. rapper method wrapper approach involves predetermined learning model selects features measuring learning performance particular learning model. ybrid method hybrid approach attempts take advantage filter wrapper approaches often found that hybrid technique capable locating good solution single technique often traps immature solution. hybrid approach filter models chosen preliminary screening remove redundant irrelevant features f-score information gain. resulted feature sets combined together pre-processed feature fine tuning. step called combination model finally wrapper model applied improve classification accuracy fine-tuning step. stream wise feature selection considers features sequentially adds predictive model. features known advance. thus flexibility stream wise regression provides dynamically decide features generate feature stream provides potentially large savings computation. empirical tests show smaller data sets stepwise regression done hybrid genetic algorithm feature selection introduced called hgafs algorithm selects salient feature subset within reduced size. hgafs incorporates local search operation devised embedded finetune search feature selection process. guide search process newly generated offspring adjusted less correlated features consisting general special characteristics given dataset. thus algorithm reduces redundancy information among selected features. stochastic algorithm based grasp meta-heuristic proposed main goal speeding feature subset selection process basically reducing number wrapper evaluations carry out. grasp multi-start constructive method constructs solution first stage runs improving stage solution. several instances proposed grasp method experimentally tested compared state-of-the-art algorithms high-dimensional datasets. complexity terms wrapper evaluations carried proposed grasp algorithm comes following parameters number predictive variables; cardinality subset selected constructive step; number iterations carried grasp algorithm; wrapper method selected improving step. hybrid algorithm saga proposed task. saga combines ability avoid trapped local minimum simulated annealing high rate convergence crossover operator genetic algorithms strong local search ability greedy algorithms high computational efficiency generalized reducing number features important statistical learning. many data sets large number features limited number observations bioinformatics data usually many features useful producing desired learning result limited observations lead learning algorithm overfit noise. paper shows perform sequential feature selection popular feature selection algorithms. also shows holdout cross-validation evaluate performance selected features. quadratic discriminant analysis closely related linear discriminant analysis assumed classes points measurements normally distributed. unlike however assumption covariance classes identical. assumption true best possible test hypothesis given measurement given class likelihood ratio test. statistics pattern recognition machine find linear combination features characterizes separates classes objects events. resulting combination used linear classifier commonly dimensionality reduction later classification. explicitly attempts model difference classes data. holdout method simplest kind cross validation. data separated sets called training testing set. function approximator fits function using training only. function approximator asked predict output values data testing errors makes accumulated give mean absolute test error used evaluate model. cross-validation sometimes called rotation estimation technique assessing results statistical analysis generalize independent data set. randomly partitioned subsamples. subsamples single subsample retained validation data testing model remaining subsamples used training data. cross-validation process repeated times subsamples used exactly validation data. results folds averaged produce single estimation. stratified k-fold cross-validation folds selected mean response value approximately equal folds. case dichotomous classification means fold contains roughly proportions types class labels. sequential forward selection sequential backward selection commonly used algorithms usually combination wrapper evaluator. algorithms worst-case complexity especially high-dimensional datasets used evaluations simpler bottom search procedure adds features feature time final feature reached. suppose features feature selected criterion function evaluated. feature yields maximum value chosen added xd.thus stage variable chosen that added current maximises selection criterion. feature initialised null set. best improvement makes feature worst maximum allowable number feature reached algorithm terminates. given -.xk dimensional vector positive definite matrix features used. following algorithm explains whole procedure stage search sets subsets generated evaluation using cross validation procedure. variable chosen largest. process repeated required cardinality remains. used clinical high dimensional data rows features. data variable consists observations features. here divide data training size test size size quadratic discriminant analysis linear discriminant analysis separately dataset classification algorithm. paper explained example filter method example wrapper method. widely-used filter method bioinformatics data apply univariate criterion separately feature assuming interaction features. example might apply t-test feature compare p-value feature measure effective separating groups. order general idea well-separated groups feature plot empirical cumulative distribution function p-values. graph obtained classification; features pvalues close zero features p-values smaller meaning features among original features strong discrimination power. sort features according p-values select features sorted list. however usually difficult decide many features needed unless domain knowledge maximum number features considered dictated advance based outside constraints. quick decide number needed features plot test function number features. since observations training largest number features applying limited otherwise enough samples group estimate covariance matrix. actually data used paper holdout partition sizes groups dictate largest allowable number features applying compute various numbers features show plot function number features. order reasonably estimate performance selected model important training samples model compute test observations. simple filter feature selection method gets smallest test features features used respectively. plot shows overfitting occurs features used. training used select features model test used evaluate performance finally selected feature. feature selection procedure evaluate compare performance candidate feature subset apply stratified -fold cross-validation training set. filter results pre-processing step select features. example select features here. apply forward sequential feature selection features. stops first local minimum cross-validation found. algorithm stopped prematurely. sometimes smaller achievable looking minimum cross-validation reasonable range number features. cross-validation shown fig. reaches minimum value features used curve stays flat range features features also curve goes features used means overfitting occurs there. usually preferable fewer features pick features calculated misclassification error rate calculated paper decide number needed features using holdout validation cross-validation. order reasonably estimate performance selected model used training samples model compute test observations. pablo bermejo jose g√°mez jose puerta grasp algorithm fast hybrid feature subset selection high-dimensional datasets pattern recognition letters r.k. sivagaminathan ramakrishnan hybrid approach feature subset selection using neural networks colony optimization expert systems applications", "year": 2014}