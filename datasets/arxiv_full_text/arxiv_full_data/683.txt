{"title": "A Neural-Network Technique to Learn Concepts from Electroencephalograms", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "A new technique is presented developed to learn multi-class concepts from clinical electroencephalograms. A desired concept is represented as a neuronal computational model consisting of the input, hidden, and output neurons. In this model the hidden neurons learn independently to classify the electroencephalogram segments presented by spectral and statistical features. This technique has been applied to the electroencephalogram data recorded from 65 sleeping healthy newborns in order to learn a brain maturation concept of newborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these data have been used for learning and testing the concept, respectively. As a result, the concept has correctly classified 80.1% of the testing segments or 87.7% of the 65 records.", "text": "abstract. technique presented developed learn multi-class concepts clinical electroencephalograms. desired concept represented neuronal computational model consisting input hidden output neurons. model hidden neurons learn independently classify electroencephalogram segments presented spectral statistical features. technique applied electroencephalogram data recorded sleeping healthy newborns order learn brain maturation concept newborns aged weeks. segments data used learning testing concept respectively. result concept correctly classified testing segments records. machine learning neural-network techniques successfully used learn classification models concepts real-world data including electroencephalograms methods explore given input variables features assumed represent classification problem discard relevant classification problem e.g. corrupted noise input variables seriously hurt generalization ability induced concepts discard irrelevant features number feature selection methods suggested based greedy hill-climbing strategy applied multivariate classification problems learning methods based strategy able find sub-optimal features acceptable learning time general learning time required learning concepts presence irrelevant features increases proportionally data size well number classes computational time additionally increases classification problem presented examples non-linearly separable class boundary overlap heavily hand concept induced given data e.g. eegs observable users want understand decision making mechanism. reason users prefer classification models based decision trees easy-to-understand interpret usually consist nodes types decision node containing test leaf node assigned appropriate class. branch represents possible outcome test. example coming root follows branches leaf node reached. name class leaf resulting classification. node test input features. multivariate nodes test features. multivariate much shorter tests single feature learn concepts presented numerical attributes authors suggested multivariate threshold logical units single neurons. multivariate known also oblique dts. paper describe neural-network technique developed induce multi-class concepts large-scale clinical eegs. research classification models evaluate brain maturation newborns whose eegs recorded sleeping hours. clinical practice evaluations brain maturation assist clinicians objectively diagnose brain pathologies newborns section describe classification problem clinical data. section briefly describe techniques section present neural-network technique developed induce multi-class concept large-scale data. finally evaluate classification accuracy induced concept discuss results. section first describe classification problem structure data second present result statistical analysis aimed evaluate contribution input variables classification problem. general learning multi-class concepts eegs still difficult problem first eegs strongly non-stationary signals whose statistical characteristics vary widely; second eegs depend background brain activity patients; third eegs weak invoked potentials corrupted noise muscle artifacts fourth user give features irrelevant classification problem. data used experiments recorded sleeping newborns standard electrodes newborns weeks therefore data represent groups classes differ week average. following represent segment spectral statistical features calculated -sec interval frequency bands sub-delta delta theta alpha beta beta features calculated standard electrodes well real absolute spectral powers variances. observing records eeg-expert manually removed muscle artifacts records cleaning average rate outlying segments whose values exceed standard deviation calculated patient’s record. total segments presented features equal therefore dataset large-scale. statistics segments classes shown table below newborns healthy. healthy newborn concept learnt data assign segments corresponding group. newborns brain development pathologies output classification data vary plot principal components calculated different time windows newborns belonging different classes. components calculated four subsequent time intervals consisting segments observing plots values vary time class boundaries move dramatically. class boundaries move eegs reflect individual brain activities newborns. activity chaotic character increases group variance classes. evaluate influence individual activity used following statistical technique. first introduce variance classes group variance class input variable value decreases proportionally group variances increases proportionally variance groups. clearly group variance grows proportionally individual activity patients belonging group. therefore conclude value reflects significance feature fig. depicts values calculated features training set. plot important less impotent features. intervals calculated features observing behavior features classes important feature dependent classes feature moreover interval feature less corresponding interval feature however using feature properly distinguish classes interval large yet. linear machine linear discriminant functions calculated order assign example classes internal node tests linear combination input variables introduce extended input vector discriminant function linear test node described follows learning weight vectors discriminant functions updated example misclassifies. weights class example actually belongs increased weights class erroneously assigns decreased. done using following error correction rule amount correction iteration. training examples linearly separable procedure train finite number iterations. however training examples non-linearly separable learning rule provide predictable classification accuracy. cases pocket algorithm suggested saves best result occurs learning. describe application algorithm problem. large-scale data classify split classes. centers classes differ average week extremely close overlap hardly. circumstances deteriorate performance pocket algorithm significantly found classification accuracy exceed testing data. assume following reason small accuracy. algorithm classifies training example erroneously updates weights linear tests sets length correctly classified example sequence zero. next training examples algorithm evaluate accuracy training examples. computational time increases quickly training data large-scale hardly overlapping non-linearly separable. case part computations wasted calculation accuracy training examples significantly decreases chance find best acceptable time. describe technique effectively learn multi-class concepts large-scale data. idea behind method train decision tree nodes separately classify pairs classes. follows given classes consists tlus learn classify examples classes. tlus deal classes. tlus collected neural network number networks becomes equal tlus belonging neural network linearly approximate dividing hyperplanes corresponding classes. formally define fi/j linear test learns divide training examples classes output described follows centers depicted fig. number tlus well neural networks therefore equal fig. lines depict dividing hyperplanes tlus perform. hyperplanes divide examples classes respectively. note region positive output grouping hyperplanes together build dividing hyperplanes depicted fig. first hyperplane superposition linear tests tests taken weights equal give positive output values examples class correspondingly second third hyperplanes signs correctly given example class superpose dividing hyperplanes two-layer feedforward neural networks consisting hidden tlus connected relevant input variables. summing contributions hidden neurons output neurons make final decision. example discussed above depicted consists three neural networks hidden neurons performing linear tests weights follows example output neurons connected hidden neurons weights equal respectively. general case classes consists hidden neurons fi/j output neurons weights output neuron connected hidden neurons fi/k fk/i equal respectively. applied neural network technique problem discuss result. learn -class concept clinical eegs applied neural-network technique described above. training testing concept used segments respectively. given classes consists linear tests. induced concept correctly classified training testing examples. summing segments belonging patient record concept correctly classified records training testing examples respectively. fig. plot results classifying testing segments records. number testing segments records varied portions segments correctly classified depicted dark parts bars. portions incorrectly classified segments depicted gray parts here. plot records misclassified. fig. depict summed outputs trained calculated patients testing segments. summed outputs interpret distributions segments classes. case provide probabilistic interpretation making decisions. example assign machine learning methods learn multi-class concepts well large-scale data. learn multi-class concepts successfully developed technique based pairwise classification. neural-network developed consists tlus perform linear multivariate tests. learning algorithm trains tests separately output neurons linearly approximate desired dividing hyperplanes. neural-network technique applied induce -class concept clinical data recorded newborns belonging groups. goal concept distinguish brain maturation newborns aged weeks. segment data represented features. training testing concept used segments respectively. result trained concept correctly classified training testing examples whilst correctly classified testing data. thus conclude neural network technique able learn accurate multi-class concepts clinical data. believe also technique applied large-scale data. work supported university jena authors grateful frank pasemann enlightening discussions joachim frenzel burkhart scheidt pediatric clinic university jena records well jonathan fieldsend university exeter useful comments. galicki witte dörschel doering eiselt grießbach. common optimization adaptive preprocessing units neural network learning period application pattern recognition. neural networks anderson devulapalli stolz. determining mental state signals using neural networks. scientific programming special issue applications analysis riddington ifeachor allen hudson mapps. fuzzy expert system interpretation. e.c. ifeachor k.g. rosen proc. int. conf. neural networks expert systems medicine healthcare university plymouth", "year": 2005}