{"title": "Deep Learning for Sentiment Analysis : A Survey", "tag": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "abstract": "Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many other application domains, deep learning is also popularly used in sentiment analysis in recent years. This paper first gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.", "text": "deep learning emerged powerful machine learning technique learns multiple layers representations features data produces state-of-the-art prediction results. along success deep learning many application domains deep learning also popularly used sentiment analysis recent years. paper first gives overview deep learning provides comprehensive survey current applications sentiment analysis. sentiment analysis opinion mining computational study people‚Äôs opinions sentiments emotions appraisals attitudes towards entities products services organizations individuals issues events topics attributes. inception rapid growth field coincide social media example reviews forum discussions blogs micro-blogs twitter social networks first time human history huge volume opinionated data recorded digital forms. since early sentiment analysis grown active research areas natural language processing also widely studied data mining mining text mining information retrieval. fact spread computer science management sciences social sciences marketing finance political science communications health science even history importance business society whole. proliferation fact opinions central almost human activities influencers behaviours. beliefs perceptions reality choices make considerable degree conditioned upon others evaluate world. reason whenever need make decision often seek opinions others. true individuals also true organizations. nowadays wants consumer product longer limited asking one‚Äôs friends family opinions many user reviews discussions product public forums web. organization longer necessary conduct surveys opinion polls focus groups order gather public opinions abundance information publicly available. recent years witnessed opinionated postings social media helped reshape businesses sway public sentiments emotions profoundly impacted social political systems. postings also mobilized masses political changes happened arab countries thus become necessity collect study opinions. however finding monitoring opinion sites distilling information contained remains formidable task proliferation diverse sites. site typically contains huge volume opinion text always easily deciphered long blogs forum postings. average human reader difficulty identifying relevant sites extracting summarizing opinions them. automated sentiment analysis systems thus needed. this many start-ups focusing providing sentiment analysis services. many corporations also built in-house capabilities. practical applications industrial interests provided strong motivations research sentiment analysis. existing research produced numerous techniques various tasks sentiment analysis include supervised unsupervised methods. supervised setting early papers used types supervised machine learning methods maximum entropy na√Øve bayes etc.) feature combinations. unsupervised methods include various methods exploit sentiment lexicons grammatical analysis syntactic patterns. several survey books papers published cover early methods applications extensively. since decade deep learning emerged powerful machine learning technique produced state-of-the-art results many application domains ranging computer vision speech recognition nlp. applying deep learning sentiment analysis also become popular recently. paper first gives overview deep learning provides comprehensive survey sentiment analysis research based deep learning. deep learning application artificial neural networks learning tasks using networks multiple layers. exploit much learning power neural networks deemed practical layers small amount data. inspired structure biological brain neural networks consist large number information processing units organized layers work unison. learn perform tasks adjusting connection weights neurons resembling learning process biological brain. based network topologies neural networks generally categorized feedforward neural networks recurrent/recursive neural networks also mixed matched. describe recurrent/recursive neural networks later. simple example feedforward neural network given figure consists three layers input layer corresponds input vector intercept term output layer corresponds output vector hidden layer whose output visible network output. circle represents element input vector circle represents neuron basic computation element neural network. also call activation function. line neurons represents connection flow information. connection associated weight value controlling signal neurons. learning neural network achieved adjusting weights neurons network alters weights based training examples training process obtain complex form hypotheses fits data. diving hidden layer neuron takes input intercept ùëäùë•+ùëè) outputs value rectified linear function =ùë†ùëñùëîùëöùëúùëñùëëùëäùë• =tanhùëäùë• =ùëÖùëíùêøùëàùëäùë• =maxùëäùë• sigmoid function takes real-valued number squashes value range function frequent historically nice interpretation firing rate neuron firing firing. non-linearity sigmoid recently fallen favour activations easily saturate either tail gradients almost zero information flow would cut. output zerocentered could introduce undesirable zig-zagging dynamics gradient updates connection weights training. thus tanh function often preferred practice output range zero-centered instead relu function also become popular lately. activation simply thresholded zero input less compared sigmoid function tanh function relu easy compute fast converge training yields equal better performance neural networks. softmax function output neuron generalization logistic function squashes k-dimensional vector arbitrary real values k-dimensional vector real values range function definition follows. connecting together neurons neural network figure parameters neuron layer neuron layer bias associated neuron layer train neural network stochastic gradient descent backpropagation usually employed minimize cross-entropy loss loss function softmax output. gradients loss function respect weights last hidden layer output layer first calculated gradients expressions respect weights upper network layers calculated recursively applying chain rule backward manner. gradients weights layers adjusted accordingly. iterative refinement process certain stopping criteria met. pseudo code training neural network figure follows. initialize weights biases neural network random values training example neural-network-prediction calculate gradients loss function respect layer weights hidden layer output layer calculate gradient respect chain rule layer weights input layer hidden layer update algorithm extended generic feedforward neural network training multiple hidden layers. note stochastic gradient descent estimates parameters every training example opposed whole training examples batch gradient descent. therefore parameter updates high variance cause loss function fluctuate different intensities helps discover possibly better local minima. research community lost interests neural networks late mainly regarded practical shallow neural networks training deep neural network complicated computationally expensive. however past years deep learning made breakthrough produced state-of-the-art results many application domains starting computer vision speech recognition recently nlp. renaissance neural networks attributed many factors. important ones include availability computing power advances hardware availability huge amounts training data power flexibility learning intermediate representations. nutshell deep learning uses cascade multiple layers nonlinear processing units feature extraction transformation. lower layers close data input learn simple features higher layers learn complex features derived lower layer features. architecture forms hierarchical powerful feature representation. figure shows feature hierarchy left right learned deep learning face image classification. learned image features grow complexity starting blobs/edges noses/eyes/cheeks faces. recent years deep learning models extensively applied field show great potentials. following several sections briefly describe main deep learning architectures related techniques applied tasks. many deep learning models need word embedding results input features. word embedding technique language modelling feature learning transforms words lower-dimensional dense vector space. dimension embedding vector represents latent feature word. vectors encode linguistic regularities patterns. learning word embeddings done using neural networks- matrix factorization. commonly used word embedding system wordveci essentially computationallyefficient neural network prediction model learns word embeddings text. contains continuous bag-of-words model skip-gram model cbow model predicts target word context words model inverse predicting context words given target word. statistically cbow model smoothens great deal distributional information treating entire context observation. effective smaller datasets. however model treats context-target pair observation better larger datasets. another frequently used learning approach global vectorii trained nonzero entries global word-word co-occurrence matrix. given input vector autoencoder first maps hidden representation encoder function latent representation mapped back decoder function reconstruction autoencoder typically trained minimize form reconstruction error ùëôùëúùë†ùë†. hidden layer. nonlinear function autoencoder able learn nonone often stacks autoencoders layers. higher level autoencoder uses output lower training data. stacked autoencoders along restricted boltzmann machines earliest approaches building deep neural networks. stack autoencoders trained unsupervised fashion parameters describing multiple levels representations used initialize supervised deep denoising autoencoder extension autoencoder input vector stochastically corrupted vector model trained denoise minimize denoising reconstruction error ùëôùëúùë†ùë†. idea behind force hidden layer discover robust features prevent simply learning identity. robust model able reconstruct input well even presence noises. example deleting adding words document change semantic document. convolutional neural network special type feedforward neural network originally employed field computer vision. design inspired human visual cortex visual mechanism animal brain. visual cortex contains cells responsible detecting light small overlapping sub-regions visual fields called receptive fields. cells local filters input space. consists multiple convolutional layers performs function processed cells visual cortex. figure shows recognizing traffic signs. input pixel image first stage filter used scan image. region input image filter projects receptive field. filter actually array numbers filter sliding multiplying weight values original pixel values image multiplications summed single number representative receptive field. every receptive field produces number. filter finishes scanning image array called activation feature map. need different filters scan input. figure apply kinds filters thus stacked feature maps first stage consists first convolutional layer. following convolutional layer subsampling layer usually used progressively reduce spatial size representation thus reduce number features computational complexity network. example subsampling first stage convolutional layer reduces dimensions note dimensionality feature reduced subsampling step retains important information commonly used subsampling operation pooling. afterwards output first stage becomes input second stage filters employed. filter size feature size last layer. second stage uses fully connected layer softmax readout layer output classes classification. convolutional layers play role feature extractor extracts local features restrict receptive fields hidden layers local. means special spatiallylocal correlation enforcing local connectivity pattern neurons adjacent layers. characteristic useful classification expect find strong local clues regarding class membership clues appear different places input. example document classification task single phrase help determining topic document. would like learn certain sequences words good indicators topic necessarily care appear document. convolutional pooling layers allow learn find local indicators regardless positions. recurrent neural network class neural networks whose connections neurons form directed cycle. unlike feedforward neural networks internal memory process sequence inputs makes popular processing sequential information. memory means performs task every element sequence output dependent previous computations like remembering information processed far. figure shows example rnn. left graph unfolded network cycles right graph folded sequence network three time steps. length time steps determined length input. example word sequence processed sentence words would unfolded neural network time steps layers. layer corresponds word. figure input vector time step hidden state time step ‚Ñé=ùëìùë§‚Ñé+ùë§ùë• equation activation function usually tanh function relu function. weight matrix used condition input weight matrix used condition previous hidden state output probability distribution vocabulary step example want ùë¶=ùë†ùëúùëìùë°ùëöùëéùë•ùë§‚Ñé hidden state regarded memory network. captures information happened previous time steps. calculated solely based memory time corresponding weight matrix parameters across steps. means performs task theoretically make information arbitrarily long sequences practice standard limited looking back steps vanishing gradient exploding gradient problem. researchers developed sophisticated types deal shortcomings standard model bidirectional deep bidirectional long short term memory network. bidirectional based idea output time depend previous elements sequence also depend next elements sequence. instance predict missing word sequence need look left right context. bidirectional consists rnns stacked other. processes input original order processes reversed input sequence. output computed based hidden state rnns. deep bidirectional similar bidirectional rnn. difference multiple layers time step rnns form chain repeating modules. standard rnns repeating module normally simple structure. however repeating module lstm complicated. instead single neural network layer four layers interacting special way. besides states hidden state cell state. figure shows example lstm. time step lstm first decides information dump cell state. decision made sigmoid function/layer called forget gate. function takes outputs ùëì=ùúéùëäùë•+ùëà‚Ñé update. next tanh function/layer creates vector candidate values ùëäùë•+ùëà‚Ñé =tanhùëäùë•+ùëà‚Ñé time update cell state cell state equation note forget gate control gradient passes allow explicit memory deletes finally lstm decides output based cell state. lstm first runs sigmoid layer decides parts cell state output equation called output gate. then lstm puts cell state tanh function multiplies output sigmoid gate lstm outputs parts decides equation lstm commonly applied sequential data also used tree-structured data. introduced generalization standard lstm tree-structured lstm showed better performances representing sentence meaning sequential lstm. slight variation lstm gated recurrent unit combines forget input gates single update gate. also merges cell state hidden state makes changes. resulting model simpler standard lstm model growing popularity. supposedly bidirectional lstm able deal long-range dependencies data. practice long-range dependencies still problematic handle. thus technique called attention mechanism proposed. attention mechanism neural networks inspired visual attention mechanism found humans. human visual attention able focus certain region image high resolution perceiving surrounding image resolution adjusting focal point time. attention mechanism allows model learn attend based input text produced rather encoding full source text fixed-length vector like standard lstm. bahdanau first utilized attention mechanism machine translation nlp. proposed encoder-decoder framework attention mechanism used select reference words original language words target language translation. figure illustrates attention mechanism bidirectional rnn. note decoder output word depends weighted combination input states last state normal case. weights define much input state weighted output. example value means decoder pays sentence. weights normally. weston introduced concept memory networks question answering problem. works several inference components combined large long-term memory. components neural networks. memory acts dynamic knowledge base. four learnable/inference components function follows component coverts incoming input internal feature representation; component updates memories given input; component generates output component converts output response format. instance given list sentences question question answering memnn finds evidences sentences generates answer. inference component reads sentence time encodes vector representation. component updates piece memory based current sentence representation. sentences processed memory matrix generated stores semantics sentences. question memnn encodes vector representation component uses vector select related evidences memory generates output vector. finally component takes output vector input outputs final response. based memnn sukhbaatar proposed end-to-end memory network neural network architecture recurrent attention mechanism long-term memory component trained end-to-end manner standard backpropagation. demonstrates multiple computational layers component uncover abstractive evidences single layer yield improved results question answering language modelling. worth noting computational layer content-based attention model. thus memnn refines attention mechanism extent. note also similar idea neural turing machines reported graves recursive neural network type neural network usually used learn directed acyclic graph structure data. recursive neural network seen generalization recurrent neural network. given structural representation sentence recnn recursively generates parent representations bottom-up fashion combining tokens produce representations phrases eventually whole sentence. sentence level representation used make final classification given input sentence. example process vector composition recnn shown figure vector node very interesting composed vectors node very node interesting. similarly node interesting composed phrase node very interesting word node ready survey deep learning applications sentiment analysis. that first briefly introduce main sentiment analysis tasks section. additional details please refer liu‚Äôs book sentiment analysis. researchers mainly studied sentiment analysis three levels granularity document level sentence level aspect level. document level sentiment classification classifies opinionated document expressing overall positive negative opinion. considers whole document basic information unit assumes document known opinionated contain opinions single entity sentence level sentiment classification classifies individual sentences document. however sentence cannot assumed opinionated. traditionally often first classifies sentence opinionated opinionated called subjectivity classification. resulting opinionated sentences classified expressing positive negative opinions. sentence level sentiment classification also formulated three-class classification problem classify sentence neutral positive negative. compared document level sentence level sentiment analysis aspect level sentiment analysis aspect-based sentiment analysis fine-grained. task extract summarize people‚Äôs opinions expressed entities aspects/features entities also called targets. example product review aims summarize positive negative opinions different aspects product respectively although general sentiment product could positive negative. whole task aspect-based sentiment analysis consists several subtasks aspect extraction entity extraction aspect sentiment classification. example sentence voice quality iphone great battery sucks entity extraction identify iphone entity aspect extraction identify voice quality battery aspects. aspect sentiment classification classify sentiment expressed voice quality iphone positive battery iphone negative. note simplicity algorithms aspect extraction entity extraction combined called aspect extraction sentiment/opinion target extraction. apart core tasks sentiment analysis also studies emotion analysis sarcasm detection multilingual sentiment analysis etc. liu‚Äôs book details. following sections survey deep learning applications sentiment analysis tasks. sentiment classification document level assign overall sentiment orientation/polarity opinion document i.e. determine whether document conveys overall positive negative opinion. setting binary classification task. also formulated regression task example infer overall rating score stars review. researchers also treat -class classification task. sentiment classification commonly regarded special case document classification. classification document representation plays important role reflect original information conveyed words sentences document. traditionally bag-of-words model used generate text representations text mining document regarded words. based document transformed numeric feature vector fixed length element word occurrence word frequency tf-idf score. dimension equals size vocabulary. document vector normally sparse since single document contains small number words vocabulary. early neural networks adopted feature settings. despite popularity disadvantages. firstly word order ignored means documents exactly representation long share words. bag-of-n-grams extension consider word order short context also suffers data sparsity high dimensionality. secondly barely encode semantics words. example words smart clever book equal distance smart closer clever book semantically. tackle shortcomings word embedding techniques based neural networks proposed generate dense vectors word representation extent able encode semantic syntactic properties words. word embeddings input words document representation dense vector derived using neural networks. notice addition approaches i.e. using learning dense vectors documents word embeddings also learn dense document vector directly bow. distinguish different approaches used related studies table documents properly represented sentiment classification conducted using variety neural network models following traditional supervised learning setting. cases neural networks used extract text features/text representations features non-neural classifiers obtain final global optimum classifier. properties neural networks complement advantages combined. besides sophisticated document/text representations researchers also leveraged characteristics data product reviews sentiment classification. product reviews several researchers found beneficial jointly model sentiment additional information classification. additionally since document often contains long dependency relations attention mechanism also frequently used document level sentiment classification. summarize existing techniques table moraes made empirical comparison support vector machines artificial neural networks document level sentiment classification demonstrated produced competitive results svm‚Äôs cases. overcome weakness mikolov proposed paragraph vector unsupervised learning algorithm learns vector representations variable-length texts sentences paragraphs documents. vector representations learned predicting surrounding words contexts sampled paragraph. glorot studied domain adaptation problem sentiment classification. proposed deep learning system based stacked denoising autoencoder sparse rectifier units perform unsupervised text feature/representation extraction using labeled unlabeled data. features highly beneficial domain adaption sentiment classifiers. zhai zhang introduced semi-supervised autoencoder considers sentiment information learning stage order obtain better document vectors sentiment classification. specifically model learns task-specific representation textual data relaxing loss function autoencoder bregman divergence also deriving discriminative loss function label information. johnson zhang proposed variant named bow-cnn employs bag-of-word conversion convolution layer. also designed model called seq-cnn keeps sequential information words concatenating one-hot vector multiple words. tang proposed neural network learn document representation consideration sentence relationships. first learns sentence representation lstm word embeddings. utilized adaptively encode semantics sentences inherent relations document representations sentiment classification. tang applied user representations product representations review classification. idea representations capture important global clues individual preferences users overall qualities products provide better text representations. chen also incorporated user information product information classification word sentence level attentions take account global user preference product characteristics word level semantic level. likewise used deep memory network capture user product information. proposed model divided separate parts. first part lstm applied learn document representation. second part deep memory network consisting multiple computational layers used predict review rating document. proposed cached lstm model capture overall semantic information long text. memory model divided several groups different forgetting rates. intuition enable memory groups forgetting rates capture global semantic features ones high forgetting rates learn local semantic features. yang proposed hierarchical attention network document level sentiment rating prediction reviews. model includes levels attention mechanisms word level sentence level allow model less attention individual words sentences constructing representation document. formulated document-level aspect-sentiment rating prediction task machine comprehension problem proposed hierarchical interactive attention-based model. specifically documents pseudo aspect-questions learn aspect-aware document representation. zhou designed attention-based lstm network cross-lingual sentiment classification document level. model consists attention-based lstms bilingual representation lstm also hierarchically structured. setting effectively adapts sentiment information resource-rich language resource-poor language helps improve sentiment classification performance. proposed adversarial memory network cross-domain sentiment classification transfer learning setting data source target domain modelled together. jointly trains networks sentiment classification domain classification sentence level sentiment classification determine sentiment expressed single given sentence. discussed earlier sentiment sentence inferred subjectivity classification polarity classification former classifies whether sentence subjective objective latter decides whether subjective sentence expresses negative positive sentiment. existing deep learning models sentence sentiment classification usually formulated joint three-way classification problem namely predict sentence positive neural negative. document level sentiment classification sentence representation produced neural networks also important sentence level sentiment classification. additionally since sentence usually short compared document syntactic semantic information used help. additional information review ratings social relationship cross-domain information considered too. example social relationships exploited discovering sentiments social media data tweets. early research parse trees used together original words input neural models sentiment composition better inferred. lately become popular need parse trees extract features sentences. instead word embeddings input already encode semantic syntactic information. moreover model architecture help learn intrinsic relationships words sentence too. related works introduced detail below. socher first proposed semi-supervised recursive autoencoders network sentence level sentiment classification obtains reduced dimensional vector representation sentence. later socher proposed matrix-vector recursive neural network word additionally associated matrix representation tree structure. tree structure obtained external parser. socher authors introduced recursive neural tensor network tensorbased compositional functions used better capture interactions elements. qian proposed advanced models tag-guided recursive neural network chooses composition function according part-of-speech tags phrase tagembedded recursive neural network recursive neural tenser network learns embeddings combines word embeddings together. kalchbrenner proposed dynamic semantic modelling sentences. dcnn uses dynamic k-max pooling operator non-linear subsampling function. feature graph induced network able capture word relations. also proposed sentence-level sentiment classification experimented several variants namely cnn-rand cnn-static cnn-non-static cnn-multichannel perform sentiment analysis short texts. wang utilized lstm twitter sentiment classification simulating interactions words compositional process. multiplicative operations word embeddings gate structures used provide flexibility produce better compositional results compared additive ones simple recurrent neural network. similar bidirectional unidirectional lstm extended bidirectional lstm allowing bidirectional connections hidden layer. wang described joint architecture sentiment classification short texts takes advantage coarse-grained local features generated long-distance dependencies learned rnn. guan employed weakly-supervised sentence level sentiment classification. contains two-step learning process first learns sentence representation weakly supervised overall review ratings uses sentence level labels finetuning. teng proposed context-sensitive lexicon-based method sentiment classification based simple weighted-sum model using bidirectional lstm learn sentiment strength intensification negation lexicon sentiments composing sentiment value sentence. jiang studied problem learning generalized sentence embeddings cross-domain sentence sentiment classification designed neural network model containing separated cnns jointly learn hidden feature representations labeled unlabeled data. zhao introduced recurrent random walk network learning approach sentiment classification opinionated tweets exploiting deep semantic representation user posted tweets social relationships. mishra utilized automatically extract cognitive features eye-movement data human readers reading text used enriched features along textual features sentiment classification. qian presented linguistically regularized lstm task. proposed model incorporates linguistic resources sentiment lexicon negation words intensity words lstm capture sentiment effect sentences accurately. different document level sentence level sentiment classification aspect level sentiment classification considers sentiment target information sentiment always target. mentioned earlier target usually entity entity aspect. simplicity entity aspect usually called aspect. given sentence target aspect aspect level sentiment classification aims infer sentiment polarity/orientation sentence toward target aspect. example sentence screen clear battery life short. sentiment positive target aspect screen negative target aspect battery life. discuss automated aspect target extraction next section. aspect level sentiment classification challenging modelling semantic relatedness target surrounding context words difficult. different context words different influences sentiment polarity sentence towards target. therefore necessary capture semantic connections target word context words building learning models using neural networks. three important tasks aspect level sentiment classification using neural networks. first task represent context target context means contextual words sentence document. issue similarly addressed using text representation approaches mentioned sections. second task generate target representation properly interact context. general solution learn target embedding similar word embedding. third task identify important sentiment context specified target. example sentence screen iphone clear batter life short clear important context word screen short important context battery life. task recently addressed attention mechanism. although many deep learning techniques proposed deal aspect level sentiment classification knowledge still dominating techniques literature. related works main focuses introduced below. dong proposed adaptive recursive neural network target-dependent twitter sentiment classification learns propagate sentiments words towards target depending context syntactic structure. uses representation root node features feeds softmax classifier predict distribution classes. zhang studied aspect-based twitter sentiment classification making rich automatic features additional features obtained using unsupervised learning methods. paper showed multiple embeddings multiple pooling functions sentiment lexicons offer rich sources feature information help achieve performance gains. since lstm capture semantic relations target context words flexible tang proposed target-dependent lstm target-connection lstm extend lstm taking target consideration. regarded given target feature concatenated context features aspect sentiment classification. ruder proposed hierarchical bidirectional lstm model aspect level sentiment classification able leverage intrainter-sentence relations. sole dependence sentences structures within review renders proposed model language-independent. word embeddings sentence-level bidirectional lstm. final states forward backward lstm concatenated together target embedding bidirectional review-level lstm. every time step output forward backward lstm concatenated final layer outputs probability distribution sentiments. considering limitation work dong zhang zhang proposed sentence level neural model address weakness pooling functions explicitly model tweet-level semantics. achieve that gated neural networks presented. first bidirectional gated neural network used connect words tweet pooling functions applied hidden layer instead words better representing target contexts. second three-way gated neural network structure used model interaction target mention surrounding contexts addressing limitations using gated neural network structures model syntax semantics enclosing tweet interaction surrounding contexts target respectively. gated neural networks shown reduce bias standard recurrent neural networks towards ends sequence better propagation gradients. wang proposed attention-based lstm method target embedding proven effective enforce neural model attend related part sentence. attention mechanism used enforce model attend important part sentence response specific aspect. likewise yang proposed attention-based bidirectional lstms improve classification performance. zhang extended attention modelling differentiating attention obtained left context right context given target/aspect. controlled attention contribution adding multiple gates. tang introduced end-to-end memory network aspect level sentiment classification employs attention mechanism external memory capture importance context word respect given target aspect. approach explicitly captures importance context word inferring sentiment polarity aspect. importance degree text representation calculated multiple computational layers neural attention model external memory. proposed neural network approach extract pieces input text rationales review ratings. model consists generator decoder. generator specifies distribution possible rationales encoder maps text task-specific target vector. multi-aspect sentiment analysis coordinate target vector represents response rating pertaining associated aspect. integrated target identification task sentiment classification task better model aspect-sentiment interaction. showed sentiment identification solved endto-end machine learning architecture sub-tasks interleaved deep memory network. signals produced target detection provide clues polarity classification reversely predicted polarity provides feedback identification targets. proposed interactive attention network considers attentions target context. uses attention networks interactively detect important words target expression/description important words full context. chen proposed utilize recurrent attention network better capture sentiment complicated contexts. achieve that proposed model uses recurrent/dynamic attention structure learns non-linear combination attention grus. designed dyadic memory network models dyadic interactions aspect context using either neural tensor compositions holographic compositions memory selection operation. perform aspect level sentiment classification needs aspects manually given automatically extracted. section discuss existing work automated aspect extraction sentence document using deep learning models. example state problem. example sentence image clear word image aspect term associated problem aspect categorization group aspect expressions category. instance aspect terms image photo picture grouped aspect category named image. review below include extraction aspect entity associated opinions. reason deep learning models helpful task that deep learning essentially good learning feature representations. aspect properly characterized feature space example hidden layer semantics correlation aspect context captured interplay corresponding feature representations. words deep learning provides possible approach automated feature engineering without human involvement. katiyar cardie investigated deep bidirectional lstms joint extraction opinion entities is-form is-about relationships connect entities. wang proposed joint model integrating conditional random fields co-extract aspects opinion terms expressions. proposed model learn high-level discriminative features double-propagate information aspect opinion terms simultaneously. wang proposed coupled multi-layer attention model co-extracting aspect opinion terms. model consists aspect attention opinion attention using units. improved lstm-based approach reported specifically aspect term extraction. consists three lstms lstms capturing aspect sentiment interactions. third lstm sentiment polarity information additional guidance. proposed attention-based model unsupervised aspect extraction. main intuition utilize attention mechanism focus aspect-related words deemphasizing aspect-irrelevant words learning aspect embeddings similar autoencoder framework. zhang extended model using neural network jointly extract aspects corresponding sentiments. proposed variant replaces original discrete features continuous word embeddings adds neural layer input output nodes. zhou proposed semi-supervised word embedding learning method obtain continuous word representations large reviews noisy labels. word vectors learned deeper hybrid features learned stacking word vectors neural network. finally logistic regression classifier trained hybrid features used predict aspect category. first learned word embedding considering dependency path connecting words. designed embedding features consider linear context dependency context information crf-based aspect term extraction. xiong proposed attention-based deep distance metric learning model group aspect phrases. attention-based model learn feature representation contexts. aspect phrase embedding context embedding used learn deep feature subspace metric kmeans clustering. poria proposed aspect extraction. developed seven-layer deep convolutional neural network word opinionated sentences either aspect nonaspect word. linguistic patterns also integrated model improvement. ying proposed rnn-based models cross-domain aspect extraction. first used rule-based methods generate auxiliary label sequence sentence. trained models using true labels auxiliary labels shows promising results. next sections discuss deep learning applications sentiment analysis related tasks. section focuses problem opinion expression extraction aims identify expressions sentiment sentence document. similar aspect extraction opinion expression extraction using deep learning models workable characteristics could identified feature space well. presented general class discriminative models based architecture word embedding. authors used pre-trained word embeddings three external sources different architectures including elman-type jordan-type lstm variations. wang proposed model integrating recursive neural networks co-extract aspect opinion terms. aforementioned cmla also proposed co-extraction aspect opinion terms. sentiment composition claims sentiment orientation opinion expression determined meaning constituents well grammatical structure. particular treestructure design recnn naturally suitable task. irsoy cardie reported recnn deep architecture accurately capture different aspects compositionality language benefits sentiment compositionality. proposed neural network integrating compositional non-compositional sentiment process sentiment composition. opinion holder extraction task recognizing holds opinion example sentence john hates opinion holder john. problem commonly formulated sequence labelling problem like opinion expression extraction aspect extraction. notice opinion holder either explicit implicit shown yang cardie. deng wiebe proposed word embeddings opinion expressions features recognizing sources participant opinions non-participant opinions source noun phrase writer. chen proposed content-based social influence model make opinion behaviour predictions twitter users. uses past tweets predict users‚Äô future opinions. based neural network framework encode user content social relation factor rashkin used lstms targeted sentiment forecast social media context. introduced multilingual connotation frames forecasting implied sentiments among world event participants engaged frame. clear word embeddings played important role deep learning based sentiment analysis models. also shown even without deep learning models word embeddings used features non-neural learning models various tasks. section thus specifically highlights word embeddings‚Äô contribution sentiment analysis. first present works sentiment-encoded word embeddings. sentiment analysis directly applying regular word methods like cbow skip-gram learn word embeddings context encounter problems words similar contexts opposite sentiment polarities mapped nearby vectors embedding space. therefore sentimentencoded word embedding methods proposed. mass learned word embeddings capture semantic sentiment information. bespalov showed n-gram model combined latent representation would produce suitable embedding sentiment classification. labutov lipson re-embed existing word embeddings logistic regression regarding sentiment supervision sentences regularization term. mikolov proposed concept paragraph vector first learn fixed-length representation variable-length pieces texts including sentences paragraphs documents. experimented sentence document-level sentiment classification tasks achieved performance gains demonstrates merit paragraph vectors capturing semantics help sentiment classification. tang presented models learn sentiment-specific word embeddings semantic also sentiment information embedded learned word vectors. wang developed neural architecture train sentimentbearing word embedding integrating sentiment supervision document word levels. adopted refinement strategy obtain joint semantic-sentiment bearing word vectors. feature enrichment multi-sense word embeddings also investigated sentiment analysis. zhang studied aspect-based twitter sentiment classification making rich automatic features additional features obtained using unsupervised learning techniques. jurafsky experimented utilization multi-sense word embeddings various tasks. experimental results show embeddings improve performance tasks offer little help sentiment classification tasks. proposed methods learn topic-enriched multi-prototype word embeddings twitter sentiment classification. multilinguistic word embeddings also applied sentiment analysis. zhou reported bilingual sentiment word embedding model cross-language sentiment classification. incorporates sentiment information english-chinese bilingual embeddings employing labeled corpora translation instead large-scale parallel corpora. barnes zhang integrated word embeddings matrix factorization personalized review-based rating prediction. specifically authors refine existing semantics-oriented word vectors using sentiment lexicons. sharma proposed semi-supervised technique sentiment bearing word embeddings ranking sentiment intensity adjectives. word embedding techniques also utilized improved help address various sentiment analysis tasks many recent studies. sarcasm form verbal irony closely related concept sentiment analysis. recently growing interest communities sarcasm detection. researchers attempted solve using deep learning techniques impressive success many problems. zhang constructed deep neural network model tweet sarcasm detection. network first uses bidirectional model capture syntactic semantic information tweets locally uses pooling neural network extract contextual features automatically history tweets detecting sarcastic tweets. joshi investigated word embeddings-based features sarcasm detection. experimented four past algorithms sarcasm detection augmented word embeddings features showed promising results. poria developed cnn-based model sarcasm detection jointly modelling pre-trained emotion sentiment personality features along textual information tweet. mishra utilized automatically extract cognitive features eye-movement data enrich information sarcasm detection. word embeddings also used irony recognition english tweets controversial words identification debates. emotions subjective feelings thoughts human beings. primary emotions include love surprise anger sadness fear. concept emotion closely related sentiment. example strength sentiment linked intensity certain emotion like anger. thus many deep learning models also applied emotion analysis following sentiment analysis. wang built bilingual attention network model code-switched emotion prediction. lstm model used construct document level representation post attention mechanism employed capture informative words monolingual bilingual contexts. zhou proposed emotional chatting machine model emotion influence largescale conversation generation based gru. technique also applied papers. abdul-mageed ungar first built large dataset emotion detection automatically using distant supervision used network fine-grained emotion detection. question-answering approach proposed using deep memory network emotion cause extraction. emotion cause extraction aims identify reasons behind certain emotion expressed text. multimodal data data carrying textual visual acoustic information used help sentiment analysis provides additional sentiment signals traditional text features. since deep learning models inputs latent space feature representation inputs multimodal data also projected simultaneously learn multimodal data fusion example using feature concatenation joint latent space sophisticated fusion approaches. growing trend using multimodal data deep learning techniques. poria proposed extracting features short texts based activation values inner layer cnn. main novelty paper deep extract features text multiple kernel learning classify heterogeneous multimodal fused feature vectors. fung demonstrated virtual interaction dialogue system incorporated sentiment emotion personality recognition capabilities trained deep learning models. wang reported structured deep network named deep coupled adjective noun neural network visual sentiment classification. idea dcan harness adjective noun text descriptions treating supervision signals learn intermediate sentiment representations. learned representations concatenated used sentiment classification. proposed unified cnn-rnn model visual emotion recognition. architecture leverages multiple layers extract different levels features within multi-task learning framework. bidirectional proposed integrate learned features different layers model. adopted attention mechanism visual sentiment analysis jointly discover relevant local image regions build sentiment classifier local regions. poria proposed deep learning model multi-modal sentiment analysis emotion recognition video data. particularly lstm-based model proposed utterance-level tripathi used deep cnn-based models emotion classification multimodal dataset deap contains electroencephalogram peripheral physiological video signals. zadeh formulated problem multimodal sentiment analysis modelling intra-modality inter-modality dynamics introduced neural model named tensor fusion network tackle long proposed attention neural model trained cognition grounded eye-tracking data sentence-level sentiment classification. cognition based attention layer built neural sentiment analysis. wang proposed select-additive learning approach tackle confounding factor problem latent representations learned neural networks achieve learning phases involved namely selection phase confounding factor identification removal phase confounding factor removal. recently sentiment analysis resource-poor languages also achieved significant progress deep learning models. additionally multilingual features also help sentiment analysis like multimodal data. deep learning applied multilingual sentiment analysis setting. singhal bhattacharyya designed solution multilingual sentiment classification review/sentence level experimented multiple languages including hindi marathi russian dutch french spanish italian german portuguese. authors applied machine translation tools translate languages english used english word embeddings polarities sentiment lexicon model classification. sentiment intersubjectivity tackled intersubjectivity problem sentiment analysis problem study surface form language corresponding abstract concepts incorporate modelling intersubjectivity proposed cnn. financial volatility prediction rekabsaz made volatility predictions using financial disclosure sentiment word embedding-based information retrieval models word embeddings used similar word expansion. opinion recommendation wang zhang introduced task opinion recommendation aims generate customized review score product particular user likely give well customized review user would written target product user reviewed product. multiple-attention memory network proposed tackle problem considers users‚Äô reviews product‚Äôs reviews users‚Äô neighbours stance detection augenstein proposed bidirectional lstms conditional encoding mechanism stance detection political twitter data. designed target-specific neural attention model stance classification. applying deep learning sentiment analysis become popular research topic lately. paper introduced various deep learning architectures applications sentiment analysis. many deep learning techniques shown state-of-the-art results various sentiment analysis tasks. advances deep learning research applications believe exciting research deep learning sentiment analysis near future.", "year": 2018}