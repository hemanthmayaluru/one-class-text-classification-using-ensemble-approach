{"title": "Improving the Performance of PieceWise Linear Separation Incremental  Algorithms for Practical Hardware Implementations", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "In this paper we shall review the common problems associated with Piecewise Linear Separation incremental algorithms. This kind of neural models yield poor performances when dealing with some classification problems, due to the evolving schemes used to construct the resulting networks. So as to avoid this undesirable behavior we shall propose a modification criterion. It is based upon the definition of a function which will provide information about the quality of the network growth process during the learning phase. This function is evaluated periodically as the network structure evolves, and will permit, as we shall show through exhaustive benchmarks, to considerably improve the performance(measured in terms of network complexity and generalization capabilities) offered by the networks generated by these incremental models.", "text": "abstract. paper shall review common problems associated piecewise linear separation incremental algorithms. kind neural models yield poor performances dealing classification problems evolving schemes used construct resulting networks. avoid undesirable behavior shall propose modification criterion. based upon definition function provide information quality network growth process learning phase. function evaluated periodically network structure evolves permit shall show exhaustive benchmarks performance offered networks generated incremental models. last years substantial effort field artificial neural networks’ theory devoted study development incremental neural networks models important feature kind neural models ability determine proper network structure handle particular task. pointed several types incremental algorithms shall concentrate piecewise linear separation models fact present compexity learning recall phases thus well suited vlsi implementations. models used mainly classification tasks starting network composed neuron find incremental discriminant function able separate categories defined input space. discriminant function obtained combining linear discriminant functions associated perceptron-like units generated training process. paper shall first briefly review methods used training individual units generated models well usual drawbacks posed algorithms. shall present novel modification criterion applied incrementals models. method permit given desired generalization error starting parameter construct appropiate network meet specification allowing time substantial reduction size generated network structures. afterwards shall present comparative exhaustive simulation study classification performance method proposed paper applied particular model neural trees algorithm finally conclusions future work related proposed criterion outlined. perceptron pocket common training algorithms used units generated incremental models. learning process completed unit weight vector vector ideally yields best correct classification rate accordance input training set. however optimizing function accounts number patterns correctly classified impose erroneus scheme incremental algorithm that stated network structure infinite size generated evolving process. several methods already proposed order allow correct network evolving process among them initially adopted improvement method proposed consist running pocket algorithm modification following conditions weight vector stored best weight vector sides separating hyperplane defined weight vector empty correct classification rate provided weight vector larger provided best weight vector stored previously. nevertheless method generates quite complex network structures rather poor generalization capabilities. furthermore network structure depends order inputs patterns presented network training process. hand empirical observations carried artificial well real databases demonstrated large percentage units generated algorithm used precise establishment boundaries used separate categories defined input space fact models present serious difficulties solving problems exists high degree overlapping classes consequence produce large amount units trying separate distributions patterns close other. taking account considerations stated above shall define function calculated periodically training process detects network begins problems determining separation borders. permit stop growth network generates units hardly provide information problem solved. without loss generality consider problem consist separatig classes denoted hereafter class class define class-i centroid vector whose components obtained calculating mean value components vectors represent patterns belonging class. deduced function present maximum values network begins problems determining separation borders categories. reason calculating auxiliary functions inverse distance components patterns components centroids emphasize fact. hand auxiliary functions normalized depend number patterns. moreover case centroids exist corresponding function value zero. initial value learning process started thus figure depicts evolution function trying solve classification problem stated phoneme database provided roars esprit project. database distinguish nasal oral vowels coming isolated syllabes. vector constituting database characterized five features corresponding first harmonics normalized total energy. database composed vectors class class algorithm used train network neural trees algorithm belongs category models. algorithm tries solve certain classification task dividing iteratively initial problem subsequent reduced versions used training sets units generated network construction process. consequence algorithm produces finally network structure resembles binary decision tree. fig. represents value function level tree generated incremental algorithm. unit training principle pocket algorithm modification criterion mentioned previously. number iterations unit evolution function depicted fig. suggest stopping criterion network growth process. consist freezing network construction procedure relative peak function arises. reference value function training process started. thus penalty/merit functions order maximize quotient generalization network complexity shall improve overall performance provided network constructed incremental models. generalization given network complexity network constant parameters gmax cmax determined using theoretical results obtained baumm haussler concerning probability poor generalization. specifically showed least training examples needed obtain generalization error less number weights network. case neural trees algorithm previously described lower bound number patterns leads following expression number patterns training generalization error dimension input space. thus completely characterized expression since related maximum complexity network cmax generalization error hence value generalization error fixed parameters gmax cmax function determided automatically. setting value generalization error automatically determined parameters gmax cmax function start network evolving scheme imposed particular model. network construction process evaluate periodically function order detect peaks amplitud times greater initial value. function presents peak value calculate current value function value greater calculated previous maximum updated network construction process still allowed. otherwise network evolution stopped point. important note election parameter critical. choose small value network growth process stopped early even network size enough providing satisfactory correct classification rate leading misclassifications resulting network structure rather poor generalization capabilities. hand choose arbitrary large value parameter complexity generated network structures increase fact maximum values given function never reach value given parameter furthermore even case could choose optimal value standard deviation measured terms complexity structures generated training phase would quite large consequence behavior function maximum values presented function highly dependents particular evolution followed network learning phase specifically order training patterns presented network. therefore order avoid expression shows mentioned relationship parameter network complexity simple parameter take large values complexity network remains small decreasing value extent network complexity grows thus proposed evolving scheme generate structures complexity around optimal value given cmax next section shall present comparative simulation study classification performance neural trees algorithm neural trees algorithm proposed method. gauss gauss gauss gauss four databases composed vectors belonging normal distributed classes dimensions respectively. distributions mean different variance. rectangular composed bidimensional vectors belonging class belonging class distributed following bidimensional distributions square overlapping. clouds database consists vectors belonging different classes vectors class. first class obtained three different gaussian distributions second class corresponds single normal distribution. experiments carried using leave-k-out cross-validation procedure original database divided equal sized parts network trained nine parts tested remaining part total training-test sets obtained database. process repeated times partition finally total amount sixty evolving processes performed database. table shows results indicated mean number units generated generalization percentage provided neural trees algorithm classification tasks stated previously. table reproduces results provided neural trees algorithm network evolving process modified criterion proposed paper. experiments desired generalization error given theoretical bayes limit. case phoneme database tested differents values generalization error since bayesian limit known. deduced comparison tables results provided proposed method represents substantial reduction number units generated algorithm also meaningful improvement generalization capability. however simulation results corresponding clouds gaussian database illustrate difficulty obtaining generalization errors close bayesian limit. case gaussian database behavior explained inherent sparseness high dimensional training data better understanding effect important note gaussian database number training patterns dimensions. hence statistical point view exists amount enough training samples order estimate probability density distribution. moreover linear smoothers generally insufficient data high dimensional spaces reliably estimate probability density distribution. case clouds database previously commented difficulties related bayes limit observed also substantial looseness generalization error respect given table point helpful note lower bound given previous section made assumption network large number weights. nevertheless fact constitutes main reason discouraging results since working conditions mentioned hypothesis fail. natural overcome limitation upper bound number patterns given baum haussler. specifically showed error training less order examples needed obtain generalization error less number weights network number threshold units. case neural trees algorithm following expression straightforward follow number patterns dimension input space cmax complexity network. however value parameter cmax difficult compute non-linearity equation derived above. table shows results provided neural trees algorithm network evolving process modified criterion expounded section together previously described heuristic procedure. observation results clear proposed heuristic overcome aforementioned difficulties. hand clouds database expected appreciated meaningful increase generalization capability. proposed criterion able reduce network complexity generated incremental algorithm thus facilitating eventual hardware implementation classifier evolved incremental model. furthermore standard deviation number units generated considerably reduced minimizing influence vector presentation order training phase. shown paper linear discriminant solutions provided usual training algorithms always useful evolving network structures generated incremental models. reviewing modification proposal training algorithms problems posed network evolving scheme associated models introduced novel approach improving performance models. modification criterion able stop network construction process improvement obtained adding units network. furthermore method permits construct automatically proper network structure given classification task input parameter generalization capability expected resulting classifier. consequence proposed method produces compact network structures given problem handled thus facilitating eventual hardware implementation together expected improvement generalization capabilities resulting networks method able reduce influence vector presentation order training phase network evolving process. current work concentrated comparing proposed method criterions proposed recently aimed also selecting proper network structure able handle given classification task. contemplate also possibility apply method incremental models aimed solving regression tasks. kwok d.y. yeung \"constructive feedforward neural networks regression problems survey\". technical report hkust-cs-. hong kong university science technology j.m. moreno castillo cabestany \"optimized learning improving evolution piecewise linear separation incremental algorithms\". trends neural computation. mira cabestany prieto pps. springer-verlag j.m. moreno castillo cabestany \"improving piecewise linear separation incremental algorithms using complexity reduction methods\". proc. european symposium artificial neural networks esann’ pps. murata yoshizawa s.-i. amari \"network information criterion-determining number hidden units artificial neural network model\". ieee trans. neural networks vol. pps. november", "year": 2007}