{"title": "Multi-Labelled Value Networks for Computer Go", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper proposes a new approach to a novel value network architecture for the game Go, called a multi-labelled (ML) value network. In the ML value network, different values (win rates) are trained simultaneously for different settings of komi, a compensation given to balance the initiative of playing first. The ML value network has three advantages, (a) it outputs values for different komi, (b) it supports dynamic komi, and (c) it lowers the mean squared error (MSE). This paper also proposes a new dynamic komi method to improve game-playing strength. This paper also performs experiments to demonstrate the merits of the architecture. First, the MSE of the ML value network is generally lower than the value network alone. Second, the program based on the ML value network wins by a rate of 67.6% against the program based on the value network alone. Third, the program with the proposed dynamic komi method significantly improves the playing strength over the baseline that does not use dynamic komi, especially for handicap games. To our knowledge, up to date, no handicap games have been played openly by programs using value networks. This paper provides these programs with a useful approach to playing handicap games.", "text": "abstract paper proposes approach novel value network architecture game called multi-labelled value network. value network different values trained simultaneously different settings komi compensation given balance initiative playing first. value network three advantages outputs values different komi supports dynamic komi lowers mean squared error paper also proposes dynamic komi method improve game-playing strength. paper also performs experiments demonstrate merits architecture. first value network generally lower value network alone. second program based value network wins rate program based value network alone. third program proposed dynamic komi method significantly improves playing strength baseline dynamic komi especially handicap games. knowledge date handicap games played openly programs using value networks. paper provides programs useful approach playing handicap games. player perfect information game originated years china. game simple learn difficult master. players black white alternately place stone player‚Äôs color empty intersection referred points paper board usually board players. normally black plays first. player capture opponent stones surrounding them make territory surrounding empty points. goal players occupy many points possible. game ends players occupy territory. agreement game often indicated choosing pass consecutively player‚Äôs turn. game ends territory difference calculated black‚Äôs territory minus white‚Äôs. compensate initiative black playing first certain number points so-called komi added white balancing game. game‚Äôs score territory difference minus komi. black wins score positive white wins otherwise. komi customarily according chinese rules japanese rules. although rules simple game tree complexity extremely high estimated common players different strengths play ‚Ñé-stone handicap games weaker player usually designated play black allowed place stones first komi white makes first move. strength difference players large handicap stones usually given weaker player. past computer listed grand challenges strengths computer programs generally away amateur players. monte carlo tree search invented computer programs started making significant progress roughly grand challenge achieved program alphago defeated sedol grandmaster world champion titles past decade. many thought time would decade away surpassing milestone. date deepmind team behind alphago published techniques methods alphago nature alphago able surpass experts‚Äô expectations proposing method uses three deep convolutional neural networks supervised learning policy network learning predict experts‚Äô moves human expert game records reinforcement learning policy network improving policy network self-play value network performs state evaluation based self-play game simulations. alphago combined dcnns mcts move generation game play. mcts fast rollout policy used compute state evaluations past. alphago uses combination fast rollout policy value network state evaluation. alphago policy network value network trained based komi conform computer competitions. games played human players game servers tygem komi however since policy network value network trained komi slightly less accuracy expected playing games different komi. naive solution would retrain another policy network value network komi needed. could proposed handicap games komi however cost training policy network value network high finite amount work supported part ministry science technology republic china contracts -e---my --e---my --e--. authors department computer science national chiao remainder paper organized follows. section reviews past work policy networks value network mcts dynamic komi. section presents network architecture dynamic komi method. section experiments analyze demonstrate merits architecture method. finally section draws conclusions. section reviews sl/rl policy network value network subsection ii.a describes combination dcnns mcts subsection ii.b introduces board evaluation network subsection ii.c discusses dynamic komi technique mcts subsection ii.d. goal supervised learning policy network predict possible moves human experts given position. prediction human expert moves useful selecting moves likely played thereby reducing branching factor significantly. networks served action selection referred policy networks alphago‚Äôs terminology. development alphago million human expert positions moves collected state-action pairs server train -layer policy network positions moves corresponding training state-action pairs randomly sampled among collected pairs; sampled state-action pair used maximize likelihood expert move given state using gradient ascent. training procedure described followed equation weights policy network probability taking action state following policy network. policy network alphago input channels filters able achieve prediction rate input features. improve policy network alphago trained policy network self-play reinforcement learning since policy network built basis policy network structure identical policy network policy network weights initialized values policy network. initially. training process started listing policy pool policy network pool initially. iteration random opponent chosen pool games played. game results used update network policy gradient. iterations current policy added opponent pool. described process followed reinforce algorithm summarized below game results modified baseline value first round self-play baseline value zero. second round value network described next paragraph baseline. alphago able achieve slightly better problem currently value networks trained based komi often used play .-komi games. practice games special category outcome would different komi unfortunately unlikely happened real games; e.g. program deepzengo‚Äôs losing match world championship yuting players world ranking date highest ranking third strongest player. time authors deepzengo proposed trick offset expected inaccuracies setting komi rollout which combined evaluation value network trained komi balanced program behave komi solve problem elegantly paper proposes approach design multi-labelled value network modified value network architecture. value network able train rates different komi settings incorporate board simultaneously. moreover also evaluation value network named bv-ml value network. useful expresses point ownership player probability exists estimate point board. estimation ultimately correlates multi-labelled values. approaches value network offer games different komi also support dynamic komi improve game playing strength. paper also proposes dynamic komi method based bv-ml value network improve strength significantly especially handicap games. program based bv-ml value network wins rate program based value network alone. shows bv-ml value network significantly improves playing strength. propose method measure prediction final score value networks experimental results show high confidence predicated score. hints predicted score used dynamic komi. proposed dynamic komi method improves playing strength following experimental results. program dynamic komi method wins baseline rates respectively -stone -stone -stone -stone handicap games. contrast program without dynamic komi method wins analyze effect using bv-ml value network trained .-komi games play .-komi games. experiments demonstrate bv-ml value network slightly improves value network without goal value network estimate value given game position. theoretically exists optimal value function given perfect play. practical situations since perfect play possible alphago attempted arrive estimated value function estimated value function based performing policy using approximated network weights network value network‚Äôs structure similar policy network small difference value network outputs single scalar prediction value given position policy network outputs probability distribution. overfitting major concern since number human expert game records limited. overcome potential problem alphago uses policy network create million game records self-play. self-play game performed playing policy network itself. monte carlo tree search best-first search algorithm search tree using monte carlo rollouts estimate state values. tree policy traditionally upper confidence bounds function used select positions root leaves. leaf position chosen children expanded selected leaf position. alphago used puct algorithm select states tree applied policy network value network mcts asynchronous updates. policy network used mcts expansion mcts takes probability outputs child states expanded network uses initial values puct algorithm. evaluation consists parts value network output monte carlo rollout. first part value network used evaluation leaf positions second part monte carlo rollouts performed starting game. compared value network rollouts generate estimated values quickly less accuracy. mcts selection value position calculated value combining results value network rollout weighting parameter alphago. minimize end-to-end evaluation time alphago uses minibatch size besides value network another neural network territory called board evaluation network proposed previously number outputs network number board points. output value corresponds board point indicating probability point belongs black endgame network‚Äôs output layer uses sigmoid activation network‚Äôs objective minimize predicted probabilities actual endgame ownership board point network uses input channels layers filters layer using gogo dataset training data. date none exploited idea except discussions given traditional computer programs mcts dynamic komi technique widely used make program play aggressively especially handicap games. known deficiencies mcts-based programs inability handle extremely favorable unfavorable situations. mcts works maximizing winning probability instead considering score margins players outcome game already apparent little incentive either strengthen one‚Äôs advantage winning catch minimize one‚Äôs losses losing. methods adjust komi dynamically providing incentive play better extreme conditions. first so-called score-based situational method adjusts komi based expected score rollouts specific amount monte carlo simulations. komi game‚Äôs komi. order aggressive komi value called komi rate. komi rate higher near beginning game lower near end. beginning game rates less precise komi adjustments allow program play proactively even program seems winning losing decisively. near game rates precise less need adjust komi significantly frequently. another value-based situational method adjusts komi dynamically specific amount monte carlo simulations rate root tree falls predefined interval least close interval possible. namely within interval komi adjusted. higher increase komi lower decrease one. criterion also used avoid oscillation komi values especially endgame. methods specific amount monte carlo simulations adjustment either amount simulations needed single move specific number rollouts e.g. latter komi adjusted faster incur inconsistent rates directly impacted different komi values. previous research reported work fine latter notable program chose former author also mentioned possibility increasing decreasing using method. however detailed methods experiments reported. figure network architecture bv-ml-vn. network layers. input feature planes calculated given position. outputs. output multi-labelled value range komi values output board evaluation indicates ownership point given position. figure position game illustration. game position black shadows indicates black‚Äôs territory. ownership output inference bv-ml value nework position sizes black squares indicate confidence ownership black general policy networks follow alphago‚Äôs slight modifications follows. addition input channels used alphago simply input channel feature indicating position current game state. side note trained policy network -step prediction proposed training data combine samples game records played game server open game records played professional players collected gogod training results policy network prediction rate reached testing data gogod testing data. purposes paper policy network went round training without value network provide baseline value training sl/rl policy networks policy network generate million self-play games. self-play game ends ownerships points determined instead ending soon side won. addition evaluating wins/losses also need know number winning points training sample contains rates different komi settings well final territory paper proposes design value network named multi-labelled value network. alphago‚Äôs value network output indicating value given position namely rate position komi approach value network includes outputs indicating value position ùëò-komi games. simplicity discussion rates black remainder paper. rules game full outputs however simplicity consider centered design. value network shown figure excluding dashed layers last layers fully connected layers first rectifier units second rectifier units finally connected tanh unit output similar alphago‚Äôs value network value network also adds input channel indicate player color. training data label output follows. self-play game first calculate territory difference game. then based chinese rule label example black occupies points territory white ùëò-komi game considered loss thus case .-komi game loss .-komi .-komi game win. table illustrates labelling position figure game ending position figure black owns points territory white. thus outcome labeled table second column lists labelled values above third column lists value inference position figure value network. values decrease increases since rate ùëò-komi higher ùëò+-komi. table also shows interesting result every rates value network output namely ùë£ùëò‚àí. ùë£ùëò+. integer close value. phenomenon assuming points occupied either black white game ends territory difference must number. possible case furthermore incorporate value network refer board evaluation multi-labelled value network follows. bv-ml value network additional networks added value network shown dashed figure network dashed shares input channels layers value network. there output layer connected additional convolution layer connected sigmoid layer outputting predicted ownership point board. value network without multi-labelled component referred value network. illustrate position shown figure position shown figure self-play game starting position figure consider points coordinate occupied black white game output labelled labelled ownership outputs inference bv-ml value network shown figure also apply value networks computer program intelligence placed mcts method basically follows alphago‚Äôs slight modifications follows. different alphago using mini-batch size minimize end-toend evaluation time larger batch size reason simply speed performance increasing throughput. addition also proposed implemented dynamic komi methods described next subsection. method dynamic komi adjusted based territory root node generated rollouts board evaluation network two. ss-b ss-m represent method using three kinds territory generations respectively. ss-m weighting parameter used like value mixed rollouts value network formula komi rate method current dynamic komi next dynamic komi adjusted based rate root node rate rollouts value komi value network. vsrepresent modified method. similarly parameter also note consider values namely rollouts alone value alone. japanese rules since possible game without counting last played moves chance even territory differences higher. details japanese rules available playing strengths. subsection iv.b analyze mses networks games different komi values verify advantage multi-labelled value networks. accuracy number winning points bv-ml value network presented analyzed subsection iv.c. subsection iv.d discusses different dynamic komi techniques mcts different handicap games. subsection iv.e analyzes performance difference using play .komi games. finally analyze correlation value network subsection implement four different value networks value network value network only value network bv-ml value network. simplicity denoted respectively bv-vn ml-vn bv-ml-vn. updating loss weight ratio network trained convergence four gpus takes weeks each. measure quality value networks analyze mses like alphago did. three sets games used benchmark least players higher. three sets .-komi games chosen random .-komi games .-komi games. sets listed consists games. ùëñ-th game assume result game black otherwise. denote ùëó-th position ùëñ-th game denote value obtained inputting position given value network. ùëó-th positions games calculated following formula. maintained. like method want mixed rates dynamic komi fall predefined interval least close interval possible. game‚Äôs komi. within interval next dynamic komi still next move. interval without loss generality locate closest komi then next dynamic komi komi rate. example next dynamic komi would adjusted assuming ordinal number current move play; mixed rate root komi real komi game; total points whole board games; parameters decide different komi rate; contending interval ùëâùëéùëôùë¢ùëí locate komi dynamic komi routine named ml-based dynamic komi contains parameters listed lines target komi determined lines komi rate decreases small amount games progress according formula lines next dynamic komi calculated based komi rate line komi rate depends parameters respectively paper. figure shows komi rate changes game progresses. section experiments done machines equipped four gpus intel xeon memory linux. subsection iv.a introduce four different network architectures measure mean squared errors fairness .-komi games testing data mses four value networks shown figure since numbers close also show average mses table results show mlvn bv-ml-vn lower mses bv-vn. figure interesting bv-ml-vn much higher bv-vn early stage games namely bv-ml-vn versus bv-vn beginning games. initial games nearly even namely .-komi games since trained .-komi games thus values bv-vn bv-ml-vn based this komi changed implies game favors black rate black beginning reality observed rate black bv-ml-vn. however collected .-komi games usually played players different ranks white often higher black. collected games found black obtained rate resulted even higher mse. however moves bvml-vn becomes lower bv-vn‚Äôs. bv-vn always uses komi estimate rates resulting progressively inaccurate evaluation game goes statistically .-komi games results b+.; game results range interesting different outcomes circumstances. namely considered winning apply four value networks respectively computer program compare strengths among networks results shown table pair programs games played s/move cores used program. results shows programs bv-ml-vn bv-vn ml-vn significantly outperform program only bv-ml-vn performs best. although bv-vn perform better ml-vn/bv-ml-vn terms strength bv-vn nearly equal ml-vn close bv-ml-vn. reason experiments remaining section based bv-ml-vn. measure network quality different komi games analyze data sets .-komi .-komi games. since bv-vn nearly ml-vn bv-ml-vn nearly show bv-vn bv-ml-vn simplicity. first analyze value networks .-komi games. bv-vn since output contains label used estimate rates games. bv-ml-vn value estimate rates games instead. figure depicts bv-vn bv-ml-vn .-komi games. bv-ml-vn slightly lower bv-vn difference much since .-komi games ended black winning points note .-komi games losses games played komi points. statistics among collected games .komi games ends b+.. since small portion games programs value networks prediction rates generally grows higher game progresses. increases -prediction rates also increase. figure observe lines tend pair i.e. ùëë-prediction rates tend close. reason similar given paired consecutive rates table prediction rates initial stage increase near game. three factors negatively impact prediction rate. first human players consistently chosen sub-optimal moves miscalculation decided play conservatively secure victory resulting less optimal values. second network able predict accurately. third uses japanese rules cause point difference games. prediction rates slightly higher initial stage increase near game. prediction rates significantly improve beginning game near end. observed prediction rates initial stage grow near end. analysis shows high confidence short prediction distance even three negative factors pointed previous paragraphs. finding hints accuracy dynamic komi; experiments dynamic komi described subsection iv.d. subsection analyzes performances different dynamic komi methods namely dynamic komi ss-r ss-b ss-m vs-m ml-dk described subsection iii.b. vs-m ml-dk rate interval ss-* ml-dk choose setting komi rates letting parameters dynamic komi methods incorporated program bv-ml-vn. experiment baseline choose version early program participated cup. dynamic komi method program method play baseline games played boards second move. since bv-ml-vn supports one-stone handicap valued points difference according experiments analyze -stone -stone handicaps addition even games. komi handicap games even games. played komi losing komi calculation error rate results larger values shown figure contrast bv-ml-vn converges since bv-ml-vn accurately estimate situation based output subsection analyzes accuracy winning points bv-ml-vn. previously label bv-vn outputs estimate rate. multiple labels different values komi bv-ml-vn instead able evaluate rates komi. extra information estimate point difference players. analysis first locate komi then expect black komi smaller equal lose komi larger equal mentioned komi rules section territory differences real games integers. black leads without komi. example table since bv-ml-vn expects black lead points without komi. since game komi network predicts position corresponding table. outcome real game i.e. black leads points without komi. simplicity discussion remainder subsection network predicts black leading points real outcome game black leading points. difference point predicted network actual outcome game called prediction distance. example prediction distance want analyze accuracy value network following manner. first .-komi games choose games ending black leading points since bvml-vn trained multiple labels game said -predict move correctly prediction distance ùëñ-th position game smaller equal consequently ùëë-prediction rates move percentage games ùëë-predict move correctly. figure shows ùëë-prediction rates next consider case available. since multi-labelled values available dynamic methods applied rollouts. table shows results previous best methods using ss-r ml-dk. results table clearly lower methods table showing multi-labelled values clearly superior available. subsection perform experiments demonstrate analyze effect using value network trained .-komi games play .-komi games. first black-winning game score .-komi game actually white-winning game score .-komi game. therefore among .-komi games collected games ending score analysis. experiment tries investigate whether games using bv-ml-vn denoted vn-. helps contrast value network trained .-komi games denoted vn-.. simplicity vn-. actually bv-ml-vn‚Äôs output instead normal value network trained .-komi games. reduce human mistakes choose position game starting position game. fairness game played twice programs; program play black white exactly game. move programs given second. experimental result shows rate .¬±.% games. among games games times vn-.. implies using matter though much expected following reasons. first games human players still often miscalculate make blunders even final moves. therefore games actually score .-komi games. second rollout often sufficient obtaining reasonable rate remedies greatly reduces problem using .-komi especially games. illustrate case black lead points optimal play; position winning game‚Äôs komi losing assume black move. moves result lead points called optimal moves. general based .-komi games evaluate moves rate nearly situation long rollout plays accurately mixed value still differentiate table shows results methods -stone -stone handicap games denoted respectively baseline playing white. generally ss-r mldk clearly outperforms methods. ml-dk performs better ss-r performs better considering even handicapped games altogether ml-dk appears perform slightly better ss-r. ss-* methods regardless using either rollout territory estimation show improvement using dynamic komi. best method seems ss-r ss-m seems worst among three ss-* methods. conjecture case territory estimates using rollout conflicting. second method performed best experiments vs-m method perform well. fact performs nearly dynamic komi used all. observed adjustment dynamic komi using rollouts relatively slow compared bv-mlvn. figure shows rates rollout bv-ml-vn respect different komi values beginning game figure value bv-ml-vn sensitive komi value change empirically accurate shown subsection iv.c value rollout less sensitive komi changes. thus vs-m method adjusts time komi changes slow handle values obtained value networks. since rollout used performed relatively well. optimal moves others. thus program tend play optimal moves. situation similar assuming white play discussion omitted. hence programs find using networks trained .-komi games sufficient. subsection correlation bv-ml-vn depicted scatter plot figure figure represents position positions chosen .-komi games random simplicity. y-axis value corresponding position x-axis indicates territory based namely ownership probabilities board points outputs figure shows correlated general indicated regressed line centered around dots right vertical line values higher dots left values lower dots indicate positive correlation board ownership rate hand dots lower right upper left indicate native correlation. paper proposes approach value network architecture game called multi-labelled value network. value network three advantages offering different value settings komi supporting dynamic komi lowering mean squared error addition board evaluation also incorporated value network help slightly improve game-playing strength. metric called ùëë-prediction rate devised measure quality final score prediction value networks. experimental results show high confidence predicted scores. shows predicted score used dynamic komi. dynamic komi methods designed support value network improved game-playing strength program especially handicap games. program using bv-ml value network outperforms using value networks. significantly wins baseline program value network rate programs using either value network value network also significantly outperform baseline. bv-ml value network experiments show -prediction rates follows. prediction rates initial stage game growing near end. increases ùëëprediction rates also increase. example prediction rates significantly improve initial stage growing near end. results show high confidence short prediction distance even subsection iv.c. experiments show proposed ml-dk method improves playing strength. program ml-dk wins baseline rates -stone -stone -stone -stone handicap games respectively. contrast program without using dynamic komi methods wins also investigate dynamic komi methods ss-r ss-b ss-m vs-m. among methods ss-r performs best. ss-r also performs better ml-dk -stone -stone handicap games conditions. figure shows another scatter plot .-komi games. difference figure figure value y-axis namely y-axis value figure interestingly dots shifted slightly left centered around line dots lower right upper left indicate native correlation. above bv-ml value network proposed paper easily flexibly generalized different komi games. especially important playing handicap games. knowledge date handicap games played openly programs utilizing value networks. expected interest handicap games played programs grow especially programs continually grow stronger. paper provides programs useful approach playing handicap games. graf platzner using deep convolutional neural networks monte carlo tree search. international conference computers games springer international publishing june mnih kavukcuoglu silver rusu a.a. veness bellemare m.g. graves riedmiller fidjeland a.k. ostrovski petersen human-level control deep reinforcement learning. nature pp.- browne c.b. powley whitehouse lucas s.m. cowling p.i. rohlfshagen tavener perez samothrakis colton survey monte carlo tree search methods. ieee transactions computational intelligence games pp.- coulom r√©mi. efficient selectivity backup operators monte carlo tree search. international conference computers games. springer berlin heidelberg enzenberger muller arneson segal fuego‚Äîan opensource framework board games engine based monte carlo tree search. ieee transactions computational intelligence games pp.- silver huang maddison c.j. guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman mastering game deep neural networks tree search. nature pp.- sutton r.s. mcallester d.a. singh s.p. mansour policy gradient methods reinforcement learning function approximation. advances neural information processing systems vol. november yamashita message computer-go mailing list dynamic komi's basics. available https//groups.google.com/forum/msg/computer-goarchive/k-fskrygly/szjezvsjgj february ti-rong currently ph.d. candidate department computer science national chiao tung university. research interests include machine learning deep learning computer games. hung-chun currently master student department computer science national chiao tung university. research interests include deep learning computer games grid computing. li-cheng currently master student department computer science national chiao tung university. research interests include machine learning deep learning computer games. i-chen department computer science national chiao tung university. received b.s. electronic engineering national taiwan university m.s. computer science ph.d. computer science carnegiemellon university respectively. serves editorial board ieee transactions computational intelligence games icga journal. research interests include computer games deep learning reinforcement learning volunteer computing. introduced game connect kind six-in-arow game. since then connect become tournament item computer olympiad. team developing various game playing programs winning gold medals international tournaments including computer olympiad. wrote papers served chairs committee academic conferences organizations conference chair ieee conference guan-wun chen currently ph.d. candidate department computer science national chiao tung university. research interests include machine learning deep learning computer games. ting-han currently ph.d. candidate department computer science national chiao tung university. research artificial intelligence computer games grid computing. ung-yi currently master student department computer science national chiao tung university. research include machine learning deep learning computer games.", "year": 2017}