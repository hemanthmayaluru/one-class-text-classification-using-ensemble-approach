{"title": "Sequence to sequence learning for unconstrained scene text recognition", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this work we present a state-of-the-art approach for unconstrained natural scene text recognition. We propose a cascade approach that incorporates a convolutional neural network (CNN) architecture followed by a long short term memory model (LSTM). The CNN learns visual features for the characters and uses them with a softmax layer to detect sequence of characters. While the CNN gives very good recognition results, it does not model relation between characters, hence gives rise to false positive and false negative cases (confusing characters due to visual similarities like \"g\" and \"9\", or confusing background patches with characters; either removing existing characters or adding non-existing ones) To alleviate these problems we leverage recent developments in LSTM architectures to encode contextual information. We show that the LSTM can dramatically reduce such errors and achieve state-of-the-art accuracy in the task of unconstrained natural scene text recognition. Moreover we manually remove all occurrences of the words that exist in the test set from our training set to test whether our approach will generalize to unseen data. We use the ICDAR 13 test set for evaluation and compare the results with the state of the art approaches [11, 18]. We finally present an application of the work in the domain of for traffic monitoring.", "text": "acknowledgements ........................................................................................................................................................... abstract ............................................................................................................................................................................ keywords .......................................................................................................................................................................... list figures ................................................................................................................................................................... introduction ............................................................................................................................................... motivation ......................................................................................................................................................... objectives .......................................................................................................................................................... background ................................................................................................................................................ cnns features ................................................................................................................................................... convolutional layer............................................................................................................................... pooling layer ......................................................................................................................................... logistic regression softmax ............................................................................................................................ binary classification .............................................................................................................................. multiclass classification ........................................................................................................................ long short-term memory lstm .................................................................................................................... constant error carrousels ................................................................................................................... state-of-the-art approaches .................................................................................................................... lexicon-based model ............................................................................................................................... character sequence encoding ......................................................................................................................... n-gram encoding ............................................................................................................................................. joint model ............................................................................................................................................... training ............................................................................................................................................................ lasagne ............................................................................................................................................................ experiments ............................................................................................................................................. lstm architecture experiments ...................................................................................................................... generalization experiment .............................................................................................................................. traffic monitoring .................................................................................................................................... recognizing license plate numbers .................................................................................................................. counting tracking vehicles ........................................................................................................................ initial clustering .................................................................................................................................. experimental results analysis .................................................................................................................... conclusion ............................................................................................................................................... summary .......................................................................................................................................................... achieved results .............................................................................................................................................. future development ........................................................................................................................................ appendix ...................................................................................................................................................................... appendix ...................................................................................................................................................................... datasets .......................................................................................................................................................................... references ....................................................................................................................................................................... slam yousry tremendous help well although barely justice support he‚Äôs given completing research working towards master‚Äôs degree. inspiration since first presentation gave self-driving cars changed thinking countless ways. hanks mohamed mahmoud abdelwahb also known fegla mostafa saad whole community best ever joined. learned them. taught write clean efficient code tackle problems even think. hanks finally samasem elsayed mamdouh abdelkarim mohamed abdelkarim hager wonderful family. words cannot express grateful continuous support. work present state-of-the-art approach unconstrained natural scene text recognition. propose cascade approach incorporates convolutional neural network architecture followed long short term memory model learns visual features characters uses softmax layer detect sequence characters. gives good recognition results model relation characters hence gives rise false positive false negative cases alleviate problems leverage recent developments lstm architectures encode contextual information. show lstm dramatically reduce errors achieve state-of-the-art accuracy task unconstrained natural scene text recognition. moreover manually remove occurrences words exist test training test whether approach generalize unseen data. icdar test evaluation compare results state approaches finally present application work domain traffic monitoring. convolutional neural networks lstm long short term memory support vector machines histogram oriented gradient icdar international conference document analysis recognition recurrent neural networks bptt back propagation time feedforward neural networks blstml bidirectional long short term memory layer optical character recognition sift scale invariant feature transform joint-cnn model joins character sequence encoding model n-gram model joint-lstm model joins output proposed model n-gram model figure pooling layer. ........................................................................................................................ figure extracting suitable corner points clusters obtained initial clustering step rectangles around initial clusters points represent negative score subrectangles maximum bounding merges intersecting rectangles scene text recognition task challenging task useful many practical applications. scene text recognition task considerably challenging well-structured documents ocr. difficulty arises texture lighting conditions diverse text patterns terms font size types. moreover complex background could visually similar text. cene text recognition widely used numerous applications. system help impaired blind people read ingredients products supermarket menus restaurants coffee shops. also used cars read road signs alert drivers break signs even help self-driving cars follow road signs. travelers augmented reality application discover sights visiting checking online. moreover systems ease humanmachine communication like data entry extracting business card information making electronic images printed documents searchable e.g. google books etc... ecently computer vision research field seen shift using hand crafted features learned features using deep neural networks dnns employed solve scene text recognition task. state approaches convolutional neural network based approaches previously researchers using hand crafted features sophisticated models conditional random fields pictorial structure models thesis describe novel approach employs techniques natural language processing tackle unconstrained scene text recognition problem. since cnn-based approach lacks contextual information sequence sequence learning using long short-term memory network aligns well unstructured scene text recognition task. even approaches detect n-gram character sequence using based models combine make contextual information solve visual similarities characters. character sequence n-gram cnn-based models start image confuse found best solve problem consider whole word take characters account. model uses character sequence encoding model corrects mistakes using long short term memory model. since lstms good learning sequences shown sequence sequence learning neural networks decided lstm learn mapping character probability maps generated cnn-based model character sequence encoding correct english word. example detects sony teach lstms detected sony. teach lstm false positives empty characters missing ones replace incorrect characters correct ones. thesis organized follows. start literature survey description relevant work chapter chapter review state-of-the-art approaches using cnns lstms. chapter presents model detail. chapter present experiments show lstm corrects cnn-based approach output lstms generalize predict words exist training set. chapter present traffic monitoring scene text recognition application. finally chapter summarizes results provides ideas future work. problem text recognition scanned documents extensively researched many well performing available systems. text recognition natural images less developed challenging. difficulty arises text regions natural images structure possibly complex background. additionally wide variability text fonts styles colors scales orientations. numerous environmental effects contrast shadow lighting occlusion objects background make even challenging. approaches proposed scene text recognition including mid-level representation multi scales mid-level representation global representation using level hand crafted features sift aggregated bags words fisher vector apply global representations. approaches lexicon word recognition task. either limit lexicon specific words include english words cases don‚Äôt handle common names places abbreviations forth. restriction makes problem much easier model makes mistake characters correct word still correctly detected searching detected word list given words. bigger words lexicon harder task applications require lexicon based approach possible words limited food names street names specific town reading menus restaurant. raditionally people calculate hand crafted features regions interest feed classifier score instance histogram oriented gradient. histogram oriented gradient engineered feature representation first introduced dalal triggs algorithm describes level image features invariant geometric transformation within object‚Äôs structure. algorithm divides image small connected cells cell counts occurrences gradient orientation. current trend machine learning feature representation learned using deep convolutional neural networks. implement convolutional neural network long short term memory-based approach unconstrained scene text recognition. model feature design character detection using softmax layer lstm correcting mistakes missing characters detecting existing ones confusing character characters visual similarities. convolutional neural network‚Äòs architecture consists four convolutional layers fully connected dense layers followed softmax classification input layer. image patches fixed size input first layer input layer output convolutional layer feature output layer input layer. last convolutional layer connected logistic regression classification like support vector machines softmax. normally intertwine subsampling max-pooling normalization layer convolutional layers. training phase parameters optimized using stochastic gradient descent minimize classification loss training set. convolutional neural networks used learn deep features images followed softmax classifier make decisions class patch. scene text recognition task class belong non-text class n-gram word index english words. convolutional neural network learn deep features proven perform much better hand crafted ones numerous computer vision problems example object detection problem deep features-based approach rich hierarchy convolutional neural networks outperforms best performing histogram oriented gradient-based approaches first introduce feature representation need another representation images patches objects rather using values -channel images gray-level intensity values gray images binary values binary images. geometrical interpretation feature vector point n-dimensional space feature vector size number features. image size point -dimensional space pixel dimension. consider image patches number rows number columns patch ùëùstarts image position ends patch image time shift pixel right starts position ends image real life computer vision applications distance small case first dimension ùëùwill compared first dimension ùëùwhich same distance image regions image ùëùand large small shift. also small changes illumination image image regions drastic effect pixel level. pixel feature representation also sensitive image scale. feature extraction important tasks solving computer vision problems like object detection scene classification object tracking. extract descriptive features objects still open research area. feature representation objects invariant scale rotation translation illumination conditions. moreover feature representations handle intra-class variation background clutter considered successful feature vectors. feature representations usually divided three categories low-level mid-level high-level features computed pixel level like edges lines corners sift; etc. low-level features. mid-level features describe objects images patch level. patches discriminative representative discriminative common objects representative occurring frequently objects. example visual words unsupervised discovery mid-level features high-level features describe whole object like convolutional neural network features. first layer learns level features like edges pixels input image. successive layer learns complex high-level features previous convolutional neural networks seen normal neural networks node convolutional filter learn spatial features local connectivity. consider images dimensions normal neural networks pixel connected neurons next hidden layer number weights hidden layer however convolutional neural networks chunk example connected neurons next hidden layer. convolutional layer contains multiple hidden neurons. normal neural network neurons first hidden layer number weights assume apply convolutional filters stride convolutional filters neural network. either weight sharing filter everywhere image neuron hidden layer different filters region input image. number parameters without parameters sharing filters‚Äô dimensions number neurons hidden layer output dimension convolutional layers consist filters produce feature maps image shown figure edge detector example filters look like. filters anything. however edge detector example. filters weights learned using back propagation algorithm. filters convolved input images feature maps generated previous layer. convolution image filter given filter‚Äôs dimensions. filters learn common important features describe specific class discriminate others. first layer filters learn input images level features like edges fine curves etc... second layer filters learn advanced features like shapes third learns even complex features. actually complex features like human faces learned. visualizing understanding convolutional networks paper shows deeper complex feature cnns learn. normally intertwine pooling layer convolutional layers figure pooling layers help reduce number model parameters summarizing output value neighborhood outputs within rectangle window. pooling layers replace output value neighborhood output maximum max-pooling average average-pooling shown figure feature vectors extracted images image patches classifier trained training dataset different classes. classifier identifies class image patch belongs simplest measure distance image feature vector training image feature vectors assign label closest training image. supervised classification approach named k-nearest neighbors chosen crossvalidation. k-nn training feature vectors compared testing samples expensive terms time memory classifier remember training data. problems supervised unsupervised classification intensively studied many classifiers introduced like naive bayes soft-max logistic regression decision trees. usually number training data available memory testing time define classifier suitable. also data linearly separable it‚Äôs better choose non-linear kernel function. simple probabilistic classifier like naive bayes takes linear time train assumes features independent. logistic regression doesn‚Äôt assume conditional features independence. learn something like someone likes cream winter summer. logistic regression gives probabilistic interpretation like naive bayes. moreover logistic regression update model data item without repeat whole training process. hand logistic regression slower naive bayes doesn‚Äôt perform well high dimensional feature vectors. model‚Äôs parameters bias respectively. input vector belongs class otherwise. decision boundary defined -dimensional hyperplane input vector size. points decision boundary decision boundary ùë§ùë°ùë•ùëé ùë§ùë°ùë•ùëè therefore vector orthogonal vectors lying within decision boundary hyperplane. since determines location decision boundary measure normal distance decision boundary origin introduce ambiguous regions could figure class point regions belongs shown figure alternatively avoid ambiguous regions considering ùêæ-class discriminant consists linear functions form oft-max model generalizes logistic regression used multi-class classification problems. perfectly fits text detection problem different classes characters digits negative class handle background patches. et‚Äôs first introduce logistic regression works binary classification takes either logistic regression uses iterative approach approximate model parameters size directly number features. estimate model parameters indirectly using generative models fitting class-conditional densities class priors separately apply bayes‚Äô theorem. number model parameters fitted gaussian class conditional densities parameters shared covariance matrix class prior number model parameters logistic regression linear compared quadratic generative models. shall show model parameters logistic regression estimate using maximum likelihood. estimate logistic model parameters using maximum likelihood. consider dataset class label input feature vector nonlinear basis function shown figure likelihood function difference predicted output target value times nonlinear basis function input feature vector data contributes gradient update model parameters iteratively optimal found using gradient descent determine model parameters using maximum likelihood. simplest extend maximum likelihood classes form ùë¶ùëõ}‚àíùë°ùëõ general form multiple classes encode target vector using -of-k encoding scheme. write general form maximum likelihood follow imilar binary case difference predicted output target value times nonlinear basis function ‚àÖùëõfor input feature vector data contributes gradient update model parameters iteratively optimal found using gradient descent recurrent neural network states encode information whole sequence. output time depends information captured also information future whole input sequence. type recurrent neural networks called bidirectional recurrent neural networks bidirectional neural networks first invented schuster paliwal used applications hand writing speech recognition bioinformatics recurrent neural networks either learn sequences fixed size like hopfield networks boltzmann machines time-varying sequences. recurrent neural networks suffer exponential decay gradient information limitation learning approaches. gradient based approaches back-propagation time real-time recurrent learning gradient evolution exponentially depends magnitude weights means back-propagated error either vanishes explodes long-short term memory networks recurrent neural network solve long term dependencies problems. problem recurrent neural networks gradients propagated many stages tend either vanish explode. lstm algorithm overcomes problems enforcing non-decaying error flow allows bridge minimal time lags excess discrete time steps using called constant error carrousels basic unit lstms memory block. replaces state unit recurrent neural networks. memory block contains memory cells additive multiplicative gates shown figure memory cell self-connected linear unit called constant error carousel solves gradient vanishing explosion problems. loss memory block ùõøùëúùë¢ùë°ùëóis summation cells size block best performing algorithms today many vision tasks text recognition particular based deep convolutional neural networks. recent approaches cnns generate probability maps characters n-grams another deep convolutional neural network approach detecting unconstrained text combines convolutional neural networks-based approaches. first character sequence detection second n-grams detection. cnnmodels start images computationally expensive. approach introduced thesis uses cnn-model character sequence encoding lstm-model make approach faster approaches much related work. convolutional neural network model generate character sequence maps. long-short term memory correct output convolutional neural network. work state approach synthetic data artificial neural networks natural scene text recognition show lstms improve performance cnn-based model even beat state performance. following three models presented scene text recognition. first model figure uses lexicon large size employ almost english words; total. second character sequence encoding. model produces probability maps classes character. third model trained spot n-grams using fully connected layer size nodes followed softmax. model encodes words predefined dictionary. dictionary covers around english words including different forms word. number classes equals number word might seem large classification problem authors perform incremental training handle huge number classes. model architecture shown figure consists four convolutional layers fully connected layers. rectified linear units used throughout weight layer except last one. max-pooling layer size follows first second third convolutional layers. convolutional layer filters‚Äô dimensions first second third fourth convolutional layers respectively. character sequence encoding model lexicon restriction detects sequence characters generating probabilities classes patch shown figure architecture described section used character sequence encoding except last layer. instead feed output fully connected layer fully connected layers size neurons each neurons represent number classes maximum word length. character sequence model trained image samples words dictionary. character sequence counted perfectly matches ground truth otherwise counted miss. third model generates n-grams using architecture figure final fully connected layer neurons n-grams presented input image applying logistic regression function. since word presented composition unordered characters n-gram learning recognize presence n-gram within input image. joint model cnn-based architecture incorporates conditional random field character sequence encoding model predicts unaries model higher order terms predicted n-gram encoding model. model include lexicon free constraints like maximum word length statistical information n-gram model. n-gram model makes n-gram frequencies corpus scales gradient inverse frequency n-gram appearance. lstm recently used many computer vision natural language processing problems. mainly lstms good problems performs well modeling sequences. problem machine translation good direct application lstm sequence sequence learning neural network main motivation work. paper teach lstms sequence words english sequence words french. main contribution handle sequence sequence learning arbitrary length. another interesting paper uses lstms computer vision problems unsupervised learning video representation using lstms encodes representation video sequences lstm decodes using single multiple lstms perform tasks predicting feature frames reconstructing input sequence. sequence sequence learning neural network relevant work multilayered long short-term memory input sequence fixed length vector decode target sequence vector. work present novel approach unconstrained scene text recognition using convolutional neural networks long short-term memory experimented different lstm architectures experiment thin deep etc‚Ä¶ section detail discussion. different architectures find best performing lstm architecture corrects mistakes. lstm sequence sequence learning algorithm able learn length sequences also learn arbitrary length length sequence shown shortly. sequence sequence learning paper authors present lstm-based approach translate french english. instead probability maps -classes negative classes detected characters convolutional neural networks model character sequence encoding correct english word. difference task translation task grammar rules mapping character sequence probability maps correct english word. lstm model learn implicitly grammar rules. problem easier machine translation task. lstm model sequence sequence learning neural networks ignores outputs input sequence‚Äôs items till terminal character lstm. feeds output terminal character lstm input gets output feeds lstm gets etc‚Ä¶ till lstm outputs terminal character stops detecting anything else. order words given lstm important translation problems sentence structure changes language language according grammar rules language. found experiments accuracy improved start producing output sequence seeing whole input sequence slower experiment extending model optimized lstm architecture section decided maximum length trick lstm decides starts producing output sequence regardless start point lstm produce empty character index maximum length comes maximum length predictions. empty characters come padding input sequences length predictions figure shows example mapping ùëäùëãùëåùëç lstm starts producing sequence length characters index stop index produces empty character index context scheme right-justify ground truth shown figure teach lstm output empty characters beginning empty character terminal character terminate prediction process. lstm predicts characters non-empty characters seeing enough probability maps characters start prediction process. words lstm generates empty characters correct english word lstm point view another empty character terminal shown figures teach lstm false positives empty characters missing ones replace incorrect characters correct ones output empty terminal character. handle arbitrary length sequence sequence different efficient presented sequence sequence learning paper neural networks lstm model lstms. first model used encoding input sentence second decoding target sequence. moreover model lstm start producing characters input sequence finished shown figure feed predicted sequence lstm. formulation lstm learns starts producing characters producing empty terminal character. train lstm models using back propagation stochastic gradient descent. default learning rate= momentum= lasagna library. network trained back-propagating gradients softmax classifier base lstm using backpropagation time. maximum sequence length presented sections network training performed solely synthetic dataset lstm memory constraints trained randomly sampled synthetic dataset besides icdar training set. datasets description appendix goal network learn robust features detect initial character sequence training character sequence. word length sequence character represents character position word empty character. given classifier character. independence assumption compute taking probable character position independence assumption would network learns instead. lstm teach network estimate input sequence corresponding output sequence whose length followed empty character. shown section output time depends input sequence till time lasagne library lstms implementation training testing. library underdevelopment written python theano lasagne supports makes training time times faster training cpu. library open source making possible contribute report bugs. library documentation well written found easy install understand. theano functions pretty different terms syntax input parameters. functions compiled part symbolic graph makes hard debug. debug variable would like values output print function finishes execution. shared variables variables share contents gpus. recommend putting part data variables otherwise code memory. section shall present multiple experiments show lstms improve performance even beat state deep structured output learning unconstrained text recognition referred joint model. pre-trained model detnet_layers.mat experiment. testing phase image scaled different scales convolved convolutional neural network‚Äôs filters generate character maps. maxout layer choose pixel character highest probability certain character labels map. labels connected component algorithm combine neighboring pixels label together component. combine character boxes word distance centers ùëêand less minimum width then long short term memory remove false positive characters shown figure datasets experiments. first synthetic dataset icdar dataset. synthetic dataset contains english words; around images word icdar contains around annotated words; training testing. synthetic dataset enrich training set. choose images synthetic dataset cover words icdar dataset. since lstm correct output section going present multiple experiments comparing performance versus cnn+lstm. convenience would call latter simply lstm. output either consider produced character maps non-maxima lstm model experiment uses context length means output least empty characters producing correct word another empty character terminal shown figure experiment learning rate momentum= stochastic gradient descent patch size lstm converges around epochs validation set. validation consist images. shall present evaluation techniques. first word-level second character-level evaluation. word level evaluation calculate edit-distance model predictions ground truth words. word level calculate percentage perfect match zero distance model predictions ground truth percentage words needing operation match model‚Äôs predictions ground truth three three operations ed>. table lstm shows significant improvement perfect match column lstm gain perfect match words needing operation match ground truth around gain lstm. longest common subsequence predicted word ground truth true positives difference length ground truth false negatives difference length prediction false positives. precision recall calculated follow. nterestingly table notice cnn-noisy without non-maxima suppression highest recall lowest precision means produces many false positives detect around characters lose around characters improve precision either non-maxima suppression lstm. substantial improvement precision lstm keeping recall almost same confirms argument lstm filter false positives even true positives missed non-maxima suppression. terms f-measure lstm-clean shows improvement cnn-clean almost improvements lstm-noisy cnn-noisy. experiment another model published alongside paper synthetic data artificial neural networks natural scene text recognition convenience remaining sections call cnn_chars model. mentioned previous experiment learning rate momentum patch size. also consider icdar synthetic datasets training icdar test testing. also allow reasonable training times around consider whole synthetic dataset training. chose images cover words training test icdar dataset. experiment find whether approach would work pick best performing lstm architecture given task. experiment tested different lstm models. sometimes architecture deep sometimes thin. moreover tried intertwine dropout layers different dropout rates. compare performance models cnn_chars joint models. joint model combines cnn_chars output another convolutional neural network based model n-gram detection explained background chapter. following section shall present different model architectures performance compares other. first model consists input layer output dense layer followed softmax classification bidirectional lstm layers number nodes first second layers respectively shown figure model sample file lasange library. results initial results good encouraged architectures like adding bidirectional lstm layers changing number nodes layer blstm layers adding dropout layers. second third architectures figure added bidirectional lstm layer. second model architecture fatter. words number nodes layer bigger third model picture clear going deep didn‚Äôt improve going thin improved comparing performance model model going deeper thinner improved comparing performance model model model number four added bidirectional lstm layer number units units shown figure expecting reducing number model parameters would affect accuracy way. surprisingly accuracy improved compared model model good motivation test even deep architectures although shown model blstm layers deep resulted lower accuracy. model consists input output followed softmax blstm layers units. accuracy model compared model odel -blstm layers like model different number nodes layer. number nodes blstm layer accuracy model improved compared model first outperform joint model shown table model -blstm-layers model like model terms number blstm layers number nodes layer number nodes blstm layer although model contains parameters model accuracy model worse model moreover models -blstm layers improve model -blstm layers model. accuracy model even much worse accuracy model message deeper necessarily better performance achieved shown table best performing model among experimented architectures model consists -blstm layers. blstm layer followed dropout layer dropout rate input layer output dense layer followed softmax classification shown figure appendix experimented architectures lstm networks factor number layers nodes layer etc‚Ä¶ would affect performance. experiments comprehensive indicate features generally improve performance. limited experiments found model best performing model large experiments shown table experiment make large amount images chosen randomly synthetic dataset enrich icdar training set. picked around million images covering around words around synthetic images word. training phase takes around days dataset experiment. architecture model parameters learning rate momentum patch size previous experiment. model converged around epochs dropout layer. results show little improvement state significant improvement cnn_chars model shown table since approach employs cnn_chars output feeds long short term memory network named cnn+lstm. convenience call lstm instead cnn+lstm. printed words lstm predicts correctly cnn_chars corrupts words lstm corrupts cnn_chars predicts correctly found interesting phenomena. starting words needing operation match ground truth. table lstm corrected type -operation errors like deleting extra character adding missing character changing incorrect one. deleting extra character like adding missing character like changing incorrect character like ords ieee visually looks like looks like looks like looks like ‚Äòa‚Äô. argue words corrected visual similarity. words digikey click multicomp visual similarity doesn‚Äôt exist anymore look like look like look like ‚Äòk‚Äô. lstm relied ngram statistics training contextual information surrounding characters. argument n-gram statistics contextual information surrounding characters applies cases deleting extra character adding missing one. nterestingly lstm even correct words needing operation match ground truth shown table example detected zsobit cnn_chars model. lstm able correct experiment show lstm correct words never seen training set. training test described experiment remove synthetic images occurrences words exist icdar test set. lstm corrected words present table spoiled words presented table accuracy still better cnn_chars model worse joint model table experiment check whether lstm detect unseen words not. autonomous navigation interesting application scenario unconstrained scene text recognition plays vital role reading road signs ads. chapter discuss subproblems domain traffic monitoring recognizing license plate numbers road help identifying speeding cars instance counting number cars flow control scenarios. cene text recognition help reading plates. feed boxes characters‚Äô response maps. non-maxima suppression disjoint find licenses plate bounding explained figure algorithm section form bounding around plate‚Äôs text. applied proposed methods using cnn+lstm collection real-world images taken highway scene. figure presents sample recognition results. sample recognized incorrectly visual similarity ‚Äòc‚Äô. second sub-problem tackle domain traffic monitoring counting vehicles. counting tracking vehicles crowded scenes help monitoring roads choosing path takes least amount time necessarily shortest path terms length. counting tracking vehicles crowded scenes challenging task. task require discipline conditions deal with. present novel algorithm automatically counting number moving vehicles regular crowded scenes different conditions. first extract interest points calculate trajectories independently. cluster interest point initial clusters based proposed mathematical relations. estimate number moving vehicles grouping initial clusters based adaptive background construction method maximum subrectangle algorithm disjoint data structure. applied algorithm collected dataset representing crowded traffic scenes shows excellent high accurate performance. addition storage computational requirements promotes real time applications. motion parameters moving points sequence images computed identifying pairs points correspond other‚Äôs images taken time ùõøùë°.. good idea match pixels other illumination lighting change frames. corner points instead. good property corner points precisely identified image rather pixels‚Äô value flat zones. corner points points high spatial gradient high curvature. motion vectors associated corner points reliable pixels. calculate corner points foreground foreground moving objects. difference consecutive frames result positive negative values moving pixels zeros everywhere else. apply morphological operations remove noisy regions. filtered foreground pixels grouped regions employing connected component labeling algorithm. corner point‚Äôs detector applied region assures corner moving pixel. pre-processing step avoid calculating corner points whole image facilitate clustering well. calculating corner points optical flow pyramid lucas kanade optical flow method applied points frame corresponding points frame points tracked sequence images using optical flow calculate trajectories. clustering important step algorithm trajectories belong object cluster. input step vector ùëáùëüùêø} trajectory number trajectories. single trajectory often represented path consists sequence points. written number points trajectory. given input data goal produce clusters ùê∂ùëôùë¢ùë†ùë° number moving objects cluster consists object‚Äôs trajectories. trajectories‚Äô angles distance trajectories parallel similarity parameters determine whether trajectories belong cluster not. need build model background make detecting foreground‚Äôs object easy task.. since background changes time illumination background variation section describe adaptive background construction method. claim occurrence background pixels values non-moving parts frequent value. global d-array size input video frame size representing background model. transform input image color system. background model consists pair values current pixel color value increase current pixel color value decrease ùê∂ùëöùëéùë• current pixel color value increase current frame ùê∂ùëöùëéùë• constant representing maximum confidence. figure extracting suitable corner points clusters obtained initial clustering step rectangles around initial clusters points represent negative score subrectangles maximum bounding merges intersecting rectangles cluster obtained section shown figure initial rectangle size generated center center shown figure element takes negative score corresponding pixel background indicated positive score otherwise shown figure obtain sub-rectangle ùëÜùëöùëéùë• covers largest part moving object shown figure worth note possibilities obtain sub-rectangle inside consider matrix rows columns whose elements retain elements sub-rectangle using matrix able compute elements current sub-rectangle order magnitude algorithm calculates matrix maximum based maximum interval problem onedimensional array version maximum sub-rectangle problem. using dynamic programming solve maximum interval problem order number elements interval algorithm applied solving maximum sub-rectangle problem order time complexity finally bounding bounds vehicle shown figure obtained joining sub-rectangles representing clusters contributing moving object. supposed rectangles share vehicle common intersection area. rectangles joined using disjoint data structure obtain exactly rectangle covering vehicle. complexity joining rectangles number rectangles. conducted three experiments newly collected dataset presenting vehicles crowded scenes taken different places different times. dataset consists three videos nuvideoi nuvideoii nuvideoiii. dataset vehicles follow lanes fixed pattern. nuvideoi recorded highway road time shown figure camera installed meters high. number vehicles passed predefined virtual zone minutes. nuvideoii nuvideoiii recorded ring road different times different shadow conditions shown figure camera installed eight meters high. number vehicles minutes respectively. first experiment applied nuvideoi camera attached panda board dual-core cortext symmetric multiprocessing ghz. algorithm counted vehicles recorded time stamp entrance exit vehicle passed virtual zone achieved accuracy real time computation. second third experiments applied nuvideoii nuvideoiii. experiments intel core .ghz ram. algorithm counted vehicles achieve accuracy respectively. average process time experiment equal seconds frames promotes algorithm real time applications. concluded algorithm count track vehicles real time limited computational resources. practical deployments shown table confirm excellent properties. thesis presented novel approach unconstrained scene text recognition. implemented approach incorporating convolutional neural networks long short term memory. showed lstm able make contextual information correct output cnns. also introduced handle arbitrary length sequence sequence learning using long short term memory. moreover showed application scene text recognition presented novel algorithm vehicle counting tracking. proposed lstm cnn-based approach achieve state results shown section faster previous state also showed lstm generalize predict words never seen training set. moreover showed factors number layers nodes layer dropout layers would affect lstm performance. also performance algorithm vehicle counting tracking joint model shows significant improvement character sequence encoding model future plan incorporate output cnn+lstm output n-gram network expect better results joint model n-gram cnn-based model detects contextual information images directly model learns contextual information english words corrects chars model mistakes. also plan make whole synthetic dataset powerful computers. besides would like train lstm models simultaneously instead training lstm independently. moreover would like lexicon lstm predictions shown figure compare lexicon based approaches‚Äô performance. peepholes true l_in lasagne.layers.inputlayer) recout lasagne.layers.bidirectionallstmlayer recout lasagne.layers.bidirectionallstmlayer l_reshape lasagne.layers.reshapelayer)) l_rec_out lasagne.layers.denselayer recout_ lasagne.layers.bidirectionallstmlayer recout_ lasagne.layers.bidirectionallstmlayer l_reshape lasagne.layers.reshapelayer) work mainly usde datasets icdar synthetic dataset evaluate approach icdar word recognition test consists images. task focused reading text real scenes. dalal navneet bill triggs. \"histograms oriented gradients human detection.\" computer vision pattern recognition cvpr ieee computer society conference vol. ieee perronnin florent christopher dance. \"fisher kernels visual vocabularies image categorization.\" computer vision pattern recognition cvpr'. ieee conference ieee", "year": 2016}