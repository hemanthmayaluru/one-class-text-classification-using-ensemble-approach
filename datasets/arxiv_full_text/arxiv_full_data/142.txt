{"title": "PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction", "tag": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction, and is continually under research. Recently, the multiple-input several multiple-outputs (MISMO) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction, exhibiting advantages compared with the two currently dominating strategies, the iterated and the direct strategies. Built on the established MISMO strategy, this study proposes a particle swarm optimization (PSO)-based MISMO modeling strategy, which is capable of determining the number of sub-models in a self-adaptive mode, with varying prediction horizons. Rather than deriving crisp divides with equal-size s prediction horizons from the established MISMO, the proposed PSO-MISMO strategy, implemented with neural networks, employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub-models, providing considerable flexibility in model construction, which has been validated with simulated and real datasets.", "text": "predict next ones. direct strategy prediction models solved predict values steps ahead. accumulation errors iterated case deteriorates accuracy prediction direct strategy requires sharply increased computational expense especially case longer prediction horizon. based upon mainstream modeling strategies revival interest developing novel multi-step-ahead prediction strategies dirrec strategy multiple-input multiple-output strategy multiple-input several multiple-outputs strategy review comparative study strategies multi-step-ahead multiple-output strategies including mimo mismo best performing approaches validated forecasting competition data study focuses mismo strategy improving terms modeling flexibility accurate prediction. mismo -step-ahead prediction task subtasks yielding sub-models fixed number multiple outputs each value chosen cross-validation analyzing performance mismo model different values learning using best value estimate outputs special case mismo regarded mimo mismo direct strategy. superior out-of-sample performance mismo strategy presented depends trade-off heterogeneity computational complexity. mismo strategy exploits heterogeneity across subtasks efficiently less computational complexity direct prediction technique. important feature mismo strategy original prediction modeling task divided several subtasks equal number outputs handling indivisibility problem explicit difficulty method. addition underlying problem arises crisp divides prediction horizons equal size induce latent separation dependencies among steps different models. problem prevent mismo strategy considering complex dependencies prediction steps within different models consequently reduce prediction accuracy. thus strategy flexibility mechanism self-adaptively determine prediction horizon divides modeling appealing. study proposes improved pso-mismo abstract—multi-step-ahead time series prediction challenging research topics field time series modeling prediction continually research. recently multiple-input several multiple-outputs modeling strategy proposed promising alternative multi-step-ahead time series prediction exhibiting advantages compared currently dominating strategies iterated direct strategies. built established mismo strategy study proposes particle swarm optimization -based mismo modeling strategy capable determining number sub-models self-adaptive mode varying prediction horizons. rather deriving crisp divides equal-size prediction horizons established mismo proposed pso-mismo strategy implemented neural networks employs heuristic create flexible divides varying sizes prediction horizons generate corresponding sub-models providing considerable flexibility model construction validated simulated real datasets. prediction models continue limited one-step-ahead prediction multi-step-ahead prediction primarily challenges arising increased uncertainty longer prediction horizons. common modeling strategies multi-step-ahead time series prediction rely either iterated direct strategies iterated strategy first constructs one-step-ahead prediction model uses predicted values known data manuscript received september work supported part natural science foundation china grant fundamental research funds central universities project humanities social science grant modern information management research center huazhong university science technology. modeling strategy incorporates heuristic based particle swarm optimization mismo modeling process self-adaptively determine number sub-models varying prediction horizons. purpose justification study compares rank four well-established strategies neural networks addition straightforward alternative binary genetic algorithm-based mismo modeling strategy selected counterpart proposed pso-mismo compared well. simulated real datasets used comparisons. paper structured follows. section provides brief review multiple-output modeling strategies including mimo mismo limited details. section describes basic concepts multi-step-ahead prediction implementation fnn-based prediction model current study. concept binary explained section section details proposed pso-mismo strategy presented. section details experimental setting data accuracy measure genetic algorithm alternative experimental procedure. experimental results discussed section concluding remarks provided section viii. multi-step-ahead prediction described estimation integer greater observation characterized multiple-input multiple-output approach predicted value scalar quantity vector future values resulting reduction modeling effort likely bias returned model results. mismo strategy solution shortcomings mimo proposed constraint fixed modeling structure relaxed tuning integer parameter calibrates dimensionality outputs basis validation criterion. given mismo generates sub-models different inputs fixed size outputs resulting slightly modeling flexibility. mismo approach highlights trade-off property preserving stochastic dependency among predicted values flexibility modeling procedure however increased flexibility mismo strategy requires additional parameter gives rise several issues itself. explicit issue handling indivisibility computing parameters integers. furthermore number outputs every sub-model derived mismo fixed induces latent separation dependencies among steps prevent mismo strategy considering complex dependencies prediction steps within different sub-models consequently reduce prediction accuracy. study solicited determine divides prediction horizons adaptively modeling rather fixing although many types neural networks proposed popular time series prediction feed-forward focus study evaluation proposed modeling strategy relative competitors multi-step-ahead prediction simple three-layer implemented model selected strategies capable creating common reliable benchmark. addition proposed modeling strategy four well-established strategies implemented flexible structure fnn. thus standard three-layer nodes adjacent layers fully connected used study. section short introduction multi-step-ahead prediction fig. shows example typical three-layer model four input nodes four hidden nodes output neurons used multi-step-ahead prediction. input nodes previous lagged observations outputs provide forecast future values multi-step-ahead fashion. hidden nodes appropriate nonlinear activation functions used information received input nodes. bias terms used hidden output layers. facilitate understanding multi-step-ahead time series prediction mimo modeling strategy used example section. functionally equation also written follows represents model vector contains parameters equation determination parameter based local error measured used index multi-step-ahead prediction performance minimized defined follows following procedure design tasks fnn-based prediction model roughly divided four parts implementation validation. implementation fnn-based model multi-step-ahead prediction presented step-by-step below standard data preprocessing. normalization requirement time series modeling prediction. thus data sets first scaled linear transference onto range linear transference deseasonalization detrending performed necessary. deseasonalization performed recent x--arima seasonal adjustment procedure. detrending performed fitting polynomial time trend data subtracting estimated trend series trends detected mann-kendall test. designing. selecting appropriate architecture normally first step designing fnn-based prediction system. standard three-layer fully connected nodes adjacent layers used study. number input nodes hidden nodes output nodes type activation functions defined present study follows. selecting number input nodes number input nodes determined input selection. filter method selects inputs optimizing criterion different combinations inputs means search algorithm employed input selection current study. filter method requires setting elements criterion i.e. statistic estimates quality selected variables search algorithm describes policy used explore input space. respect criterion partial mutual information used models using iterated direct strategies extension delta test used models mimo mismo ga-mismo pso-mismo strategies. respect search algorithm forward-backward selection method offers flexibility reconsider input variables previously discarded discard input variables previously selected used. maximum embedding order selecting number hidden nodes number hidden nodes cannot determined advance. thus empirical experimentations needed determine number small sample size many series experimentation limited five possible values hidden nodes i.e. best number hidden nodes determined using original akaike’s information criterion selecting number output nodes number output nodes determined modeling strategy. modeling strategies found literature classified categories single-output structure multiple-output structure. single-output structure differences binary original pso. first difference representation particle. binary every particle characterized binary solution representation difference velocity particle binary probability vector probability element determines likelihood binary variable value one. fig. lists pseudocode algorithm basic binary pso. algorithm binary particle swarm optimization initialize population particles random positions velocities throughout input space. sufficiently good performance maximum number iterations cognitive interaction coefficients random real numbers uniformly rand distributed inertia weight user-specified parameter controls momentum particle. larger inertia weight pushes towards global exploration smaller inertia weight helps fine-tuning current search area. following weighting function usually utilized strategies iterated direct strategies multiple inputs single output. mimo mismo ga-mismo pso-mismo strategies based multiple-output structure maps multiple inputs multiple outputs. therefore iterated direct strategies output node used model. employing mimo strategy number output nodes equal number prediction horizons. mismo strategy original -step-ahead prediction tasks separated subtasks resulting sub-models fixed multiple outputs each unlike mismo strategy uses fixed number outputs sub-model ga-mismo pso-mismo strategies self-adaptively determines number sub-models varying number outputs described section selecting type activation functions sigmoid transfer function used hidden node linear transfer function used output node. implementation. levenberg marquardt algorithm provided matlab toolkit used training. node biases applied hidden output layers. stopping criterion number learning epochs chosen prior knowledge appropriate value. determine optimal parameters common practice fivefold cross-validation used. validation series simulated real dataset last observations separated ex-ante performance assessment statistically independent parameter estimation process. remainder data used parameter estimation modeling. population-based self-adaptive search algorithm exploits population individuals probe promising regions search space. population referred swarm consists number particles. particle represents potential solution optimization task position represented vector. particle moves position according local information global information thus ability converge local and/or global optimal solutions small number generations. section brief introduction variant original i.e. binary presented understand mechanism proposed pso-mismo strategy applied study. initially designed solve continuous optimization problems. since introduction successful applications several optimization problems demonstrated potential. however major obstacle applying successfully continuous nature. first variant algorithm solving problems binary-value solution elements also developed creator original pso. address limitations mismo mentioned first section illustrated section ii.b improved binary pso-based mismo developed self-adaptively determines number sub-models varying prediction horizons. proposed strategy abbreviated pso-mismo. fig. depicts flowchart proposed pso-mismo modeling strategy. shown detailed pso-mismo algorithm consists three major operations initialization evaluation update. describing details operations coding issues addressed. overall learning process fig. elaborated step step below. represents number consecutive outputs outputs). values determined binary vector represents segmentation point particle representing possible solution optimization problem determined components binary vector either value variable original task divided segmentation point. value variable prediction task divided segmentation point. fig. solution representation. fig. pseudocode computing value initialization. particles randomly generated using probability. velocity components assigned initial value concerning selection parameters binary another challenging model selection task fortunately several empirical theoretical studies performed parameters valuable information obtained study parameters determined according recommendations studies selected based prediction performance computational time trial-error fashion. table summaries final parameters. competition organized targeting computational-intelligence approaches. competition dataset monthly time series drawn homogeneous population real business time series used evaluation such three datasets logistic time series mackey-glass time series time series used evaluating performances proposed pso-mismo counterparts study. series split estimation sample hold-out sample. last observations saved evaluating comparing out-of-sample various multi-step-ahead prediction strategies. performance out-of-sample points comparisons based logistic mackey-glass datasets out-of-sample points datasets. accuracy measure prediction horizon three alternative forecast accuracy measures considered mean absolute percentage error symmetric mean absolute percentage error mean absolute scaled error definitions follows forecast particle prediction horizon. update. current generation fitness values particles swarm calculated pbest particle gbest swarm obtained. then particles’ velocities positions updated according equations respectively. termination conditions output gbest corresponding particle otherwise back previous step. evaluate performances proposed pso-mismo counterparts terms forecast accuracy simulated time series i.e. logistic mackey-glass time series real world dataset i.e. competition dataset used present study. logistic mackey-glass time series recognized benchmark time series commonly used reported number studies related time series modeling forecasting data-generating process i.e. logistic mackey-glass process simulate twenty time series different initialization sample size shown table data time series generated chaotic operatation. designing operators selection crossover mutation major issue implementation always done hoc. generation fitness values chromosomes population calculated selection operator chooses chromosomes current generation’s population inclusion next generation’s population executed. many ways choosing parents study roulette method used logistic time series percent method adopted mackey-glass time series preliminary simulation trial-error fashion. selection parents crossover operator applied produce offspring exchanging genetic information parents. crossover occurs evolution according crossover probability. crossover probability often crossover performed. study probability crossover shown table iii; two-point operator used. process selection reproduction repeated number offspring becomes equal number eliminated chromosomes; adding offspring population size becomes equal initial size mutation third operator applied population excluding best chromosome. simple mutation operator selects percent genes population randomly flips values zero vice versa used study study probability mutation shown table iii. process repeated termination conditions reached. termination conditions best chromosome corresponding step-ahead forecast step-ahead forecast time series true time series value series number prediction horizon preprocessed data including normalization deseasonalization detrending convert outputs neural networks back original scales. performance prediction compared directly. genetic algorithm alternative proposed pso-mismo modeling multi-step-ahead prediction compare experimental results pso-mismo produced discrete evolutionary algorithms genetic algorithm subsection briefly presents implementation ga-mismo straightforward alternative pso-mismo. fig. depicts flowchart ga-mismo modeling strategy. shown fig. detailed ga-mismo algorithm consists initialization evaluation operatation. describing details parts coding issues addressed. overall learning process fig. elaborated step step below. coding. chromosome design ga-mismo particle design pso-mismo presented section actually individual regarded chromosome terms ga-mismo particle terms pso-mismo. values components binary vector either value variable original task divided segmentation point; value variable prediction task divided segmentation point. coding issue ga-mismo pso-mismo presented details save space. parameters population size number iterations crossover probability mutation probability initialization. chromosomes randomly generated using probability. setting parameters required extensive simulations find suitable values various parameters. study ga’s parameters determined preliminary simulation selected according recommendations trial-error fashion. table summarizes final parameter evaluation. evaluation ga-mismo fig. shows experimental procedure using simulated real time series. series first split estimation sample hold-out sample. then input selection model selection series determined using filter method levenberg marquardt algorithm fivefold cross-validation iterated direct mimo mismo ga-mismo pso-mismo strategies. finally models tested hold-out samples mapeh smapeh maseh computed prediction horizon datasets modeling process series repeated times. upon termination loop performance examined models selected strategies prediction horizon dataset judged terms mean averaged mapeh smapeh maseh analysis variance test procedures used determine means performance measures significantly different among three models prediction horizon datasets. tukey’s honesty significant difference tests used prediction performances modeling strategies examined terms three accuracy measures average rank three datasets shown table columns labeled ‘average show average accuracy measures prediction horizon last column shows average ranking model prediction horizons out-of-sample prediction performance. column table entry smallest value boldface marked asterisk entry second smallest value boldface type. rankings best worst pso-mismo ga-mismo almost mismo mimo direct iterated strategies regardless accuracy measures considered. thus findings robust choice accuracy measures. modeling strategies based multiple-output structure outperform based single-output structure agreement despite multi-step-ahead prediction literature consistently worst performing strategy prediction. comparing iterated strategy direct strategy direct strategy better regardless accuracy measures considered demonstrates accumulation errors case iterated strategy drastically deteriorates accuracy prediction. mimo strategy consistently achieves accurate forecasts either iterated direct strategies prediction horizons. conceivable reason superiority mimo strategy preserves among predicted values stochastic dependency characterizing time series. mismo strategy seems produce forecasts accurate mimo strategy conceivable reason superiority mismo strategy trades property preserving stochastic dependency future values greater flexibility predictor. comparing heuristic-based modeling strategies mismo strategy heuristic-based strategies generally better indicate fixed number outputs every sub-model derived mismo strategy tends prevent mismo strategy considering complex dependencies prediction steps within different models consequently reduces prediction accuracy. ga-mismo pso-mismo pso-mismo ga-mismo ga-mismo pso-mismo pso-mismo ga-mismo pso-mismo ga-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo pso-mismo ga-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo pso-mismo ga-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo pso-mismo ga-mismo pso-mismo ga-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo pso-mismo ga-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo pso-mismo pso-mismo ga-mismo pso-mismo ga-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo pso-mismo ga-mismo ga-mismo pso-mismo ga-mismo pso-mismo mismo mismo mismo mismo mimo mismo mismo mismo mismo mismo mimo mismo mismo mismo mismo mismo mismo mismo mismo mismo mismo mismo mismo mismo mimo mismo mimo mismo mismo mismo mismo mismo mimo mimo mimo mimo mimo mimo mimo mimo mismo mimo mismo mimo mimo mimo mimo mimo mismo mimo mimo mimo mimo mimo mismo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo mimo iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter iter mimo iter iter iter iter mimo report results randomly selected three time series logistic mackey-glass datasets respectively. series logistic time series length= length= mackey-glass time series series dataset. addition following experiment swarm/population size fixed pso-mismo ga-mismo strategies generation number described section strategy times time series attempt eliminate influence lucky initial solution. numerical experiments performed personal computer inter core .-gb memory matlab environment mentioned above learning process repeated times pso-mismo ga-mismo time series. thus typical convergences best average fitness values function generations pso-mismo ga-mismo three aforementioned time series shown fig. respectively. seen typical pso-mismo optimization process best fitness value decreases rapidly converges generations logistic time series generations mackey-glass time series generations time series datasets; whereas ga-mismo takes generations logistic time series generations mackey-glass time series generations time series datasets. show evolution process going pso-mismo ga-mismo convergence average fitness values also shown fig. looking fig. clear pso-mismo seems perform better ga-mismo. thus present problem performance ga-mismo evolutionary point view. comparison pso-mismo ga-mismo concerned almost results mixing among prediction measures examined. terms mape pso-mismo wins logistic dataset dataset loses mackey-glass dataset. terms smape pso-mismo loses mackey-glass dataset dataset wins logistic dataset. terms mase pso-mismo wins mackey-glass dataset dataset loses logistic dataset. following experimental procedure presented fig. anova procedure performed determine exists statistically significant difference among modeling strategies hold-out sample performance measures prediction horizon. results included detail save space. anova results significant level suggesting significant differences among modeling strategies. identify significant difference strategies tukey’s test used compare pair-wise differences simultaneously. tukey’s test post-hoc test meaning tukey’s test performed unless results anova procedure positive. table shows results multiple comparison tests three datasets. accuracy measure prediction horizon strategies rank ordered several observations made table accuracy measure dataset pso-mismo ga-mismo significantly outperforms mismo overwhelming majority prediction horizons. considering heuristic-based modeling strategies that whatever dataset used whatever accuracy measure considered whatever prediction horizon examined difference prediction performance pso-mismo ga-mismo significant level. comparison mismo mimo concerned difference prediction performance significant level even exceptions generally strategies based single-output structure perform significantly worse multiple-output structure. concerning current leading strategies direct strategy significantly outperforms iterated strategy majority prediction horizons. iterated strategy performs poorest statistical confidence level cases even exceptions. pso-mismo ga-mismo convergence computational time compared proposed pso-mismo five competitors terms prediction accuracy. subsection convergence computational time heuristic-based prediction strategies examined. noted although used number time series datasets i.e. logistic mackey-glass datasets general results change much within time series datasets. therefore save space multi-step-ahead time series prediction usually proved intractable task growing amount uncertainties arising various sources. instance accumulation errors information make multi-step-ahead prediction difficult. thus modeling strategies multi-step-ahead prediction major research topics significant practical implications. contribution study extension well-established mismo modeling strategy incorporating heuristic based binary particle swarm optimization mismo modeling process self-adaptively determine number sub-models varying prediction horizons conduct large scale comparative study neural networks validation. quantitative comprehensive assessments performed simulated real time series basis prediction accuracy convergence computational time. experimental superiority proposed pso-mismo modeling strategy multi-step-ahead time series prediction. atiya el-shoura shaheen el-sherif comparison neural-network forecasting techniques-case study river flow forecasting\" neural networks ieee transactions vol. chevillon \"direct multi step estimation forecasting\" econ. surveys vol. sorjamaa reyhani lendasse \"methodology series\" neurocomputing vol. sorjamaa lendasse \"time series prediction using dirrec strategy\" presented european symposium artificial neural networks bruges bontempi \"long term time series prediction multi-input multi-output local learning\" proceedings european symposium time series prediction estsp helsinki finland taieb bontempi sorjamaa lendasse \"long-term prediction time series combining direct mimo strategies\" proceedings ieee international joint conference neural networks atlanta u.s.a. taieb sorjamaa bontempi \"multiple-output modeling forecasting\" neurocomputing vol. taieb bontempi atiya sorjamaa review comparison strategies multi-step ahead time series forecasting based forecasting competition\" expert syst. appl. hippert pedreira souza \"neural networks short-term load forecasting review evaluation\" power systems ieee transactions vol. sharma \"seasonal interannual rainfall probabilistic forecasts improved water supply management part strategy system predictor identification\" hydrol vol. zhang kline \"quarterly time-series forecasting neural networks\" neural networks ieee transactions vol. kennedy eberhart \"particle swarm optimization\" proceedings ieee international conference neural networks vol. juang hybrid genetic algorithm particle swarm optimization recurrent network design\" systems cybernetics part cybernetics ieee transactions vol. fig. convergence best average fitness pso-mismo ga-mismo strategies. logistic time series length=. mackey-glass time series length=. time series dataset. second test performed designed measure average computational time needed time series datasets within strategies. result shown table pso-mismo needs generally less time ga-mismo. table generally speaking thus experiments draw following three main conclusions. heuristic-based modeling strategies capable obtaining high-quality multi-step-ahead time series forecasts related four well-established strategies. performance pso-mismo better ga-mismo evolutionary point view. pso-mismo showed faster ga-mismo terms average running time. currently associate professor department management science information systems school management huazhong university science technology p.r.china. principal investigator research projects funded natural science foundation china served referee paper review several ieee journals international several international academic conferences. research interests time series modeling forecasting business intelligence data mining. currently working toward ph.d. degree management science engineering huazhong university science technology china. research include multi-step-ahead time series forecasting interval data analysis computational intelligence. hongyi received b.sc. degree information system harbin institute technology weihai p.r.china. received m.sc. degree management science huazhong university science technology p.r.china. currently working toward ph.d. degree management science engineering huazhong university science technology p.r.china. research interests include support vector machines swarm intelligence memetic algorithms time series forecasting. chen peng jian \"particle swarm optimization recombination dynamic linkage discovery\" systems cybernetics part cybernetics ieee transactions vol. wang effective pso-based memetic algorithm flow shop scheduling\" systems cybernetics part cybernetics ieee transactions vol. wunsch comparison study validity indices swarm-intelligence-based clustering\" systems cybernetics part cybernetics ieee transactions vol. nguyen yang self-learning particle swarm optimizer global optimization problems\" kennedy eberhart discrete binary version particle swarm algorithm\" systems cybernetics computational cybernetics simulation. ieee international conference orlando vol. jong \"parameter setting year perspective\" parameter setting evolutionary algorithms springer trelea \"the particle swarm optimization algorithm convergence analysis parameter selection\" inform process lett vol. kennedy kennedy eberhart swarm intelligence morgan kaufmann eberhart \"empirical study particle swarm optimization\" evolutionary computation proceedings congress wang y.-h. effective pso-based memetic algorithm flow shop scheduling\" systems cybernetics part cybernetics ieee transactions vol. rubio pomares rojas herrera heuristic method parameter selection ls-svm application time series prediction\" forecasting vol. l.-c. chang p.-a. chen f.-j. chang \"reinforced two-step-ahead weight adjustment technique online training recurrent neural networks\" neural networks learning systems ieee transactions vol. \"simultaneous training negatively correlated neural networks ensemble\" systems cybernetics part cybernetics ieee transactions vol. \"simple mathematical models complicated dynamics\" nature vol. mackey glass \"oscillation chaos physiological control systems\" science vol. goldberg \"genetic algorithms search optimization machine learning\" reading addison-wesley \"blgan bayesian learning genetic algorithm supporting negotiation incomplete information\" systems cybernetics part cybernetics ieee transactions vol. davis \"handbook genetic algorithms\" york nostrand reinhold golmohammadi creese valian kolassa \"supplier selection based neural network model using genetic algorithm\" neural networks ieee transactions vol. goldberg comparative analysis selection schemes used genetic algorithms\" urbana vol. baghmisheh madani navarbaf discrete shuffled frog optimization algorithm\" artif intell vol. haupt haupt practical genetic algorithms wiley-interscience elbeltagi hegazy grierson \"comparison among five evolutionary-based optimization algorithms\" inform vol. ramsay schaefer statistical sleuth duxbury boston mass", "year": 2013}