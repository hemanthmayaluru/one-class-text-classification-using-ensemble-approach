{"title": "Feature extraction using Latent Dirichlet Allocation and Neural  Networks: A case study on movie synopses", "tag": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "stat.ML"], "abstract": "Feature extraction has gained increasing attention in the field of machine learning, as in order to detect patterns, extract information, or predict future observations from big data, the urge of informative features is crucial. The process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high-dimensional space, to higher level meaningful abstractions. This dissertation employs Neural Networks for distributed paragraph representations, and Latent Dirichlet Allocation to capture higher level features of paragraph vectors. Although Neural Networks for distributed paragraph representations are considered the state of the art for extracting paragraph vectors, we show that a quick topic analysis model such as Latent Dirichlet Allocation can provide meaningful features too. We evaluate the two methods on the CMU Movie Summary Corpus, a collection of 25,203 movie plot summaries extracted from Wikipedia. Finally, for both approaches, we use K-Nearest Neighbors to discover similar movies, and plot the projected representations using T-Distributed Stochastic Neighbor Embedding to depict the context similarities. These similarities, expressed as movie distances, can be used for movies recommendation. The recommended movies of this approach are compared with the recommended movies from IMDB, which use a collaborative filtering recommendation approach, to show that our two models could constitute either an alternative or a supplementary recommendation approach.", "text": "introduction ................................................................................................................................. appendix ..................................................................................................................................... appendix ..................................................................................................................................... appendix ..................................................................................................................................... bibliography ................................................................................................................................... thanks enormous amount electronic data digitization material registration material sensor data governmental private digitization intentions general amount data available sorts expanding increasing last decade. simultaneously need automatic data organization tools search engines become obvious. naturally increased scientific interest activity related areas pattern recognition dimensionality reduction fields related mostly feature extraction. although history text categorization dates back introduction computers early text categorization become important part mainstream research text mining thanks increased application-oriented interest rapid development powerful hardware. categorization successfully proved strengths various contexts automatic document annotation document filtering automated metadata generation word sense disambiguation hierarchical categorization pages document organization name few. probabilistic models neural networks consist state methods extract features. efficiency scalability quality document classification algorithms heavily rely representation documents among set-theoretical algebraic probabilistic approaches vector space model representing documents vectors vector space used widely. dimensionality reduction term vector space important concept that addition increasing efficiency compact document representation also capable removing noise synonymy polysemy rare term use. examples dimensionality reduction include latent semantic analysis probabilistic latent semantic analysis deerwester proposed basic approaches topic modelling called lsi. method based theory linear algebra uses bag-of words assumption. core method apply co-occurrence count matrix documents terms obtain reduced dimensionality representation documents. thomas hofmann suggested model called probabilistic latent semantic indexing topic distributions words still estimated co-occurrence statistics within documents introduced latent topic variables model. plsi probabilistic method shown superior number applications including information retrieval since increasing focus using probabilistic modelling tool rather using linear algebra. interesting approach suitable dimensional representation generative probabilistic model text corpora namely latent dirichlet allocation blei jordan models every topic distribution words vocabulary every document distribution topics thereby latent topic mixture document reduced representation. according blei plsi shortcomings regard overfitting generation documents. motivating factors propose latent dirichlet allocation model quickly became popular widely used data mining natural language processing related topics. probabilistic topic models latent dirichlet allocation probabilistic latent semantic analysis model documents finite mixtures specialized distributions words known topics. important assumption underlying topic models documents generated first choosing document-specific distribution topics repeatedly selecting topic distribution drawing word topic selected. word order ignored document modelled bag-of-words. weakness approach however word order important component document structure irrelevant topic modelling. example sentences unigram statistics quite different topics. information order words used sentence help disambiguate possible topics. information chapter n-gram language models decompose probability string text sentence document product probabilities individual words given number previous words. differently models assume documents generated drawing word probability distribution specific context consisting immediately preceding words assuming local linguistic structure. figure features autoencoder neural network highly used machine learning technique uses paragraph vectors represent network input. paragraph vectors closely related n-grams able capture disadvantage bag-of-words models linguistic structure. precisely paragraph vector unsupervised algorithm learns fixed-length feature representations variable-length pieces texts sentences paragraphs documents. moreover neural networks represent dimensionality reduction models hidden layer smaller size input. precisely autoencoder neural network structure assumes output size input. characteristic helps interpreting data precisely hidden layer compact representation data. information chapter capturing features either probabilistic model neural network important find similarities extracted features. k-nearest neighbors approach uses cosine distance similarity measure. furthermore t-sne nonlinear dimensionality reduction technique well suited embedding high-dimensional data space three dimensions able transfer extracted features -dimensional space visualize features scatter plot. representing data captures respective distance similarities does human friendly interpretable way. information chapter urge informative features crucial order detect patterns extract information. thinking recommendation systems problem comes initial information needed recommend relevant object scope. beginning data users following time user-data sparse certain items speaking movies recommendation feature extraction movie plots constitutes clustering relevant movies recommend relevant ones according plots namely genre. thesis show feature extraction either probabilistic model state latent dirichlet allocation neural network namely autoencoder constitute significant base recommendation systems. among different recommendation approaches collaborative filtering techniques widely used largely domain independent require minimal information user item features still achieve accurate predictions even though manage prediction accuracy rating predictions highly increased content information proved moreover probabilistic topic model used movie text reviews show extracted features texts provide additional diversity. meanwhile autoencoder used recommendations algorithms. precisely content-based collaborative filtering recommendation methods algorithms used divided memory-based model-based approaches. memorybased methods predict users‚Äô preference based ratings similar users model-based methods rely prediction model using clustering thesis want testify autoencoder used methods feature extraction present astonishing movie recommendations reaffirming system makes content might able make predictions movie even absence ratings moreover compare methods recommending results collaborating filtering process imdb discover comparison among opic modeling classic problem natural language processing machine learning. chapter present latent dirichlet allocation successful generative latent topic models developed document collections discrete data. specifically unsupervised graphical model discover latent topics unlabeled data exact characteristic renders model prior main inspiration behind find proper modeling framework given discrete data namely text corpora thesis. essential objective dimensionality reduction data ribeiro-neto made significant progress field. researchers proposed reduce every document corpus vector real numbers represents ratios tallies. here documents represented finite dimensional vectors vocabulary size. weight represents much term contributes content document equal tf-idf value. scheme models every document corpus real valued vectors. weights reflect inverse document frequency terms occur frequently document lessen weight term relatively terms occur rarely increase weight corresponding term. rather large subsequently would likewise large. case implies important term document corpus term large discriminatory power. next important case equal zero basic definition declaring term occurs documents. similitude documents found several ways using tf-idf weights common cosine similarity. latent semantic analysis introduced deerwester method natural language processing particularly vector space model extracting representing contextual significance method dimensionality reduction matrix namely reconstructs matrix least possible information throwing away noise maintaining strong patterns. moreover information retrieval field find similar documents across languages analyzing base translated documents also find coherent documents query terms distribution poison distribution usually observed. nevertheless better perform many using probabilistic latent semantic analysis preferred method lsa. robabilistic latent semantic analysis probabilistic latent semantic indexing field information retrieval) embeds idea probabilistic framework. contradictory linear algebra method plsa sets foundations statistical inference. plsa originated domain statistics thomas hofmann later domain machine learning blei jordan plsa considered generative model topic distributions words still estimated co-occurrence statistics within documents latent topic variables introduced model. probabilistic latent semantic analysis prominently applications information retrieval natural language processing machine learning. plsa also used bioinformatics. plsa practices generative latent class model accomplish probabilistic mixture decomposition. models every document topic-mixture. mixture assigned documents individually without generative process mixture weights learned expectation maximization. aspect model aspect model statistical model plsa relies assumes every document mixture latent aspects. aspect model latent variable model co-occurrence data associates unobserved class variable observation occurrence word particular document. figure illustrates generative model word-document co-occurrence. first plsa selects document probability picks latent variable probability finally generates word probability result joint distribution model presented expression plsa statistical latent variable model introduces conditional independence assumption namely independent associated latent variable. parameters model according conditional independence assumption number parameters equals namely grows linearly number documents. parameters estimated maximum likelihood expectation maximization typical procedure substitutes steps expectation step posterior probabilities latent variables calculated maximization step parameters redesigned. -step implies apply bayes‚Äô formula equation -step calculates expected complete data-log-likelihood depends outcome first step update parameters. -step plsa influenced solution unsatisfactory statistical foundation. main difference objective function; based gaussian assumption plsa relies likelihood function multinomial sampling aims explicit maximization predictive power model. figure notice difference lsa‚Äôs plsa‚Äôs perplexity collection documents clearly revealing plsa‚Äôs predictive supremacy lsa. noted disadvantages section gaussian distribution hardly stands count data. dimensionality reduction plsa performs method aspects keeps steps alternated termination condition. either convergence parameters early stopping namely cancel updating parameters performance improving. singular values method. consequently selection proper value heuristic plsa‚Äòs model selection determine optimal maximizing predictive power. regarding computational complexity plsa counting iterations) computed precisely iterative method affirms find local maximum likelihood function. however hofmann‚Äôs experiments shown using regularization techniques ‚Äòearly stopping‚Äô order avoid overfitting presented global local maximums even poor local maximum plsa might performs better lsa‚Äôs solution. latent dirichlet allocation generative probabilistic topic model collections discrete data representative example topic modeling first presented graphical model topic high modularity easily extended giving much interest study. also instant mixed membership model according bayesian analysis document associated years main issue find information. nowadays human knowledge behavior high percentage digitized precisely scientific articles books blogs emails images sound detect patterns unstructured collection documents organize electronic archives scale would inconceivable human annotation. topic models statistical models initial information smaller interpretable space keeping essential statistical dependences facilitate efficient processing data. speaking text corpora collection data analysis declaring nature article mixture multiple topics intuition model. generative probabilistic model treats data observations arrive thematic structure collection topics histogram right figure representing document‚Äôs topic distribution. moreover word mixture topic represented left also given procedure assumes order words necessarily matter. even though documents would unreadable words shuffled able discover thematically coherence terms multiple topics different probabilities. every topic contains probability every word even though word high probability topic might greater another. algorithm information topics. inferred topic distributions generated computing hidden structure observed documents. problem inferring latent topic generative process first model document poisson distribution words. first step presented algorithm considered known line also seen lda‚Äôs notation figure second topic assume random variable ùúë‚Éó‚Éóùëò dirichlet distribution words topic parameterized v-dimension vector positive reals summing estimated. third document compute random variable dirichlet distribution topics occurring document parameterized Œ∫-dimension vector positive reals summing one. document following steps repeated every word document first identify topic word multinomial distribution given topic distribution document secondly choose term ùúë‚Éó‚Éóùëò word distribution chosen topic forward step. steps show documents exhibit latent topics different documents corpus goal estimate posterior distribution hidden variables first defining joint probability distribution observed latent random variables. representation. hyperparameters corpus level parameters assumed sampled process generating corpus. variable document-level variables computed document. finally variables word level variables sampled word document smoothed p.and word level variables sampled word document smoothed words within document moreover every single circled node declares hidden variable unique double circled node represents observed variable words corpus. figure illustrates conditional dependences define model. specifically topic indicator ùëß‚Éóùëöùëõ depends topic proportions document futhermore observed word ùë§‚Éó‚Éó‚Éóùëöùëõ depends ùëß‚Éóùëöùëõ topic indicator ùúë‚Éó‚Éóùëò word distribution topic ùëß‚Éóùëöùëõ indicates. choice topic assignment choice word word distribution topic ùë§‚Éó‚Éó‚Éóùëöùëõ represented multinomial distributions ùúë‚Éó‚Éóùëò respectively. here multinomial distribution assigned multinomial trial. case multinomial distribution called \"generalized bernoulli distribution\" less precisely \"discrete distribution\") probability distribution describes result random event take possible outcomes outcomes numerical labels attached convenience describing distribution often range note k-dimensional categorical distribution general distribution model assume topic mixture proportions document drawn distribution. want distribution multinomials. k-tuples non-negative numbers one. represent topic proportions document word distributions topic ùúë‚Éó‚Éóùëò topic simplex vocabulary simplex repsectively. simplex assume geometric interpretation multinomials. precisely assume single values figure come dirichlet distribution. event observed times. bayesian statisitcs dirichlet distribution used prior distribution namely probability distribution event evidence about. representation k-dimensional dirichlet random variable v-dimensional dirichlet random variable simplex deeper analyzed section understand concetration plsa multinomial random variable model learns topic mixtures training documents thus plsa fully generative model particularly level documents since clear solution assign probability previously unseen document. lead consequence llustrating differences latent space document geometrically represented model. document distribution words observe distribution point simplex namely document‚Äôs word simplex. documents represented mixture topics topic particular probability generating words. thus assumes data arise generative process includes hidden setting hidden variables. denominator marginal probability observations probability seeing observed corpus topic model. posterior distribution cannot algorithms variational algorithms. sampling based algorithms collect samples posterior approximate empirical distribution variational methods place parameterized class algorithm statistical physics. algorithm described geman brothers eight decades gibbs‚Äôs death. part gibbs sampler work presented based thesis istv√°n b√≠r√≥ gibbs sampling applicable joint distribution known surely difficult sample burn-in period stabilizes eliminating influence initialization parameters. mcmc simulation probability distribution follows dimensions sampled alternately time conditioned values dimensions denoted ùë•‚Éó‚Éó‚Éó‚àíùëñ order construct gibbs sampler estimate probability distribution ùëß‚Éó‚Éó‚Éó ùë§‚Éó‚Éó‚Éó denotes word-positions corpus. distribution directly proportional joint distribution joint distribution cannot inferred denominator summation terms. gibbs sampling requires full conditionals infer first estimate joint distribution ecause first term independent conditional independence ùë§‚Éó‚Éó‚Éó given ùëß‚Éó‚Éó‚Éó second term independent elements joint distribution managed independently. first term obtained simply words corpus observed according independent multinomial trials parameters conditioned topic indices second equation split product words product topics vocabulary separating contributions topics. term denotes number times term observed topic distribution obtained integrating done using dirichlet integrals within product computing corresponding count. note terms products contain index others cancelled out. knowledge calculate values given state markov chain current samples done posterior estimation predicting distribution topic-word pair observed document given state ibbs sampler algorithm using equations approximate infer initial wanted posterior distribution. moreover state keys gibbs sampling algorithm. initial stage sampling process namely burn-in period gibbs samples discarded poor estimates posterior. burn-in period successive gibbs samples start approximate target posterior topic assignments. representative samples distribution number gibbs samples saved regularly spaced intervals prevent correlations samples. utoencoder autoassociator diabolo network special sort artificial neural networks. neural networks stated mcculloch pitts inspired biological neural networks hidden layer lower dimension. compressed representation data leads representation reduced space renders model capable discovering latent concepts recognition image retrieval ability recommendation. neural networks first described mcculloch pitts possible approach applications system consisting neurons learning algorithm capable approximating non-linear functions inputs. weight-values comprise flexible part neural network define behavior. appropriately eural networks basic functions nonlinear functions linear combination inputs stated bishop subsequently basic neural network model first construct linear combinations inputs superscript denoted first layer network. parameter weight input represents bias. finally represents activation. activation transformed nonlinear activation function usually chosen sigmoidal function functions take input weighted values coming units connected note output values function range never make never negative denominator fraction tends gets negative direction tends gets positive direction. tendency comes easily output layers parameterized weights. finding proper network functions various learning tasks performed minimizing cost function network function weights. igure illustrates general architecture behind autoencoder neural defined represents input/output layer denoted vector declares hidden layer bottleneck denoted vector positive integers. transformation class similar class training vectors dissimilarity distortion function defined utoencoders autoassociators special kind artificial neural networks provide fundamental paradigm unsupervised learning. first introduced hinton address problem backpropagation without teacher\" using input data avoid need teacher later related concept sparse coding presented olshausen input output layer size smaller hidden layer between. autoencoder tries reconstruct input vector output layer much accuracy learning input lower dimension space. thus network evaluated evaluating input hidden layer output layers. goal reconstruct input-vector output layer precisely possible network back-propagated error reconstruction original pattern. smaller sized hidden layer represent larger input data. therefore system learns compressed representation data. activation hidden layer provides compressed representation data encoding data. recently autoencoders come forefront deep architecture neural networks create stacked autoencoders capable learn deep networks deep architectures shown present great results number challenging classification regression problems. autoencoder simplest representation feedforward non-recurrent neural network similar multilayer perceptron. difference autoencoder output layer size input layer instead training neural predict target value autoencoder trained reconstruct inputs framework autoencoder represented baldi enerally options autoencoder network. mainly refer autoencoder neural network assume layer hidden nodes often referred bottleneck smaller dimension input output layer. figure precisely p<n. structure results compression data smaller dimension decompressing output space resulting butterfly structure figure notice hidden layers narrower input-output layers subsequently activations final hidden layer regarded compressed representation input. moreover limit number hidden units discover interesting structure data. general speaking autoencoders presented also deep networks symmetric topology number hidden layers containing encoder dimensional representation decoder center small layer operating bottleneck. hidden layers narrower input/output layers activations final hidden layer regarded compressed representation input. usual activation functions mlps used autoencoders; significantly number hidden units smaller either number input output units transformations network generate general possible linear transformations inputs outputs information lost dimensionality reduction hidden units. case sigmoid functions like logical hyperbolic function. moreover linear activations used single sigmoid hidden layer optimal solution auto-encoder strongly related principal component analysis hand hidden layers bigger size input layer. case autoencoder potentially learn identity function become useless; however experimental results shown autoencoders might still learn useful features case reasons autoencoder network preferred recall applications linear nonlinear relationships inputs. though autoencoders unsupervised networks assumption output known renders supervised comportment model. autoencoders backpropagation method training method backpropagation method implementing gradient descent method requires known desired output input value order calculate loss function gradient. consequently autoencoders simple learning circuits transform inputs outputs least possible amount distortion force output multi-layer neural network input tiny layer middle form bottleneck value bottleneck layer forced efficient code input. many layers non-linearities codes found autoencoder compact powerful. ackpropagation method abbreviation \"backward propagation errors\" typical method training artificial neural networks used combination optimization method like gradient descent. method evaluates gradient loss-error function according weights network. gradient driven chosen optimization method turn uses update weights attempt minimize loss function. backpropagation requires known desired output input value order calculate loss function gradient subsequently explained. backpropagation method usually used supervised method although also used unsupervised networks autoencoders output considered known. igure illustrates forward backward processes backpropagation method. precisely noted activation functions every node generated going forward neural meet final node representing error function noted derivatives nodes travelling backward last node initial node adjust weight path. procedure iterative paths covered step forward propagation equations stating activation unit input another unit final output. step represent flow information neural net. consider derivative respect weight ùë§ùëñùëó. outputs various units depend particular input pattern however order keep notation uncluttered shall omit subscript network variables. first note depends weight summed input unit therefore apply chain rule partial derivatives give words. therefore features words hidden neurons network learnt weight inputs. would form matrix sized weights/features learnt depicted figure column represents weights/features learnt words vocabulary herefore training would enlarged weights/features learnt word would represent cases slightly different producing versatile training set. precisely layer used predict fifth word output layer capturing initial intuition using word vectors predict word precisely given sequence training words find objective word vector model maximize average probability astonishing models even though simple understand applied importantly increase degree language understanding using basic mathematical operations translating language‚Äôs vector another language statistical machine translation uses linear projection performing rotation scaling. linear projections used training large space also close values. specifically minimizing objective function attempt ensure close also close. problem also known semi-supervised label propagation introduced euclidean distance grows weight decrease exponentially tending infinity. means data points long d-dimensional euclidean distance pushed much also described section autoencoder unsupervised neural network assumes output known. considering goal reconstruct input-vector output layer well possible representations pieces texts like sentences paragraphs whole documents extending wordvec model seen section respectively wordvec model docvec model represents vector model. precisely every paragraph mapped unique vector represented column matrix while every word also mapped unique vector-col matrix subsequently difference distributed words model obtaining paragraph vectors close skipgram model word vectors models. precisely pv-dbow forces model predict words randomly advantages using paragraph vectors instead models feature representation mainly held capability confront weaknesses bag-of-words models namely semantics shown distributed memory model outperforms distributed words model. meanwhile distributed words model stores less data distributed memory model paragraph learning distributed representation data namely assumes every output non-linear function linear combination hidden units finds features result precisely movie dataset test hypothesis illustrate extracted features better human apprehension. moreover though waiting autoencoder give better conceptual results preprocessing. precisely metadata includes following wikipedia movie freebase movie movie name movie release date movie office revenue movie runtime movie languages movie remaining words filtered frequency using term frequency inverse document frequency score tf-idf measures importance word corpus seen section increases sentences paragraphs entire documents. docvec class extends gensim‚Äôs original wordvec class. review theory representing features autoencoder chapter find information mikolov experiments showed distributed memory algorithm performs noticeably better distributed words algorithm pv-dm default algorithm running implementation) uses unsupervised learner implementing neighbor searches. algorithm receives input-vectors probabilities movie plot amelioration stochastic neighbor embedding first developed hinton roweis t-sne differs gives better visualizations reducing central crowd better non-parametric visualization techniques including sammon mapping isomap locally linear embedding. widely used many areas music analysis cancer principal components analysis classical multidimensional scaling primarily t-sne mainly focuses appropriately modeling small pairwise distances i.e. local structure t-sne technique t-sne non-linear method subsequently keeps low-dimensional representations similar datapoints close together linear methods focus keeping low-dimensional pairs high dimensional objects ascribe high probability similar objects infinitesimal probability dissimilar ones. pairwise similarities low-dimensional given ometimes datapoint outlier namely values small location low-dimensional point little effect cost function. solve problem defining joint probabilities high-dimensional space symmetrized conditional present characteristic captured topics. often seen words every topic testify topic theme. observing word probabilities topic understand power topic theme capture. moreover words different topics observed different proportion capturing intuition behind documents exhibit multiple topics different word captured -dimensional space. moreover every algorithm compare recommendation power imdb performs collaborative filtering movies recommendation. section going review outcomes symmetric latent dirichlet allocation. described section topics interesting wait differences topics‚Äô sparsity results symmetric asymmetric section source code presented appendix topic strongest movie captured corpus symmetric term school holds almost topic vocabulary declaring topic school life precisely high school life next likely word topic term high. observe similar word-topic-theme relationships presented topics. accordingly topic related relationships occurent words wife husband marriage also highly relationships likely words topic words like divorce lover suicide topic occurrence. surprisingly manages capture ideas behind movies. example topic again wife husband wife bigger occurrence topic someone could estimate movies related topic observe relationship feminin aspect. respectively topic mainly family movie categoy shows father likely occurrence topic bigger rate mother. father movies seems bigger role mother. opic represents part movies money watching next shown terms take information movies plots. topic presents linked animations mainly princesses palaces. topic religious topic topic deals music. ccordingly topic linked shows directors hollywood topic airplane plots. topic asian movies. topic assume involves movies respectively topic finally vocabulary topic reveals movies earth threatened. observe term every topic appears topic vocabulary likely term highly divergence. first two-three likely terms understand basic thematic structure topics. moreover capturing number likely terms category movie give better intepretation contexts. presented topics present topics mainly talk wars. capturing number terms every topic manage capture subcategories. watch topics topics related war. watching terms shown every them observe subcategories topic related federal mainly linked america. topic words like camp german jewish understanding relationship world probably. topic adventure thriller guns fights. topic linked police-gangsters movies closely related topic linked police investigations. example topics includes term police high percentages subsequently closely related topics. finally topic actual topic mostly related topic here plot t-sne movies dataset. capture close neighborhoods related movies. close thematic stricture presented topics section captured space. capturing whole dataset -dimensional space first representation better understand plots find nearest movies chosen movie. give nearest twenty movies namely twenty movies less distance chosen movie. alternate feature representation. movie selected star wars episode empire strikes back observe symmetric depicts closest movies star wars episode empire strikes back mainly movies relevant space terms. imdb proposes movies relevant adventure highly concept star wars. point witness bag-of-word-assumption weakness capture higher linguistic terminologies semantics discussed hand proposed movies conceptually linked selected movie way. imdb surprisingly proposes forrest gump according user evaluation collaborative filtering. movie selected lord rings fellowship ring imdb recommendations captures better representation lord rings fellowship ring star movie observe smaller distances. moreover recommendations movies. intuition recommendation methods clear example captures contexts even assumption collaborative filtering proposes known movies wide range topic though manages capture. autoencoder contrast estimate paragraph vectors distribution words compressed information hidden layer encoder manage represent initial information reduces space. here space -dimensional space namely -hidden topics. utoencoder performs excellent representations features. closely related movies star wars lord rings story really near encircled movies share semantics. better view interpretation knn. begin with review movie‚Äôs representation models testify thought would theory autoencoder uses paragraph vectors outperforms lda. autoencoder captures precisely thematic structure semantic relations documents. seen parallel below models feature extraction movie database showing relationships movies distances among them. captured representing distances extracted data either filtering even though manage reflect astonishing relationships autoencoder outperforming three models able capture best results. consequently private systems tracking gain increased space recommendation systems held content field much interest. extension shown methods feature extraction mainly used", "year": 2016}