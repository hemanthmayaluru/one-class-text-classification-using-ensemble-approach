{"title": "Multimodal Affect Analysis for Product Feedback Assessment", "tag": ["cs.HC", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Consumers often react expressively to products such as food samples, perfume, jewelry, sunglasses, and clothing accessories. This research discusses a multimodal affect recognition system developed to classify whether a consumer likes or dislikes a product tested at a counter or kiosk, by analyzing the consumer's facial expression, body posture, hand gestures, and voice after testing the product. A depth-capable camera and microphone system - Kinect for Windows - is utilized. An emotion identification engine has been developed to analyze the images and voice to determine affective state of the customer. The image is segmented using skin color and adaptive threshold. Face, body and hands are detected using the Haar cascade classifier. Canny edges are identified and the lip, body and hand contours are extracted using spatial filtering. Edge count and orientation around the mouth, cheeks, eyes, shoulders, fingers and the location of the edges are used as features. Classification is done by an emotion template mapping algorithm and training a classifier using support vector machines. The real-time performance, accuracy and feasibility for multimodal affect recognition in feedback assessment are evaluated.", "text": "consumers often react expressively products food samples perfume jewelry sunglasses clothing accessories. research discusses multimodal affect recognition system developed classify whether consumer likes dislikes product tested counter kiosk analyzing consumer’s facial expression body posture hand gestures voice testing product. depth-capable camera microphone system kinect windows utilized. emotion identification engine developed analyze images voice determine affective state customer. image segmented using skin color adaptive threshold. face body hands detected using haar cascade classifier. canny edges identified body hand contours extracted using spatial filtering. edge count orientation around mouth cheeks eyes shoulders fingers location edges used features. classification done emotion template mapping algorithm training classifier using support vector machines. real-time performance accuracy feasibility multimodal affect recognition feedback assessment evaluated. introduction people rely multiple modalities express identify affect considerable research unimodal bimodal affect recognition past decade. bartlett provided automatic facial action detection results spontaneous expressions. another work performed automated facial expression recognition real time based optical flow. research bimodal affect recognition focused using body hand gesture speech second modality addition face. recognition rates using modality reported higher unimodal affect recognition rates study multimodal affect recognition gained significant attention researchers recent years. experiments done aimed detecting interest level child solving puzzle using sensory information face body posture. spontaneous smiles analyzed using multiple modalities work done cohn fusion data face body speech feature level result level compared work done discussed effects user interaction automated affect recognition. multimodal affect analysis performed speech interaction user agent bayesian classifier used study audio visual cues application multimodal affect recognition interactive interfaces assessed emotion synthesis using virtual agent evaluated facial gesture posture used emotion recognition. despite ongoing research automated multimodal affect recognition systems exist challenges reliable fusion techniques noise processing speed discussed survey research paper performs initial study multimodal affect recognition system containing main components. first component frame based feature point skeletal joint image processing speech recognition component second component responsible offline affect recognition tracked feature data. system used multiple modalities video speech facial expressions body posture head hand position input. study used precision recall tracking processing time metrics assessment tracking affect recognition components using multiple modalities. furthermore paper evaluated feasibility multimodal affect recognition system application obtaining product feedback customer behavior. multimodal affect recognition system developed determine customer exhibits negative affect unhappy disgusted frustrated angry positive affect happy satisfied content product offered. system used four different scenarios involving product feedback. instance multimodal affect recognition system used feedback assessment customer tests sample food perfume jewelry shoes sunglasses products invoke strong emotions person conveyed facial expressions body posture head hand position. additionally consider scenario customer returns product testing home. body language head position serve whether customer returning product unhappy product simply match needs. third scenario restaurant customers provide hints whether liked cuisine using facial expression head body posture. finally multimodal affect recognition system prove useful detect angry frustrated customer ticket counter waiting line analyzing body posture facial expression head position hand gestures speech. ability identify product feedback based automatic multimodal recognition affect prove useful determination popular products customer preferences. feedback data used manufacturers decide future product lines designs pricing. study done shown people display basic emotions facial expressions body posture hand head positions provide cues recognizing basic emotions. thus incorporated basic emotions indicators determine customer affect testing product. research examines automatic disgust anger recognition associates emotions dissatisfaction product. person exhibit disgust tasting recipe smelling awful perfume stinking product. customer display anger frustration return counter faulty electronic device delayed restaurant service show annoyance towards terrible hotel room service checkout counter. hand research focuses automatic recognition smile associates emotion overall customer satisfaction. customers often smile happy sunglasses tested test driving luxury dreams handling newly launched notebook cell phone. thus important objective research improve smile disgust anger recognition using multiple modalities importance emotions product feedback. research following issues investigated first measure precision recall rate recognition affect using individual modalities facial expression head hand position body posture speech compare results results obtained using multimodal affect recognition. second geometric features facial action units used studies successfully detect emotions facial expression modalities hand head. thus research uses skeletal facial tracking available depth sensing camera kinect sensor train classifier using support vector machines measures accuracy speed affect recognition using supervised learning. goal research implement prototypical multimodal affect recognition system uses kinect sensor. third research proposes word based lookup technique affect recognition speech emotion template mapping algorithm affect recognition facial images. template based algorithm determines affect based rules opposed supervised learning based approach. study uses snapshot image video feeds input module uses emotion template algorithm affect recognition. edge length location orientation count analyzed determine affect. analysis performed face detected haar cascade classifiers segmentation based skin color adaptive threshold. research objective find evidence whether template based mapping algorithm word lookup technique improve complement accuracy results traditional supervised learning approach. finally study measures processing times tracking multiple modalities determine feasibility real-time usage multimodal affect recognition system. rest paper organized following parts section describes features used facial expression tracking head position hand position body posture tracking. section explains emotion template algorithm approach affect recognition image snapshot analysis. section also focuses word lookup based approach affect recognition speech. section contains details prototype program multimodal affect recognition system. section provides results experiments. section discusses conclusions based research. features facial expression facial expressions tracked using face tracking available microsoft kinect software development toolkit. currently total facial points available tracking toolkit. available points non-rigid points around eyebrows eyes mouth cheeks tracked. feature vector defined using technique described technique geometric features distance angle horizontal axis co-ordinates tracked points used. feature vector facial expression modality screen coordinate location tracked points euclidean distance pair points angle straight line passing pair points horizontal axis used. feature vector facial expression modality defined follows head position head position tracked using face tracking available microsoft kinect toolkit. head position tracking points used described follows skull leftmost side skull rightmost side skull left point forehead right point forehead center point nose point nose point left cheek point right cheek point left side chin point right side chin point bottom part chin. feature vector head position modality screen coordinate location tracked points euclidean distance pair points angle straight line passing pair points horizontal axis used. feature vector head position modality defined follows value co-ordinate value tracked point frame euclidean distance distinct tracked points. angle distinct tracked points. hand position hand position tracked using skeletal tracking available microsoft kinect api. hand position tracking points used. left shoulder left elbow left wrist left palm right shoulder right elbow right wrist right shoulder tracked. feature vector hand position modality screen coordinate location tracked points euclidean distance pair points value co-ordinate value tracked point frame euclidean distance distinct tracked points. angle distinct tracked points. body posture body posture tracked using skeletal tracking available microsoft kinect api. body posture tracking region waist considered. cases system used counter upper body visible person’s lower body would occluded. total points including points previously discussed hand position tracking points consisting head shoulder center spine center left right tracked. feature vector body posture modality screen coordinate location tracked points euclidean distance pair points angle straight line passing pair points horizontal axis used. feature vector body posture modality defined follows image snapshot speech analysis emotion template mapping algorithm video input kinect sensor multimodal affect recognition system form color depth skeletal data processed tracking detecting facial points joints hand torso. addition tracked points joints research also introduces concept image snapshot. given point input data skeleton contains frames color images skeletal data depth data. affect recognition system takes snapshot color frame interval seconds resulting image. interval determined based processing time results discussed detail section image emotion template algorithm recognizing affect face. segmentation skin color adaptive threshold face region detected using haar cascade classifier mouth region detected using haar cascade classifier serves region interest template mapping algorithm. edges mouth region identified using canny edge detection. library used segmentation emgucv wrapper opencv. mouth region analyzed affect recognition system using emotion template algorithm. study developed emotion template detecting smile. result image snapshot processing combined modalities decision level explained section rules used emotion template mapping algorithm recognize smile facial expression described below edge left right half mouth region length threshold values location coordinates edge bounded region mouth respectively algorithm inspects edge count edge satisfies rule rule rule equation respectively classifies mouth region region contains smile. word lookup research proposes word lookup technique affect recognition speech. microsoft speech recognition used purpose combination words associated certain class emotion. file stored file containing rules vocabulary disgust anger happiness. word lookup based approach depends speech recognition accuracy. speech assigns confidence level recognized word. threshold used selecting recognized word word lookup speech modality currently evaluated using english language. final affect recognized based keyword lookup combined outcome modalities decision level. study evaluated disgust recognition using keyword lookup technique. words defined recognizing disgust shown below decision level fusion study support vector machine based classifier trained modality. although research focuses emotions basic emotions classifier modality trained using data emotions. different individuals enacted different emotions single recording session emotion. session frames long. multimodal affect recognition system used capture data modality resulting dataset combination emotion modality person. input classifier feature vector using tracked points angles euclidean distance discussed section frame basis. thus output classifier modality generalized series predictions input session containing frames empirical probability emotion frames given number times emotion identified classifier. contains basic emotions denoted number anger denoted happiness denoted surprise denoted disgust denoted fear denoted sadness denoted neutral denoted thus probability emotion measured test input frames denoted anger happiness disgust fear sadness neutral respectively. final emotion identified modality given system prototype multimodal affect recognition system prototype developed using scripting language windows presentation foundation image processing classifier training affect recognition implemented using emgucv library. facial expression head tracking hand body position tracking implemented using skeletal tracking microsoft kinect toolkit api. user interface consists viewing area video skeletal frame facial expression toolbox area consisting command buttons checkboxes. dropdown tool used selecting enacted emotions training purpose. voice input available kinect sensor keyword lookup based affect recognition supported using microsoft speech api. system functionality includes ability record video speech input training classifier detecting emotions real time. modality input tracking affect recognition switched using check boxes affect tool box. system also checkbox enable cross validation function performs -fold cross validation. data stored text files containing comma separated feature values frame emotion modality used. line data file represents feature vector. button start stop input kinect sensor. record button used begin tracking feature points real time depending selected modalities. detect button used offline affect recognition captured data. train button used create based classifier model. three tabs camera motion face used viewing areas color video hand body positions tracking face head tracking respectively. results first study discussed offline affect recognition component results using classification rate recall precision. evaluation done using -fold cross validation. videos real time enacted video sessions front kinect sensor. enacted sessions performed meter meters range limitation sensor near frontal view controlled lighting conditions. data kinect sensor stored comma separated text file containing extracted feature vectors recognized class. ensured video recording viewing screen area affect recognition system. classification results shown table individual modalities. column contains comma separated results disgust anger smile respectively. overall classification rate recall rate lower expected. also observed recall rate classifier affinity towards detecting anger compared emotion classes. precision rate values modest scope improvement using multiple modalities. template based mapping algorithm shows recognition rate indicates result oriented approach yield good results compared traditional supervised learning techniques important rules algorithm depends exhaustive. face database used validation template based mapping algorithm. current algorithm would need improved used value multimodal classification rates. keyword lookup based approach affect recognition speech showed good precision results compared modalities. lower recall rates could attributed dependency speech recognition accents test subjects. table provides classification results multimodal affect recognition system. measurements agree findings existing research multiple modalities improve recognition rates observed using unimodal affect recognition techniques. moreover better precision rate detecting disgust anger smile using multimodal affect recognition compared unimodal measurements indicate affect recognition system performed using multiple modalities instead individual modalities product feedback assessment. secondly study measures processing time seconds affect detection using multiple modalities frame. individual session decision level affect recognition processing time within second receiving outcomes modality shown results table also provides results affect recognition processing times individual modality total processing time. total affect recognition processing time frame calculated follows time taken recognize emotion using modality processing time taken decision level fusion. results indicate tracked frames processed second indicates current implementation multimodal affect recognition component better suited offline execution. context product feedback assessment acceptable since real time tracked features processed offline feedback analysis. finally study measures processing time real time tracking features using individual multiple modalities. metrics used input frames received second shown first column average time ttrack taken tracking frame shown second column table case facial expression modality frequency candidate frames input system frames/sec depends underlying hardware kinect sensor processing time. e.g. given input session seconds customer provides feedback actual number candidate frames received processed system keyword based lookup technique average time milliseconds measured emotion recognition performing lookup. multimodal affect recognition system prototype implemented component handles processing facial head tracking another component processes hand body posture tracking resulting similar frames/sec ttrack/frame values pair modality. study measures real-time performance using number actual tracked frames modality compares number actual tracked frames multimodal system. real-time tracking performance degrades significantly still feasible multimodal affect recognition system real time tracking data used product feedback assessment offline mode. precision gain certainly outweighs lesser processing time multiple modalities since affect recognition component used offline. conclusion study shown multimodal affect recognition feasible application product feedback assessment system performing disgust anger happiness recognition. results also show real time multimodal feature tracking offline affect recognition certainly feasible using kinect sensor better accuracy compared individual modalities decision level fusion strategy provided high recognition results agree existing findings classification rates recall rates indicate experiments need performed using additional features training data. study find evidence using template based mapping algorithm keyword look based approach affect recognition speech significantly improve results traditional supervised learning techniques decision level. future scope effectiveness template mapping algorithm keyword based lookup need investigated using rules vocabulary emotions addition smile disgust. prototype multimodal affect recognition system successfully implemented. system provided functionality ability capture features facial expressions voice head hand positions body postures real time. system also provided ability train classifiers automatically detect affect using multiple modalities offline mode.", "year": 2017}