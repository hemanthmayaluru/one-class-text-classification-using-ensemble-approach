{"title": "Formulation of Deep Reinforcement Learning Architecture Toward  Autonomous Driving for On-Ramp Merge", "tag": ["cs.LG", "cs.AI"], "abstract": "Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions.", "text": "abstractâ€” multiple automakers development production automated driving systems offer freeway-pilot functions. type typically limited restricted-access freeways only transition manual automated modes takes place ramp merging process completed manually. major challenge extend automation ramp merging automated vehicle needs incorporate optimize long-term objectives near-term actions must safely executed. moreover merging process involves interactions vehicles whose behaviors sometimes hard predict influence merging vehicleâ€™s optimal actions. tackle complicated control problem propose apply deep reinforcement learning techniques finding optimal driving policy maximizing interactive environment. specifically apply long short-term memory architecture model interactive environment internal state containing historical driving information conveyed deep q-network used approximate q-function takes internal state input generates q-values output action selection. architecture historical impact interactive environment long-term reward captured taken account deciding optimal control policy. proposed architecture potential extended applied autonomous driving scenarios driving complex intersection changing lanes varying traffic flow conditions. highly fully automated systems tesla autopilot google self-driving widely available major automakers. automakers likely offer partially highly automated features automation level near future provided volvo swedish drive experiment. despite advancement high automation levels proposed demonstrated implementation autonomous driving highway on-ramp merge still presents considerable challenges. first needs decide immediate actions consideration delayed impacts future vehicle states. example actions accelerating decelerating steering vehicle takes current time affect success failure merge mission. process handled relative ease cases experienced human drivers algorithms automatic execution merge maneuver consistently smooth safe reliable manner become complex. second vehicleâ€™s merge maneuver depends states actions also interaction surrounding vehicles cooperative adversarial. typical scenario vehicle mainline arriving merge point behind operate cooperatively merging vehicle merge adversarially deter merging vehicle entering mainline traffic. interaction trivial serious risks emerge vehicle fails respond potentially complicated interaction. paper propose machine learning framework deep reinforcement learning achieve robust reliable merging policy. approach vital importance learn interactive environment optimize long term cumulative formulate architectural framework based deep reinforcement learning techniques on-ramp merge problem tackle issues attaining long-term effects mitigating unexpected adversarial impacts agents dealing system continuous states/actions. literature view related works described next section followed proposed architecture methodology. then implementation procedure ramp-merge problem presented. finally concluding remarks discussions given closing section. several modeling methods previously suggested solve autonomous on-ramp merging problem assuming specific rules. davis presented cooperative merging strategy vehicles mainline always slow create enough space on-ramp vehicle merge into. marinescu proposed slot-based merging algorithm defined slotâ€™s occupancy status based mainline vehiclesâ€™ speed position behavior acceleration deceleration. chen used driving rules acceptance theory model decision-making process urban expressway on-ramp merge problem. rule-based models conceptually comprehensible pragmatically vulnerable inability adapt unforeseen situations real world. words merge maneuvers executed predefined rules cases outside domains rules models fail lead hazardous situations e.g. crashes severe disturbance traffic flows. machine learning models contrast potential superior dealing complex situations without resorting detailed hard-coded rules pre-determined models. particularly reinforcement learning different standard supervised learning techniques need ground truth input/output pairs efficiently learn optimal actions trials errors reinforcement learning agent observes environment interacts taking actions balanced exploration uncharted territory exploitation current knowledge. receives immediate rewards system moves state. maximizing cumulative reward agent finds optimal policy achieve goal. reinforcement learning extensively applied field robotics recently applied vehicle traffic control problems. fares designed reinforcement learning based density control agent control number vehicles entering mainline ramp merging area. yang developed rampmetering control algorithm based reinforcement learning increase capacity weaving sections. applications basic q-learning state space action space discrete problem considered markov decision process ramp merging much complex. driving environment includes merging vehicleâ€™s state also dynamic states agents necessarily predictable view merging agent. therefore on-ramp merge case intrinsically nonmarkovian problem. moreover vehicleâ€™s state space action space continuous makes impractical tabular settings basic q-learning. instead q-function approximation good deal non-mdp partially observed markov decision process continuous state/action space. example google deepmind successfully applied deep q-network play atari games screen images game scores inputs. studies applied similar reinforcement learning framework browser-based simulators implement autonomous vehicle control specific scenarios. instance sallab used end-to-end deep reinforcement learning lane-keeping assist open-source simulator racing called investigated deep reinforcement learning training agent control simulated running track javascript racer. applications inputs game screens represent simplified real-world driving rules policy learned applied virtual video games. mobileye used another approach employed supervised learning models describe interactive environment recurrent neural network based models learn optimal policy. shalev-shwartz decompose driving strategy learnable part estimate comfort driving non-learnable part hard constrains safety driving. associated shortcomings show basic q-learning appropriate handling problems non-mdp properties continuous states/actions deep q-network structure limited image inputs ready implemented real-world driving scenarios. besides historical driving information extensively incorporated studies. important develop highly representative model describe real-world environment design appropriate q-function approximator long dependencies history learning robust reliable driving policy. study deep reinforcement learning incorporate influence historical information driving environment merging policy optimization. specifically model environment long shortterm memory architecture learn internal relations vehicle surrounding vehicles based relatively long duration past time. internal state representation lstm time step deep q-network action selection. that q-network immediately updated experience replay second target q-network avoid local optima divergence problems. interactive merging policy learned. overview architecture shown fig. first introduce architecture lstm architecture deep q-learning. lstm special recurrent neural network designed handle long-term dependency problems lstm ability remember values long short durations. mentioned earlier driving environment on-ramp merge scenario involves interactions surrounding vehicles necessarily predictable view vehicle. lstm historical driving information incorporated internal state adequate representation interactive environment. words internal state given lstm cell gives compact fixed-sized representation history q-network. study train lstm model supervised learning. architecture lstm model shown fig. lstm unit includes modules observation module state module. observation module used estimate next observation based inputs internal state action last time step. state module used current observation informative internal state employing vehicle action composed longitudinal control lateral control note action cannot arbitrarily large small values vehicle physical mechanics acceleration steering angle certain ranges within range acceleration steering real value. best action obtained highest q-value random noise added value chosen reward function measures safeness smoothness timeliness merging maneuver formulated function merging vehicleâ€™s acceleration steering angle speed distance surrounding vehicles shown equation predicted q-values target q-values equation calculated immediate reward ğ‘Ÿmaximum q-value next internal state ğ‘ -m&. ğ‘„-lâˆ’ğ‘„-k ğ‘„-k=ğ‘„ğ‘ -ğ‘-ğœƒ ğ‘„-l=ğ‘Ÿ-+ğ›¾ğ‘šğ‘ğ‘¥>q ğ‘„ğ‘ -m&ğ‘?ğœƒ% discounted factor q-network parameters updated backpropagation ğœƒâ‰”ğœƒ+âˆ‡w replay different q-function parameters break data correlation avoid sticking local saved action execution stored replay memory mini-batch sampled uniformly random ğ‘’&â€¦ğ‘’z used parameter updates supervised learning. parameters used estimation target q-values fixed number iterations updated periodically q-network parameters variables describe driving state speed position heading angle distances right left lane makings current lane. speeds positions total ğ‘-]ğœ‘-]ğ‘™] ğ‘Ÿ]ğ‘£-_ ğ‘-_ğ‘£-^ merge vehicle denoted ğ‘-=ğœ-]). immediate reward measured guarantee representativeness driving environment data train lstm model obtained real-world driving data. cameras placed high pole used record on-ramp merging scenarios bird view. video analysis performed obtain aforementioned observation action variables. video analysis inputs video images output vehicle dynamics. interactive driving behaviors merge vehicle vehicles learned internal state lstm units. deep q-learning process couple neural networks used calculate values based internal state time step. parameter update q-network conducted gradient descent. pseudo codes shown below. initialize q-network parameters initialize environment state internal state ğ‘ lstm input ğ‘ q-function approximator best action chosen action ğ‘-=ğ‘-âˆ—+ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ execute ğ‘obtain immediate reward ğ‘Ÿnext observation ğ‘œ-m& store transition element sample mini-batch uniformly random target ğ‘ \"m& ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™ ğ‘Ÿ\"+ğ›¾âˆ—ğ‘šğ‘ğ‘¥>qğ‘„ğ‘ \"m&ğ‘?;ğœƒ% ğ‘ \"m& ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™ calculate perform gradient descent ğœƒ%=ğœƒ iterations work propose deep reinforcement learning architecture learning on-ramp merge driving policy. driving environment based lstm architecture incorporate influence historical interactive driving behaviors action selection. deep q-learning process internal state lstm taken input q-function approximator used action selection based past information. q-network parameters updated experience replay second target q-network used relieve problems local optima instability. model training fine-tuning refinement proposed methodologies performance evaluation deep learning approaches remain topics studies. also envision another research topic call supervisory control highly automated driving systems worth pursuing. sensible operational concept system manual automated modes cooperatively co-exist instead forced switching modes. supervisory control term sheridan suggests human-machine systems exist spectrum automation shift across spectrum control levels real time suit situation hand. case ramp merge driver selectively provide control inputs automated driving system seeks maximize reward achieve successful effective merge maneuver. accommodate balanced common reward machine learning approach cooperative inverse reinforcement learning concept considered adopted framework. https//www.tesla.com/autopilot https//www.google.com/selfdrivingcar/ http//www.volvocars.com/intl/about/our-innovationbrands/intellisafe/autonomous-driving/drive-me. davis l.c. effect adaptive cruise control systems mixed traffic flow near on-ramp. physica statistical mechanics applications vol. marinescu Äurn bouroche cahill on-ramp traffic merging using cooperative intelligent vehicles slot-based approach. international ieee conference intelligent transportation systems alaska chen chan gong decision-making analysis urban expressway ramp merge autonomous vehicle. annual meeting transportation research board washington d.c. https//en.wikipedia.org/wiki/reinforcement_learning fares gomaa freeway ramp-metering control based reinforcement learning. ieee international conference control automation taiwan. yang rakha reinforcement learning ramp metering control weaving sections connected vehicle environment. annual meeting transportation research board sallab abdou perot yogamani end-to-end deep reinforcement learning lane keeping assist. conference neural information processing systems barcelona spain hadfield-menell dragan abbeel russell cooperative inverse reinforcement learning conference neural information processing systems barcelona spain", "year": 2017}