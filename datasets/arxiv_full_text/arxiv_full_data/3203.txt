{"title": "Multi-label ensemble based on variable pairwise constraint projection", "tag": ["cs.LG", "cs.CV", "stat.ML", "I.2.6"], "abstract": "Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches.", "text": "multi-label classification attracted increasing amount attention recent years. many algorithms developed classify multi-label data effective manner. however usually consider pairwise relations indicated sample labels actually play important roles multi-label classification. inspired this naturally extend traditional pairwise constraints multi-label scenario flexible thresholding scheme. moreover improve generalization ability classifier adopt boosting-like strategy construct multi-label ensemble group base classifiers. achieve goals paper presents novel multi-label classification framework named variable pairwise constraint projection multi-label ensemble specifically take advantage variable pairwise constraint projection learn lower-dimensional data representation preserves correlations samples labels. thereafter base classifiers trained data space. boosting-like strategy employ variable pairwise constraints bootstrap steps diversify base classifiers. empirical studies shown superiority proposed method comparison approaches. traditional supervised learning deals data sample associated single label indicates class membership. however broad range real-world applications sample usually related multiple class labels simultaneously data appeared large volume various domains e.g. scene sentiment classification video annotation functional genomics address problem multi-label learning received growing interest last decade compared single-label learning multi-label learning challenging sample contains label e.g. document concerning health might also related topics education entertainment government typical multi-label learning methods include multi-label neural networks multi-label transfer learning multi-label kernel learning fundamentally derived corresponding single-label learning approaches. details refer readers paper study supervised multi-label classification. past years many multi-label classification methods proposed number works whereas often neglect pairwise relations suggested sample labels. relations indicate whether given sample pair similar label sets vital importance multi-label classification. besides pairwise label constraints proved effective single-label learning therefore naturally extend traditional pairwise constraints multi-label scenario flexible thresholding strategy. hand ensemble learning combines multiple base learners jointly accomplish common task shown beneficial enhancing generalization ability single classifier moreover ensemble learning able handle imbalanced data well discourage over-fitting incurred singular problems inspired this adopt boosting-like strategy construct multi-label ensemble improve generalization ability classifier. achieve goals herein propose novel multi-label classification framework named variable pairwise constraint projection multi-label ensemble deal multi-label data. matter fact framework mildly decomposed components i.e. variable pairwise constraint projection boosting-like process. main contributions work highlighted follows. present novel multi-label classification framework construct multi-label ensemble. takes advantage variable pairwise constraint projection obtain well preserved low-dimensional data representation base classifiers learned. boosting-like strategy utilized obtain group diversified base classifiers whose diversities essentially encouraged variable pairwise constraints bootstrap steps. attained base classifiers combined construct multi-label ensemble final classification. remainder paper structured follows. briefly review multi-label classification ensemble learning section introduce multi-label classification framework well rigorous analysis section experimental results broad range real-world multi-label datasets reported section finally provide concluding remarks discuss future work section work focuses multi-label classification proposed framework closely related ensemble learning. section provide brief description multi-label classification ensemble learning. multi-label classification received much attention past years since practically relevant interesting theoretical view traditional supervised classification concentrates single-label data associated label. referred binary classification cardinality discrete label equals scales treated multi-class problem. however many real-world applications large number instances usually correlated multiple labels called multi-label problem since problems type ubiquitous great number multi-label classification methods continuously developed deal multi-label data broad range areas. general existing multi-label classification techniques divided groups problem transformation algorithm adaptation. former aims transform multi-label problem several single-label problems e.g. binary relevance label power-set latter attempts generalize existing single-label classification algorithms multi-label cases e.g. boostexter learns binary classifiers different label regards unique labels class single-label classification task. zhang proposed multi-label k-nearest neighbor method implements single classifier combination bayesian inference. actually applied many practical tasks promising results well simplicity. since multi-label data contains multiple labels important explore interdependencies among labels samples. example cheng come approach based unified framework called iblr-ml combines model-based similarity-based inference multi-label classification particular iblr regards information instances similar query additional feature treats instance-based classification logistic regression. fürnkranz forward calibrated label ranking method multi-label classification considers calibrated scenario extends common learning using pairwise comparison multi-label case. besides makes artificial calibration label discriminate relevant labels irrelevant ones sample. recently chen attempted discover multi-label temporal patterns sequence database treated events multi-label ones many states furthermore presented general sparse representation framework multi-label transfer learning approach learns multi-label encoded sparse linear embedding space relevant dataset maps target data data space. addition explore account correlation information jointly modeled audio tagging task cost-sensitive multi-label learning problem ensemble learning established powerful learning paradigm machine learning community received lots interests last decade paradigm group base learners combined construct ensemble target problem. widely accepted combining multiple classifiers improve generalization ability system assumption base learners accurate diverse possible since identical base classifiers provide additional information desirable diversify several alternative manners e.g. sub-resampling training data feature subsets selection adding stochastic factors learning algorithm roughly speaking mainstream ensemble methods include bagging boosting random forest random subspace rotation forest bagging generates replicated training sets sampling replacement combines classification results boosting complex sense distribution training data changing sequentially constructed classifiers emphasizes misclassified instances random subspace base classifiers constructed random subspaces comparison bagging reduce variation learner suitable decision tree neural networks; boosting able reduce variation bias time appropriate weak learners. addition rotation forest based feature extraction features randomly split subsets principle component analysis used reduce dimension thus axis rotations represent features base classifiers. besides classical methods ensemble methods decision tree ensemble active ensemble dynamic ensemble selection feature sets ensemble artificial neural network ensemble targets single-label problem. compare them multi-label ensemble complicated since takes account correlations among multiple labels. works concerning topic. instance zhang presented ensemble method based twin-svm multi-class multi-label text categorization tsoumakas proposed random k-label sets algorithm construct individual ensemble learners draws random subset size labels member ensemble constructs label power-set classifier using small random subset. furthermore read forward pruned sets method focuses important label correlations proposed classifier chains model label correlations tolerable computational complexity section introduce proposed vpcme framework. first give explicit definition variable pairwise constraints. second inherent components multi-label classification framework described i.e. variable pairwise constraint projection boosting-like strategy. finally summarize algorithm. pairwise constraints often used characterize relations among labels many tasks semi-supervised clustering traditional pairwise constraints exclusively utilized single-label problem none works applied concept multi-label problem best knowledge. work explore multi-label pairwise constraints discover latent relations among multiple labels. achieve goal propose concept called variable pairwise constraints extension traditional ones. details shown follows. recall traditional pairwise constraints single-label problem generally defined follows sample pair shares identical label impose must-link constraint them; otherwise impose cannot-link constraint them. constraints called fixed pairwise constraints. however comes multi-label problem fixed constraints become improper sense strict constraints lead serious unbalances must-link cannot-link constraints. worse still constraints fixed might impossible diversify base classifiers. matter fact shortcomings would bring many inconveniences regard succeeding tasks e.g. constraint projection multi-label classification. therefore natural extend traditional pairwise constraint multi-label scenarios i.e. variable pairwise constraints. since multi-label data related multiple labels give intuitive definition percentage labels sample pair larger equal threshold sample pair imposed must-link constraint; otherwise cannot-link constraint. reality sample pairs must-link constraints form must-link sample pairs cannot-link constraints form cannot-link set. hopefully performance multi-label classification algorithms could improved taking account inherent correlations among multiple labels. framework label correlations reflected terms variable pairwise constraints. concretely must-link collects sample pairs similar labels re-sampling process. meanwhile cannot-link collects sample pairs dissimilar labels. kinds label relationships well preserved variable pairwise constraint projection shown following part. clear promising multi-label ensemble requires base classifiers accurate diverse possible. part focuses obtaining well preserved lower-dimensional data representation using variable pairwise constraint projection. base classifier learned data space capture better discriminating power. begin mathematical definitions described follows. assume given multi-label dataset consists samples classes. sample features stacked vector. feature matrix denoted label matrix sample categorized class otherwise goal precisely estimate label sets test data using proposed multi-label classification framework. mentioned earlier makes sense learn lower-dimensional data representation variable pairwise constraint projection preserve inherent correlations among multiple labels. formulate kinds variable pairwise constraints according definition section follows： denotes must-link denotes cannot-link constant ranging label sample cardinality label besides |r∩r| represents number labels shared data pair principle threshold used tradeoff balance must-link cannot-link large influences data representation. order obtain low-dimensional data representation follow seek group projection vectors best preserve correlated pairwise constraints data space. thus derive low-dimensional data representation transformation wtxi. ideally expect samples data pair close possible samples data pair apart possible. therefore want maximize objective function formulated denote sizes respectively. practically varied necessary. work number instances. term scaling coefficient utilized govern contribution data pairs objective function. since distance samples typically smaller estimated linear algebraic operations objective simplified matrix trace i.e. assume group vectors eigenvectors corresponding largest eigenvalues matrix sc－rsm diagonal matrix eigenvalues diagonal elements. achieves optimal value number eigenvectors number non-negative eigenvalues also dimension data space. thus rewritten obtain data representation base classifier learned. note formulations similar designed single-label problem. contrast framework aims solve multi-label problem takes account correlations among multiple labels. section obtain data representation learn accurate classifier suffers weak generalization ability. overcome drawback adopt boosting-like strategy obtain group diverse base classifiers section. different bagging boosting resample training data directly resample pairwise constraints simultaneously considers features label sets. using boosting-like strategy selected pairwise constraints changing flexible thresholding scheme thus providing diverse information much possible variable pairwise constraint projection. base classifiers learned lower-dimensional data space expected become diverse possible. hand framework also technique similar boosting i.e. emphasizing misclassified samples. here iteration defined bootstrap step provides diversity base classifier. particularly assume sample initially assigned weight endow misclassified samples increased weight iteration weight scaling factor e.g. training error rate previous iteration. note correctly classified samples remain weights process. boosting-like strategy cooperatively take advantage variable pairwise constraints bootstrap steps diversify base classifiers obtain collection base classifiers diverse possible. part summarize proposed multi-label classification framework named variable pairwise constraint projection multi-label ensemble. pseudo-codes sketch shown table following elaborate framework clearly. first randomly select data pairs training data corresponding constrained according flexible thresholding scheme described section total select samples cannot-link samples must-link second obtain data representation reduced dimension using variable pairwise constraint projection i.e. wtx. third base multi-label classifier learned data space. furthermore adopt boosting-like strategy repeat process desired number base classifiers achieved. ultimately collection base classifiers combined together construct robust multi-label ensemble. prediction simply popular majority voting estimate label sets test data since method require prior field knowledge computational cost ssentially framework consists components i.e. variable pairwise constraint projection boosting-like strategy. cooperatively applied construct robust multi-label ensemble e.g. variable pairwise constraints bootstrap steps used diversify base classifiers. notice constrained projection closely related dimensionality reduction applied differently. thing projection based variable pairwise constraint sets rather original data set. another boosting-like strategy together variable pairwise constraint projection jointly contributes obtain multiple base classifiers accurate diverse possible case feature extraction methods. addition projection could preserve pairwise relations multiple labels samples beneficial training desirable base classifiers. section show empirical studies broad range real-world multi-label datasets. first give brief description evaluation metrics data sets. second experimental setup performance comparisons presented. third report results well analysis. true label l={λj total label set. given instance estimated label denoted estimated rank label denoted relevant label takes rank least gets lowest rank following explain evaluation metrics mathematical viewpoint. average precision assesses average fraction labels ranked particular label actually metric virtually reflects average classification accuracy predicted labels instance. denoted criteria hamming loss ranking loss one-error coverage suffice smaller better average precision f-metric recall characterizes larger better. metrics employed jointly investigate performances multi-label classification methods examine multi-label algorithms compile variety multi-label datasets including text categorization image classification bioinformatics summary twelve datasets used labels less examples ones. statistics listed table yahoo datasets available download http//www.kecl.ntt.co.jp/as/members/ueda/yahoo.tar.gzand remaining datasets obtained http//mlkd.csd.auth.gr/multilabel.html label cardinality average number labels assigned sample used quantify number distinct labels. larger cardinality difficult obtain impressive classification performance. besides inherent property direct impacts coverage. specifically datasets identical cardinality distinct label numbers might exhibit property label density percentage averaged labels total labels. value density associates hamming loss ranking loss affect values f-metric recall well. label distinct denotes number different label combinations importance many algorithm adaptation methods operate label subsets. larger distinct takes complex multi-label problem becomes. minority label combinations appear extremely small size i.e. imbalanced problem coverage reduce. yeast genbase datasets biological field. yeast data associated functional classes comprehensive yeast genome database munich information center protein sequences. total number genes amounts gene represented -dimensional feature vector. genbase dataset primary protein families including pdoc pdoc. preprocessing data consists proteins protein might belong classes. multimedia domain scene dataset contains possible scenes beach sunset field fall foliage mountain urban. targets recognizing scenes observed pictures. scene image spatial color moments regarded features picture decomposed blocks using grid. enron dataset collected prepared cognitive assistant learns organizes project containing data users mostly senior management enron organized folders. subset labeled email messages characteristics used experiments message labeled persons. medical data compiled computational medicine centers challenge international natural language processing community. documents involves brief clinical free-text summary patient symptom history prognosis labeled insurance codes. associated labels subset candidate labels. seven yahoo datasets collected real pages linked yahoo.com domain including fourteen top-level categories category divided several second-level subcategories focused second-level categories independent text categorization problems considered containing documents. experiments compared vpcme several state-of-the-art algorithms including iblr-ml cchains rakel mlknn investigate individual components vpcme three typical ensemble methods also examined i.e. adaboost pruned decision tree bagging variable pairwise constraint projection multi-label dimensionality reduction dependence maximization experiments carried machine intel core matlab version. instance-based learning method mlknn used base classifier vpcme excellent predictive performance number nearest neighbor found yield satisfactory performances. eliminate bias incurred different base learners respectively utilized compared algorithms rakel iblr-ml cchains. results show slightly degrades performance results similar report results rakel |labels|/ smaller leads lower computational cost. parameters iblr-ml cchains established described original literatures remaining parameters default values mulan weka java note adaboostpdt handles assigned multiple labels sequentially. settings baggingvpcp vpcme except bootstrap steps. threshold mddm since generalization ability vital importance learning framework investigate performance framework various parameter settings light refinement methods threshold selection different ensemble sizes. tests five-fold cross validations carried estimate labels. detail original dataset randomly divided five parts almost size fold held testing remainder training. process repeated five times part would treated test data exactly once. without loss generality repeated test times recorded averaged results well standard deviations. furthermore paired t-tests significance level done validate efficacy approach. subsection briefly describe four experimental groups performance comparisons designed explore performance vpcme method. note remaining parameters group group group fixed group. detailed descriptions shown follows. group explore performances compared multi-label classification algorithms experiments twelve datasets conducted. vpcme variable pairwise constraint threshold empirically ensemble size tuned evaluation results recorded terms five metrics. group exhibit influences different variable pairwise constraint thresholds framework tested ascending threshold list ranging interval yeast business. empirically provides intuitive selection parameter. group investigate performance vpcme different ensemble sizes examined sequence sizes grid medical entertainment. larger ensemble size time complexity multi-label classification method cost. attain satisfactory performance proper ensemble size desired. group examine individual components vpcme compared three approaches i.e. adaboostpdt baggingvpcp mddm. ensure fairness tested several representative datasets since come distinct domains i.e. yeast scene entertainment. section report results four experimental groups section respectively well analysis. table table report results group fig. depicts results group results group tabulated table comparison results group shown fig. table shows results different multi-label algorithms several data sets. records tabulated terms averaged mean values well standard deviations test runs. examine whether results statistically significant paired t-tests carried significance level. marker suggests approach statistically superior/inferior others. note symbol indicates smaller better indicates larger better. specifically presented method achieves significantly better/worse performance others win/loss counted marker aside record. otherwise counted marker placed. obtained win/tie/loss counts vpcme compared algorithms summarized table table table number interesting points observed follows vpcme approach systematically consistently performs better algorithms since takes advantage variable pairwise constraint projection boosting-like strategy. particularly misclassified samples receive emphasis iteration. multiple diversified base classifiers combined construct robust multi-label ensemble able achieve cchains performs worse others might fact order classifier chains iteration inappropriate datasets. rakel strives learn label power classifier k-subset labels divided subsets randomly selected dataset might degrade performance. iblr-ml outperforms rakel cchains since combines logistic regression unified framework. however biased estimations optimal regression coefficients usually lead imbalance global local inference negative effects performance. performs better text datasets algorithms except vpcme. recall label ranking method artificial calibration label determines separating boundary relevant irrelevant labels. result confidence calibration label desired tends outperformed others. mlknn compared baseline here since selected base classifier vpcme framework. typically binary relevance learner implements individual classifier combination bayesian inference moreover often exhibits simplicity compared ranksvm adaboost.mh text datasets. evidence demonstrates smaller cardinality lower density lead robustness metric since ratio predictive label ground-truth label becomes large extent. left panel fig. reflects comparison performance biology data yeast. right panel fig. displays behavior vpcme text data business. figures seen terms average precision f-metric recall curves vpcme begin ascend initial stage keeps relatively stable intermediate period declines threshold approaches one. obviously vpcme behaves differently compared hamming loss ranking loss one-error. must-link constraints neither large small since extreme values seriously break balance them. herein curves reflect framework tends perform well wide range varied thresholds. examine proposed method show performances vpcme different ensemble sizes table tables table summarizes results medical data table makes records entertainment data. results clearly show proposed method consistently outperforms others ensemble size increases. particular single mlknn used vpcme still good performance data space. however exceeds performance improved slightly. implies ensemble performance achieve stable value peak value. generally choose larger ensemble size proper threshold since training base classifiers would cost much time memory. vividly depicted fig. readily proposed vpcme framework consistently performs better compared approaches terms various evaluation criteria. particularly find adaboostpdt performs worst since consider correlations among multiple labels. baggingvpcp performs better adaboostpdt mddm exploits variable pairwise constraints samples. outperformed vpcme fact baggingvpcp framework treats every sample equally vpcme framework puts emphasis misclassified samples iteration. addition mddm dimensionality reduction method maximizing dependence original features associated class labels take account pairwise constraints. paper introduce novel multi-label classification framework called variable pairwise constraint projection multi-label ensemble construct multi-label ensemble handling multi-label data. framework involves inherent components i.e. variable pairwise constraint projection boosting-like strategy. detail employ variable pairwise constraint projection obtain well preserved lower-dimensional data space base classifiers learned. besides make boosting-like strategy improve generalization ability classifier. boosting-like strategy variable pairwise constraints bootstrap steps exploited diversify group base classifiers. work majority voting adopted decide estimated label test data. conducted extensive interesting experiments range multi-label datasets. results show proposed approach performs better competing methods. nevertheless still remain problems explored future. example sensible develop principle select optimal threshold variable pairwise constraints. high dimensions many real-world data practically important study joint learning multi-label feature selection multi-label ensemble select informative feature subsets. another interesting problem explore speed multi-label ensemble approach computational costs greatly reduced. would like thank anonymous reviewers zhang lijun insightful comments suggestions greatly help improve paper. work supported part national science foundation outstanding young scientists china fundamental research funds central universities", "year": 2014}