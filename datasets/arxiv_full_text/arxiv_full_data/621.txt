{"title": "Supervised Speech Separation Based on Deep Learning: An Overview", "tag": ["cs.CL", "cs.LG", "cs.NE", "cs.SD"], "abstract": "Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multi-talker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.", "text": "abstract‚Äî speech separation task separating target interference. traditionally speech separation studied signal processing problem. recent approach formulates speech separation supervised learning problem discriminative patterns speech speakers background noise learned training data. past decade many supervised separation algorithms forward. particular recent introduction deep supervised speech separation dramatically accelerated progress boosted separation performance. article provides comprehensive overview learning based supervised speech separation last several years. first introduce background speech separation formulation supervised separation. discuss three main components supervised separation learning machines training targets acoustic features. much overview separation algorithms review monaural methods enhancement speaker separation speech dereverberation well multi-microphone techniques. important issue generalization unique supervised learning discussed. overview provides historical perspective advances made. addition discuss number conceptual issues including constitutes target source. index terms‚Äîspeech separation speaker separation speech enhancement supervised speech separation deep learning deep neural networks speech dereverberation time-frequency masking array separation beamforming. goal speech separation separate target speech background interference. speech separation fundamental task signal processing wide range applications including hearing prosthesis mobile telecommunication wang department computer science engineering center cognitive brain sciences ohio state university columbus also holds visiting appointment center intelligent acoustics immersive communications northwestern polytechnical university xi‚Äôan china. robust automatic speech speaker recognition. human auditory system remarkable ability separate sound source others. acoustic environment like cocktail party seem capable effortlessly following speaker presence speakers background noises. speech separation commonly called cocktail party problem term coined cherry famous paper speech important means human communication background interference crucial speech interest target speech usually corrupted additive noises sound sources reverberation surface reflections. although humans perform speech separation apparent ease proven challenging construct automatic system match human auditory system basic task. book cherry made observation machine constructed conclusion unfortunately field remained largely true decades although recent advances reviewed article started crack problem. importance speech separation extensively studied decades. depending number sensors microphones monaural categorize separation methods array-based traditional approaches monaural separation speech enhancement computational auditory scene analysis speech enhancement analyzes general statistics speech noise followed estimation clean speech noisy speech noise estimate simplest widely used enhancement method spectral subtraction power spectrum estimated noise subtracted noisy speech. order estimate background noise speech enhancement techniques stationary i.e. spectral properties change time least stationary speech. casa based perceptual principles auditory scene analysis exploits grouping cues pitch onset. example tandem algorithm separates voiced speech alternating pitch estimation pitch-based grouping array microphones uses different principle achieve speech separation. beamforming spatial filtering boosts signal arrives specific direction configuration hence attenuating interference directions simplest beamformer delay-and-sum technique adds multiple microphone signals target direction phase uses phase differences attenuate signals directions. amount noise attenuation depends spacing size configuration array generally number microphones array length increase. obviously spatial filtering cannot applied target interfering sources co-located near another. moreover utility beamforming much reduced reverberant conditions smear directionality sound sources. recent approach treats speech separation supervised learning problem. original formulation supervised speech separation inspired concept time-frequency masking casa major goal casa ideal binary mask denotes whether target signal dominates unit time-frequency representation mixed signal. listening studies show ideal binary masking dramatically improves speech intelligibility normal-hearing hearing-impaired listeners noisy conditions computational goal speech separation becomes binary classification elementary form supervised learning. case used desired signal target function training. testing learning machine aims estimate served first training target supervised speech separation since formulation speech separation classification data-driven approach extensively studied speech processing community. last decade supervised speech separation substantially advanced state-of-the-art performance leveraging large training data supervised separation especially benefited rapid rise deep learning topic overview. supervised speech separation algorithms broadly divided following components learning machines training targets acoustic features. paper first review three components. describe representative algorithms monaural array-based algorithms covered separate sections. generalization issue unique supervised speech separation issue treated overview. clarify related terms used overview avoid potential confusion. refer speech separation segregation general task separating target speech include nonspeech noise interfering speech both well room reverberation. furthermore equate speech separation cocktail party problem goes beyond separation speech utterances originally experimented cherry speech enhancement mean separation speech nonspeech noise. limited separation multiple voices term speaker separation. overview organized follows. first review three main aspects supervised speech separation i.e. learning machines training targets features sections respectively. section devoted monaural separation algorithms section array-based past decade dnns significantly elevated performance many supervised learning tasks image classification handwriting recognition automatic speech recognition language modeling machine translation dnns also advanced performance supervised speech separation large margin. section briefly introduces types dnns supervised feedforward multilayer perceptrons convolutional neural networks recurrent neural networks generative adversarial networks popular model neural networks feedforward connections input layer output layer layer-by-layer consecutive layers fully connected. extension rosenblatt‚Äôs perceptron introducing hidden layers input layer output layer. trained classical backpropagation algorithm network weights adjusted minimize prediction error gradient descent. prediction error measured cost function predicted output desired output latter provided user part supervision. example used function approximation regression common cost function mean square error representational power increases number layers increases even though theory hidden layers approximate function backpropagation algorithm applicable depth. however deep neural network many hidden layers difficult train random initialization connection weights biases so-called vanishing gradient problem refers observation that lower layers gradients calculated backpropagated error signals upper layers become progressively smaller vanishing. result vanishing gradients connection weights lower layers modified much therefore lower layers learn little training. explains mlps single hidden layer widely used neural network prior advent dnn. breakthrough training made hinton idea perform layerwise unsupervised pretraining unlabeled data properly initialize supervised learning performed labeled data. specifically hinton proposed restrictive boltzmann machines pretrain layer layer pretraining found improve subsequent supervised learning. later remedy rectified linear unit replace traditional sigmoid activation function converts weighted inputs model neuron neuron‚Äôs output. recent practice shows moderately deep relus class feedforward networks known convolutional neural networks demonstrated well suited pattern recognition particularly visual domain. cnns incorporate well-documented invariances pattern recognition translation invariance. typical architecture cascade pairs convolutional layer subsampling layer. convolutional layer consists multiple feature maps learns extract local feature regardless position previous layer weight sharing neurons within module constrained connection weights despite different receptive fields. receptive field neuron context denotes local area previous layer connected neuron whose operation weighted akin convolution. convolutional layer followed subsampling layer performs local averaging maximization receptive fields neurons convolutional layer. subsampling serves reduce resolution sensitivity local variations. weight sharing also benefit cutting number trainable parameters. incorporates domain knowledge pattern recognition network structure better trained backpropagation algorithm despite fact deep network. rnns allow recurrent connections typically hidden units. unlike feedforward networks process input sample independently rnns treat input samples sequence model changes time. speech signal exhibits strong temporal structure signal within current frame influenced signals previous frames. therefore rnns natural choice learning temporal dynamics speech. note recurrent connections introduces time dimension flexible infinitely extensible characteristic shared feedforward networks matter deep viewed infinite depth recurrent connections typically trained backpropagation time however training susceptible vanishing exploding gradient problem alleviate problem long short-term memory introduces memory cells gates facilitate information flow time specifically memory cell three gates input gate forget gate output gate. forget gate controls much previous information retained input gate controls much current information added memory cell. gating functions lstm allows relevant contextual information maintained memory cells improve training. generative adversarial networks recently introduced simultaneously trained models generative model discriminative model generator learns model labeled data e.g. mapping noisy straightforwardly correlation. speech samples discriminator usually binary classifier learns discriminate generated samples target samples training data. framework analogous twoplayer adversarial game minimax proven strategy. training aims learn accurate mapping generated data well imitate real data fool hand learns better tell difference real data synthetic data generated competition game adversarial learning drives models improve accuracy generated samples indistinguishable real ones. idea gans discriminator shape loss function generator. overview refers neural network least hidden layers contrast popular learning machines hidden layer commonly used mlps support vector machines kernels gaussian mixture models dnns deeper practice hidden layers actually used depth required neural network considered matter qualitative rather quantitative distinction. also term denote neural network deep structure whether feedforward recurrent. supervised speech separation defining proper training target important learning generalization. mainly groups training targets i.e. masking-based targets mapping-based targets. masking-based targets describe time-frequency relationships clean speech background targets correspond spectral representations clean speech. section survey number training targets proposed field. supervised speech separation ideal binary mask inspired auditory masking phenomenon audition exclusive allocation principle auditory scene analysis defined twodimensional representation noisy signal cochleagram spectrogram assigns value unit within unit exceeds local criterion threshold otherwise. fig. shows example defined -channel cochleagram. mentioned sect. masking dramatically increases speech intelligibility noise normal-hearing hearing-impaired listeners. labels every unit either target-dominant interference-dominant. result estimation naturally treated supervised classification problem hence bridging speech separation supervised learning. like target binary mask categorizes units binary label. different derives label comparing target speech energy unit fixed interference speech-shaped noise stationary signal corresponding average speech signals. example shown fig. target binary masking also leads dramatic improvement speech intelligibility noise used training target clean speech noisy speech respectively. unlike upper-bounded obtain separated speech apply estimate spectral magnitudes noisy speech resynthesize separated speech phases noisy speech viewed form defined stft domain. fig. illustrates smm. figure comparison training targets. terms stoi. terms pesq. clean speech mixed factory noise snr. results different training targets well speech enhancement algorithm method highlighted mixtures. note results data figure obtained matlab toolbox http//web.cse.ohio-state.edu/pnl/dnn_toolbox/. unlike defined spectrogram target defined cochleagram based gammatone filterbank. specifically target defined power cochleagram response clean speech. estimate gftps easily converted separated speech waveform cochleagram inversion fig. illustrates target. interpreted target combines ratio masking spectral mapping seeking maximize related earlier target aims maximal context estimation target better separation performance achieved two-stage training first stage learning machine trained target. second stage learning machine fine-tuned minimizing loss function number training targets compared using fixed feedforward three hidden layers input features separated speech using various training targets evaluated terms short-time objective intelligibility score perceptual evaluation speech quality score stoi pesq standard metrics predicted speech intelligibility speech quality respectively. value range stoi typically interpreted percent correct pesq ranges addition representative speech enhancement algorithm supervised nonnegative matrix factorization algorithm evaluated benchmarks. evaluation results given figure number conclusions drawn components clean speech respectively. imaginary unit denoted ‚Äòi‚Äô. thus cirm real component imaginary component separately estimated real domain. complex-domain calculations mask values become unbounded. form compression used bound mask values tangent hyperbolic sigmoidal function cartesian coordinates structure exists real imaginary components cirm whereas polar coordinates structure exists magnitude spectrogram phase spectrogram. without clear structure direct phase estimation would intractable supervised learning although mention recent paper uses complex-domain estimate complex stft coefficients hand estimate cirm provides phase estimate property possessed estimation. case supervised learning aims estimate magnitude spectrogram clean speech noisy speech. power spectrum forms spectra spectrum used instead magnitude spectrum operation usually applied compress dynamic range facilitate training. estimated speech magnitude combined noisy phase produce separated speech waveform. fig. shows example tms. study. first terms objective intelligibility masking-based targets group outperform mappingbased targets although recent study indicates masking advantageous higher input snrs lower snrs mapping advantageous terms speech quality ratio masking performs better binary masking. particularly illuminating contrast except estimation attributed fact target magnitude spectrum insensitive interference signal whereas many-to-one mapping makes estimation potentially difficult estimation. addition estimation unbounded spectral magnitudes tends magnify estimation errors overall emerge preferred targets. addition based ratio masking performs substantially better supervised unsupervised speech enhancement. list training targets meant exhaustive targets used literature. perhaps straightforward target waveform signal clean speech. indeed used early study trains frame noisy speech waveform frame clean speech waveform called temporal mapping although simple direct mapping perform well even used place shallow network target defined time domain target estimation includes modules ratio masking inverse fourier transform noisy phase. target closely related psm. recent study evaluates oracle results number ideal masks additionally introduces socalled ideal gain mask defined terms priori posteriori commonly used traditional speech enhancement conclusion also nuanced speaker separation first pointed hakan erdogan personal communication. play complementary roles supervised learning. features discriminative place less demand learning machine order perform task successfully. hand powerful learning machine places less demand features. extreme linear classifier like rosenblatt‚Äôs perceptron needed features make classification task linearly separable. extreme input original form without feature extraction suffices classifier capable learning appropriate features. majority tasks feature extraction learning important. early studies supervised speech separation features interaural time differences interaural level differences binaural separation pitch-based features amplitude modulation spectrogram monaural separation. subsequent study explores monaural features including mel-frequency cepstral coefficient gammatone frequency cepstral coefficient perceptual linear prediction relative spectral transform feature selection using group lasso study recommends complementary feature comprising rasta-plp mfcc since used many studies. conducted study examine extensive list acoustic features supervised speech separation snrs features previously used robust automatic speech recognition classification-based speech separation. feature list includes mel-domain linearprediction zero-crossing autocorrelation medium-time-filtering modulation pitchbased features. mel-domain features mfcc deltaspectral cepstral coefficient similar mfcc except delta operation applied melspectrum. linear prediction features rastaplp. three gammatone-domain features gammatone table stoi improvements list features averaged test noises sim. rec. indicate simulated recorded room impulse responses. boldface indicates best scores condition. cochannel cases performance shown separately female interferer male interferer male target talker. feature gfcc gammatone frequency modulation coefficient computed passing input signal gammatone filterbank applying decimation operation subband signals. zero-crossing feature called zero-crossings peak-amplitudes computes zero-crossing intervals corresponding peak amplitudes subband signals derived using gammatone relative filterbank. autocorrelation autocorrelation sequence mfcc phase autocorrelation mfcc apply mfcc procedure autocorrelation domain. medium-time filtering features power normalized cepstral coefficients suppression slowly-varying components falling edge power envelope modulation domain features gabor filterbank features. pitch-based features calculate level features based pitch tracking periodicity discriminate speech-dominant units noise-dominant ones. addition existing features proposed feature called multi-resolution cochleagram computes four cochleagrams different spectrotemporal resolutions provide local information broader context. features post-processed auto-regressive moving average filter evaluated fixed based mask estimator. estimated masks evaluated terms classification accuracy hit‚àífa rate denotes percent speechdominant units correctly classified refers percent noise-dominant units incorrectly classified. hit‚àífa rate found well correlated speech intelligibility hit‚àífa results shown table shown table gammatonedomain features consistently outperform features accuracy hit‚àífa rate mrcg performing best. cepstral compaction discrete cosine transform effective revealed comparing gfcc features. neither modulation extraction shown comparing gfcc gmfc latter calculated former. worth noting poor performance pitch features largely inaccurate estimation snrs ground-truth pitch shown quite discriminative. recently delfarah wang performed another feature study considers room reverberation speech denoising speaker separation. study uses fixed trained estimate evaluation results given terms stoi improvements unprocessed noisy reverberant speech. features added study include spectral magnitude mel-spectrum feature commonly used supervised separation also included waveform signal without feature extraction. impulse responses recorded rirs used reverberation seconds. denoising evaluation done separately matched noises first half nonstationary noise used training second half testing unmatched noises completely noises used testing. cochannel separation target talker male interfering talker either female male. table shows stoi gains individual features evaluated. anechoic matched noise case stoi results largely consistent table feature results also broadly consistent using simulated recorded rirs. however best performing features different matched noise unmatched noise speaker separation cases. besides mrcg pncc gfcc produce best results unmatched noise cochannel condition respectively. feature combination study concludes effective feature consists pncc log-mel speech enhancement pncc gfcc log-mel speaker separation. large performance differences caused features table table demonstrate importance features supervised speech separation. inclusion waveform signal table suggests that without feature extraction separation results poor. noted that feedforward used couple well waveform signals cnns rnns better suited so-called end-to-end separation. come issue later. section discuss monaural algorithms speech enhancement dereverberation plus denoising speaker separation. explain representative algorithms discuss generalization supervised speech separation. speech enhancement knowledge deep learning first introduced speech separation wang wang conference papers later extended journal version used subband classification estimate ibm. conference versions feedforward dnns pretraining used binary classifiers well feature encoders structured perceptrons conditional random fields reported strong separation results cases usage better results used feature learning journal version input signal passed -channel gammatone filterbank derive subband signals acoustic features extracted within unit. features form input subband dnns learn discriminative features. speech separation illustrated figure training input features learned features last hidden layer concatenated linear svms estimate subband efficiently. algorithm extended two-stage first stage trained estimate subband usual second stage explicitly incorporates context following way. first-stage trained unitlevel output binarization interpreted posterior probability speech dominates unit. hence first-stage output considered posterior mask form ratio mask. second stage unit takes input local window posterior mask centered unit. two-stage illustrated fig. second-stage structure reminiscent convolutional layer without weight sharing. leveraging contextual information shown significantly improve classification accuracy. subject tests demonstrate produced large intelligibility improvements listeners listeners benefiting first monaural algorithm provide substantial speech intelligibility improvements listeners background noise much subjects separation outperformed subjects without separation. basic autoencoder unsupervised learning machine typically symmetric architecture hidden layer tied weights learns input signal itself. multiple trained stacked subject traditional supervised fine-tuning e.g. backpropagation algorithm. words autoencoding alternative pretraining. algorithm learns mel-frequency power spectrum noisy speech clean speech regarded first mapping based method. subsequently perhaps unaware published study using pretraining power spectrum noisy speech clean speech shown fig. unlike used standard feedforward pretraining. training estimates clean speech‚Äôs spectrum noisy input. experimental results show trained yields pesq gains noisy speech untrained noises higher obtained representative traditional enhancement method. many subsequent studies since published along lines masking spectral mapping. rnns lstm used speech enhancement application robust training aims signal approximation rnns also used estimate psm. deep stacking network proposed estimation mask estimate used pitch estimation. accuracy mask estimation pitch estimation improves modules iterate several cycles. used simultaneously estimate real imaginary components cirm yielding better speech quality estimation speech enhancement phoneme level recently studied takes account perceptual masking piecewise gain function. skip connections non-consecutive layers authors also published paper interspeech trained unsupervised fashion melspectrum clean speech itself. trained used recall clean signal noisy input robust asr. aside masking mapping based approaches recent interest using deep learning perform end-to-end separation i.e. temporal mapping without resorting representation. potential advantage approach circumvent need phase noisy speech reconstructing enhanced speech drag speech quality particularly input low. recently developed fully convolutional network speech enhancement. observe full connections make difficult high frequency components waveform signal removal enhancement results improve. convolution operator filter feature extractor cnns appear natural choice temporal mapping. another recent study employs perform temporal mapping so-called speech enhancement generator fully convolutional network performing enhancement denoising. discriminator follows convolutional structure transmits information generated waveform signals versus clean signals back viewed providing trainable loss function segan evaluated untrained noisy conditions results inconclusive worse masking mapping methods. untrained conditions crucial issue. case speech enhancement data-driven algorithms bear burden proof comes generalization issue arise traditional speech enhancement casa algorithms make minimal supervised training. supervised enhancement three aspects generalization noise speaker snr. regarding generalization simply include levels training practical experience shows supervised enhancement sensitive precise snrs used training. part reason that even though mixture snrs included training local snrs frame level unit level usually vary wide range providing necessary variety learning machine generalize well. effort address mismatch training test conditions smaragdis proposed two-stage first stage standard perform spectral mapping second stage autoencoder performs unsupervised adaptation test stage. trained magnitude spectrum clean utterance itself much like hence training need labeled data. stacked serves purity checker shown fig. rationale well enhanced speech tends produce small difference input output whereas poorly enhanced speech produce large error. given test mixture already-trained fine-tuned error signal coming introduction module provides unsupervised adaptation test conditions quite different training conditions shown improve performance speech enhancement. noise generalization fundamentally challenging kinds stationary nonstationary noises interfere speech signal. available training noises limited technique expand training noises noise perturbation particularly frequency perturbation specifically spectrogram original noise sample perturbed generate noise samples. make dnnbased mapping algorithm robust noises incorporate noise aware training i.e. input feature vector includes explicit noise estimate. noise estimated binary masking noise aware training generalizes better untrained noises. noise generalization systematically addressed study trained estimate frame level. addition simultaneously estimated several consecutive frames different estimates frame averaged produce smoother accurate mask five hidden layers relus each. input features frame cochleagram response energies training includes mixtures created ieee sentences noises sound effect library fixed total duration noises hours total duration training mixtures hours. evaluate impact number training noises noise generalization also trained noises done test sets created using ieee sentences nonstationary noises various snrs. neither test sentences test noises used training. separation results measured stoi shown table large stoi improvements obtained k-noise model. k-noise model substantially outperforms -noise model average performance matches noise-dependent models trained first half training noises tested second half. subject tests show noise-independent model resulting large-scale training significantly improves real environment speech usually corrupted reverberation surface reflections. room reverberation corresponds convolution direct signal distorts speech signals along time frequency. reverberation creates major headache speech processing tasks particularly combined background noise. result dereverberation actively investigated long time proposed first based approach speech dereverberation. approach uses spectral mapping cochleagram. words trained window reverberant speech frames frame anechoic speech illustrated fig. trained reconstruct cochleagram anechoic speech surprisingly high quality. later work apply spectral mapping spectrogram extend approach perform dereverberation denoising. sophisticated system proposed recently observe dereverberation performance improves frame length shift chosen differently depending reverberation time based observation system includes control parameter dereverberation stage estimated used choose appropriate frame length shift feature extraction. so-called reverberation-time-aware model illustrated fig. dereverberation performance improve estimation anechoic speech reverberant noisy speech xiao proposed trained predict static delta acceleration features time. static features magnitudes clean speech delta acceleration features derived static features. argued predicts static features well also predict delta acceleration features well. incorporation dynamic features structure helps improve estimation static features dereverberation. zhao observe spectral mapping effective dereverberation masking whereas masking works better denoising. consequently construct two-stage first stage performs ratio masking denoising second stage spectral mapping dereverberation. furthermore alleviate adverse effects using phase reverberantnoisy speech resynthesizing waveform signal enhanced speech study extends time-domain signal reconstruction technique training target defined time-domain clean phase used training unlike noisy phase used. stages individually trained first jointly trained. results show two-stage model significantly outperforms single-stage models either mapping masking. speech intelligibility listeners unseen noises. study strongly suggests large-scale training wide variety noises promising address noise generalization. speaker generalization separation system trained specific speaker would work well different speaker. straightforward attempt speaker generalization would train large number speakers. however experimental results show feedforward appears incapable modeling large number talkers. typically takes window acoustic features mask estimation without using long-term context. unable track target speaker feedforward network tendency mistake noise fragments target speech. rnns naturally model temporal dependencies thus expected suitable speaker generalization feedforward dnn. recently employed lstm address speaker generalization noise-independent models model shown figure trained mixtures created noises mixed speakers. tested trained speakers shown fig. performance degrades training speakers added training whereas lstm benefits additional training speakers. untrained test speakers shown fig. lstm substantially outperforms terms stoi. lstm appears able track target speaker time exposed many speakers training. large-scale training many speakers numerous noises rnns lstm represent effective approach speakernoiseindependent speech enhancement. speaker separation underlying speakers allowed change training testing speakerdependent situation. interfering speakers allowed change target speaker fixed called targetdependent speaker separation. least constrained case none speakers required called speakerindependent. perspective huang al.‚Äôs approach speaker dependent studies deal speaker target dependent separation. relaxing constraint interfering speakers simply train cochannel mixtures target speaker many interferers. zhang wang proposed deep ensemble network target-dependent address speaker-dependent well separation employ multi-context networks integrate temporal information different resolutions. ensemble constructed stacking multiple modules performing multi-context masking mapping. several training targets examined study. speakerdependent separation signal approximation shown effective; target-dependent separation combination ratio masking signal approximation effective. furthermore performance target-dependent separation close speaker-dependent separation. capable speech enhancement successfully applied speaker separation similar framework illustrated figure case two-speaker cochannel separation. according literature search huang first introduce task. study addresses two-speaker separation using feedforward rnn. authors argue summation spectra signal approximation training target introduced section iii.i. binary ratio masking found effective. addition discriminative training applied maximize difference speaker estimated version other. training following cost minimized months later appeared independently proposed speaker separation similar study trained estimate power spectrum target speaker cochannel mixture. different paper trained cochannel signal spectrum target speaker well spectrum interfering speaker illustrated fig. extended version). notable extension compared papers also address situation target speaker training testing interfering speakers different training testing. figure mean intelligibility scores standard errors subjects listening target sentences mixed interfering sentences separated target sentences percent correct results given four different target-tointerferer ratios. indicator matrix built ibm. unit belongs speaker estimated affinity matrix derived embedding matrix units. denotes squared frobenius inference mixture segmented embedding matrix computed segment. then embedding matrices segments concatenated. finally k-means algorithm applied classify units segments speaker classes. segment-level clustering accurate utterance-level clustering clustering results individual segments problem sequential organization addressed. deep clustering shown separation complement corresponding target talker interfering talker. compared earlier dnn-based cochannel separation studies algorithm uses diverse features predicts multiple frames resulting better separation. intelligibility results shown figure group dnn-based separation percentage points target-to-interferer ratio respectively. statistically significant improvements smaller extent. remarkable large intelligibility improvements obtained listeners allow perform equivalently listeners common tirs treated unsupervised clustering units clustered distinct classes dominated individual speakers clustering flexible framework terms number speakers separate benefit much discriminative information fully utilized supervised training. hershey first address speaker-independent multi-talker separation framework approach called deep clustering combines based feature learning spectral clustering. ground truth partition clustering-based methods naturally lead speakerindependent models based masking/mapping methods output specific speaker lead speaker-dependent models. example mapping based methods minimize following cost function magnitudes speaker respectively denotes time frame. untie outputs speakers train speaker-independent model using masking mapping technique recently proposed permutationinvariant training shown fig. two-speaker separation trained output masks applied noisy speech produce source estimate. training cost function dynamically calculated. assign output reference speaker possible assignments associated mse. assignment lower chosen trained minimize corresponding mse. training inference takes segment multiple frames features estimates sources segment. since outputs tied speaker speaker switch output another across consecutive segments. therefore estimated segment-level sources need sequentially organized unless segments long utterances. although much simpler speaker separation results shown match obtained deep clustering insight body work overviewed speaker separation subsection model trained many pairs different speakers able separate pair speakers never included training case speaker independent separation frame level. speaker-independent separation issue group well-separated speech signals individual frames across time. precisely issue sequential organization much investigated casa permutation-invariant training considered imposing sequential grouping constraints training. hand typical casa methods utilize pitch contours vocal tract characteristics rhythm prosody even common spatial direction multiple sensors available usually involve supervised learning. seems integrating traditional casa techniques deep learning fertile ground future research. array microphones provides multiple monaural recordings contain information indicative spatial origin sound source. sound sources spatially separated sensor array inputs localize sound sources extract source target location direction. traditional approaches source separation based spatial information include beamforming mentioned first study supervised speech segregation conducted roman binaural domain. study performs supervised classification estimate based binaural features extracted individual unit pairs left-ear right-ear cochleagram. classification based maximum posteriori likelihood given density estimation technique. another classic two-sensor separation technique duet published yilmaz rickard time. duet based unsupervised clustering spatial features used phase amplitude differences microphones. contrast classification clustering studies persistent theme anticipates similar contrasts later studies e.g. binary masking clustering beamforming deep clustering versus mask estimation talker-independent speaker separation spatial information afforded array features deep learning straightforward extension earlier monaural separation; simply substitutes spatial features monaural features. indeed leveraging spatial information provides natural framework integrating monaural spatial features source separation point worth emphasizing traditional research tends pursue array separation without considering monaural grouping. worth noting human auditory scene analysis integrates monaural binaural analysis seamless fashion taking advantage whatever discriminant information existing particular environment first study employ binaural separation published jiang study signals ears passed corresponding auditory filterbanks. features extracted unit pairs sent subband estimation frequency channel. addition monaural feature extracted left-ear input. number conclusions drawn study. perhaps important observation trained generalizes well untrained spatial configurations sound sources. spatial configuration refers specific placement sound sources sensors acoustic environment. supervised learning infinite configurations training cannot enumerate various configurations. based binaural separation rirs reverberation times. also observed incorporation monaural feature improves separation performance especially target interfering sources colocated close other. araki subsequently employed spectral mapping includes spatial features interaural phase difference enhanced features initial mask derived location information addition monaural input. evaluation related metrics shows best enhancement performance obtained combination monaural enhanced features. proposed spectral mapping approach utilizing binaural monaural inputs. binaural features study uses subband ilds found effective concatenated left-ear‚Äôs frame-level power spectra form input trained spectrum clean speech. quantitative comparison shows system produces better pesq scores separated speech similar stoi numbers. complex binaural separation algorithm proposed spatial features used include so-called mixing vector form combined stft values unit pair. used first trained unsupervisedly autoencoders subsequently stacked subject supervised finetuning. extracted spatial features first mapped highlevel features indicating spatial directions unsupervised training. separation classifier trained high-level spatial features discretized range source directions. algorithm operates subbands covering block consecutive frequency channels. recently zhang wang developed estimation sophisticated spatial spectral features. algorithm illustrated fig. left-ear right-ear inputs different modules spectral spatial analysis. instead monaural analysis single spectral analysis conducted output fixed beamformer removes background inference extracting complementary monaural features spatial analysis form cross-correlation function extracted. spectral spatial features concatenated form input estimation frame level. algorithm shown produce substantially better separation results conventional beamformers including mvdr interesting observation analysis much benefit using beamformer prior spectral feature extraction obtained simply concatenating monaural features ears. although methods binaural involving sensors extension sensors array sensors usually straightforward. take system fig. instance. microphones spectral feature extraction traditional beamformers already formulated arbitrary number microphones. spatial feature extraction feature space needs expanded sensors available either designating microphone reference deriving binaural features considering matrix sensor pairs correlation covariance analysis. output mask spectral envelope corresponding target speech viewed monaural. since traditional beamforming array also produces monaural output corresponding target source masking based spatial features considered beamforming accurately nonlinear beamforming opposed traditional beamforming linear. beamforming name would suggest tunes signals zone arrival angles centered given angle tuning signals outside zone. applicable beamformer needs know target direction steer beamformer. steering vector typically supplied estimating direction-of-arrival target source broadly sound localization. reverberant multi-source environments localizing target sound trivial. well recognized casa localization separation closely related functions chapter human audition evidence suggests sound localization largely depends source separation fueled chime- challenge robust independent studies made first based monaural conjunction conventional beamforming published icassp chime- challenge provides noisy speech data single speaker recorded microphones mounted tablet studies monaural speech separation provides basis computing steering vector cleverly bypassing tasks would required estimation localizing multiple sound sources selecting target source. explain idea first describe mvdr representative beamformer. mvdr aims minimize noise energy nontarget directions imposing linear constraints maintain energy target direction captured signals array stft domain written denote stft spatial vectors noisy speech signal noise frame frequency respectively denotes stft speech source. term ùêúùëìùë†ùë°ùëì denotes received speech signal array steering vector array. vector minimizes average output power direction. omitting brevity optimization ùê∞bct=argminùê∞ denotes conjugate transpose spatial eigenvalue) spatial covariance matrix speech. speech noise uncorrelated therefore noise estimate crucial beamforming performance like traditional speech enhancement. bidirectional lstm used estimation. common neural network trained monaurally data sensors. trained network used produce binary mask microphone recording multiple masks combined mask median operation. single mask used estimate speech noise covariance matrix beamformer coefficients obtained. results show mvdr work well beamformer. spatial clustering based approach proposed compute ratio mask. approach uses complex-domain describe distribution units dominated noise another cgmm describe units speech noise. parameter estimation cgmms used calculating covariance matrices noisy speech noise mvdr beamformer speech separation. algorithms perform well higuchi al.‚Äôs method used best performing system chime- challenge similar approach i.e. dnn-based estimation combined beamformer also behind winning system recent chime- challenge method different studies given nugraha perform array source separation using monaural separation complex multivariate gaussian spatial information. study used model source spectra spectral mapping. power spectral densities spatial covariance matrices speech noise estimated updated iteratively. estimated speech signals multiple microphones averaged produce single speech estimate evaluation. number design choices examined study algorithm yields better separation results based monaural separation array version nmf-based separation. success higuchi heymann chime- challenge using estimated masks beamforming motivated many recent studies exploring different ways integrating masking beamforming. erdogan trained monaural speech enhancement ratio mask computed order provide coefficients mvdr beamformer. illustrated fig. ratio mask first estimated microphone. multiple masks array combined mask maximum operator found produce better results using multiple masks combination. noted results chime- data compelling. instead fixed beamformers like mvdr beamforming coefficients dynamically predicted dnn. employed deep network predict spatial filters array inputs noisy speech adaptive beamforming. waveform signals sent shared whose output sent separate rnns predict beamforming filters microphones. complementary monaural features combined multiple ratio masks array single maximum operator. ratio mask used calculating noise spatial covariance matrix time mvdr beamformer follows frame frequency element noise covariance matrix calculated frame integrating window neighboring frames. find adaptive estimating noise covariance matrix perform much better estimation entire utterance signal segment. enhanced speech signal beamformer refine estimate mask estimation beamforming iterate several times produce final output. chime- real evaluation data represents relative improvement previous best independently xiao also proposed iterate ratio masking beamforming. estimating speech mask noise mask. mask refinement based loss order directly benefit performance. showed approach leads considerable reduction conventional mvdr although recognition accuracy high related studies include pfeifenberger cosine distance principal components consecutive frames noisy speech feature mask estimation. meng rnns adaptive estimation beamformer coefficients. results chime- data better baseline scores best scores. nakatani integrate mask estimation cgmm clustering based estimation improve quality mask estimates. results chime- data improve obtained cgmm generated masks. paper provided comprehensive overview based supervised speech separation. summarized components supervised separation i.e. learning machines training targets acoustic features explained representative algorithms reviewed large number related studies. formulation separation problem supervised learning based separation short years greatly elevated state-of-the-art wide range speech separation tasks including monaural speech enhancement speech dereverberation speaker separation well array speech separation. rapid advance likely continue tighter integration domain knowledge data-driven framework progress deep learning itself. learn appropriate features task rather design features. role feature extraction deep learning? believe answer yes. so-called nofree-lunch theorem dictates learning algorithm included achieves superior performance tasks. aside theoretical arguments feature extraction imparting knowledge problem domain stands reason useful incorporate domain knowledge recent example). instance success visual pattern recognition partly shared weights pooling layers architecture helps build representation invariant small variations feature positions possible learn useful features problem domain computationally efficient particularly certain features known discriminative domain research. take pitch example. much research auditory scene analysis shows pitch primary auditory organization research casa demonstrates pitch alone long separating voiced speech perhaps trained discover harmonicity prominent feature hint recent study extracting pitch input features seems like straightforward incorporating pitch speech separation. discussion meant discount importance learning machines overview made abundantly clear argue relevance feature extraction despite power deep learning. mentioned sect. convolutional layers amount feature extraction. although weights trained particular architecture reflects design choices user. vast majority supervised speech separation studies conducted domain reflected various training targets reviewed sect. iii. alternatively speech separation conducted time domain without recourse frequency representation. pointed sect. temporal mapping magnitude phase potentially cleaned once. end-to-end separation represents emergent trend along cnns gans. comments order. first temporal mapping welcome addition list supervised separation approaches provides unique perspective phase enhancement second signal transformed back forth time domain representation domain representation. such would incorrect treat representation feature. third human auditory system frequency dimension beginning auditory pathway i.e. cochlea. interesting note licklider‚Äôs classic duplex theory pitch perception postulating processes pitch analysis spatial process corresponding frequency dimension cochlea temporal process corresponding temporal computational models pitch estimation fall three categories spectral temporal spectrotemporal approaches sense cochleagram individual likely ensue various deep learning models suitably constructed training sets. considerable often unnoticed benefit treating signal processing learning signal processing ride progress machine learning rapidly advancing field. finally remark human ability solve cocktail party problem appears much extensive exposure various noisy environments research indicates children poorer ability recognize speech noise adults musicians better perceiving noisy speech non-musicians presumably musicians‚Äô long exposure polyphonic signals. relative monolingual speakers bilinguals deficit comes speech perception noise although groups similarly proficient quiet effects support notion extensive training part reason remarkable robustness normal auditory system acoustic interference. acknowledgments preparation overview supported part afosr grant nidcd grant grant thank masood delfarah help manuscript preparation yuxuan wang useful comments earlier version. m.c. anzalone calandruccio k.a. doherty l.h. carney \"determination potential benefit timefrequency gain manipulation\" hear. vol. araki \"exploring multi-channel features denoising-autoencoder-based proceedings icassp avendano hermansky dereverberation speech based temporal envelope filtering\" proceedings icslp f.r. bach m.i. jordan \"learning spectral clustering application speech separation\" mach. learn. res. vol. barker marxer vincent watanabe \"the third chime speech separation recognition challenge dataset task baselines\" proceedings ieee asru a.j. bell t.j. sejnowski information-maximization approach blind separation blind deconvolution\" neural comp. vol. benesty chen huang microphone array signal processing. berlin springer bengio lecun \"scaling learning algorithms towards large-scale kernel machines bottou chapelle decoste weston cambridge m.a. press blauert spatial hearing psychophysics human sound localization. cambridge press acoustic environment treated target sound particular time? definition ideal masks presumes target source known often case speech separation applications. speech enhancement speech signal considered target nonspeech signals considered interference. situation becomes tricky multi-speaker separation. general issue auditory attention intention. complicated issue attended shifts moment next even input scene speech signal. however practical solutions. example directional hearing aids around issue assuming target lies look direction i.e. benefiting vision sources separated reasonable alternatives target definition e.g. loudest source previously attended familiar full account however would require sophistical model auditory attention look like? casa defines solution cocktail party problem system achieves human separation performance listening conditions p.). actually compare separation performance machine human listener? perhaps straightforward would compare scores human speech intelligibility scores various listening conditions. tall order performance still falls short realistic conditions despite tremendous recent advances thanks deep learning. drawback evaluation dependency peculiarities. suggest different concrete measure solution cocktail party separation system elevates speech intelligibility hearing-impaired listeners level normal-hearing listeners listening situations. broad defined casa definition benefit tightly linked primary driver speech separation speech understanding handicap millions listeners impaired hearing definition based speech enhancement described criterion limited conditions clearly conditions. versatility hallmark human intelligence primary challenge facing supervised speech separation research today. efore closing point supervised learning signal processing goes beyond speech separation automatic speech speaker recognition. related topics include multipitch tracking voice activity detection even task basic signal processing estimation matter task formulated data-driven problem advances d.s. brungart p.s. chang b.d. simpson d.l. wang \"isolating energetic component speech-on-speech masking ideal time-frequency segregation\" acoust. soc. vol. chen j.a. bilmes \"mva processing speech features\" ieee trans. audio speech lang. proc. vol. chen d.l. wang \"long short-term memory speaker generalization supervised speech separation\" proceedings interspeech chen d.l. wang \"dnn-based mask estimation supervised speech separation\" audio source separation makino berlin springer press chen d.l. wang \"long short-term memory speaker generalization supervised speech separation\" acoust. soc. vol. chen wang d.l. wang feature study classification-based speech separation signal-to-noise ratios\" ieee/acm trans. audio speech lang. proc. vol. chen wang d.l. wang \"noise perturbation supervised speech separation\" speech comm. vol. chen wang s.e. yoho d.l. wang e.w. healy \"large-scale training increase speech intelligibility hearing-imparied listeners novel noises\" acoust. soc. vol. cheveigne \"multiple estimation\" computational auditory scene analysis principles algorithms applications d.l. wang g.j. brown hoboken wiley ieee press boomerang y.-h. \"the ustc-iflyteck system chime challenge\" proceedings chime- workshop l.-r. c.-h. regression approach single-channel speech separation highresolution deep neural networks\" ieee/acm trans. audio speech lang. proc. vol. l.-r. c.-h. \"speech separation target speaker based deep neural networks\" proceedings icsp ephraim malah \"speech enhancement using minimum mean-square error short-time spectral amplitude estimator\" ieee trans. acoust. speech sig. process. vol. erdogan hershey watanabe roux \"phase-sensitive recognition-boosted speech separation using deep recurrent neural networks\" proceedings icassp erdogan j.r. hershey watanabe mandel j.l. roux \"improved mvdr beamforming using single-channel mask prediction networks\" proceedings interspeech graves novel connectionist system unconstrained handwriting recognition\" ieee trans. pattern anal. machine intell. vol. j.w. hall j.h. grose buss m.b. \"spondee recognition two-talker speech-shaped noise masker adults children\" hear. vol. d.l. wang classification based approach speech separation\" acoust. soc. vol. e.w. healy delfarah j.l. vasko b.l. carter d.l. wang algorithm increase intelligibility hearingimpaired listeners presence competing talker\" acoust. soc. vol. hermansky morgan \"rasta processing speech\" ieee trans. speech audio proc. vol. hershey chen roux watanabe \"deep clustering discriminative embeddings segmentation separation\" proceedings icassp hertz krogh r.g. palmer introduction theory neural computation. redwood city addisonwesley heymann drude haeb-umbach \"neural network based spectral mask estimation acoustic beamforming\" proceedings icassp higuchi yoshioka \"robust mvdr beamforming using time-frequency masks online/offline noise\" proceedings icassp p.-s. huang hasegawa-johnson smaragdis \"joint optimization masks deep recurrent neural networks monaural source separation\" ieee/acm trans. audio speech lang. proc. vol. p.c. loizou algorithm improves speech intelligibility noise normal-hearing listeners\" acoust. soc. vol. smaragdis \"adaptive denoising autoencoders fine-tuning scheme learn test mixtures\" proceedings lva/ica kolbak z.h. jensen \"speech intelligibility potential general specialized deep neural network based speech enhancement systems\" ieee/acm trans. audio speech lang. proc. vol. kolbak z.-h. jensen \"multi-talker speech separation utternance-level permutation invariant training deep recurrent neural networks ieee/acm trans. audio speech lang. proc. press kulmer mowlaee \"phase estimation single channel speech enhancement using phase decomposition\" ieee sig. proc. lett. vol. y.-s. c.-y. yang s.-f. wang j.-c. wang c.-h. \"fully complex deep neural network phaseincorporating monaural source separation\" proceedings icassp p.c. loizou \"factors influencing intelligibility ideal binary-masked noise reduction\" acoust. soc. vol. j.c.r. licklider duplex theory pitch perception\" experientia vol. meng watanabe hershey erdogan \"deep long short-term memory adaptive beamforming networks multichannel robust speech recognition\" proceedings icassp papadopoulos tsiartas narayanan \"long-term estimation speech signals known unknown channel conditions\" ieee/acm trans. audio speech lang. proc. vol. d.e. rumelhart g.e. hinton r.j. williams \"learning internal representations error propagation\" parallel distributed processing d.e. rumelhart j.l. mcclelland cambridge press m.r. schadler b.t. meyer kollmeier \"spectrotemporal modulation subspace-spanning filter bank features robust automatic speech recognition\" acoust. soc. vol. l.-r. c.-h. \"multiple-target deep learning lstm-rnn based speech enhancement\" proceedings workshop hands-free speech communication microphone arrays sundermeyer schluter \"from feedforward recurrent lstm neural networks language modeling\" ieee/acm trans. audio speech lang. proc. vol. c.h. taal r.c. hendriks heusdens jensen algorithm intelligibility prediction time-frequency weighted noisy speech\" ieee trans. audio speech lang. proc. vol. tabri k.m. chacra pring \"speech perception noise monolingual bilingual trilingual listeners\" international journal language communication disorders vol. l.-r. c.-h. \"speech separation based improved deep neural networks dual outputs speech features target interfering speakers\" proceedings iscslp virtanen j.f. gemmeke \"active-set newton algorithm overcomplete non-negative representations audio\" ieee/acm trans. audio speech lang. proc. vol. xiao \"speech dereverberation enhancement recognition using dynamic features constrained deep neural networks feature adaptation\" eurasip adv. sig. proc. vol. wang \"localization based stereo speech source separation using probabilistic time-frequency masking deep neural networks\" eurasip audio speech music proc. vol. k.-h. h.-c. wang \"robust features noisy speech recognition based temporal trajectory filtering short-time autocorrelation sequences\" speech comm. vol. x.-l. zhang d.l. wang \"boosting contextual information deep neural network based voice activity detection\" ieee/acm trans. audio speech lang. proc. vol. zhang z.-q. wang d.l. wang speech enhancement algorithm iterating singlemultimicrophone processing application robust asr\" proceedings icassp d.l. wang kjems m.s. pedersen j.b. boldt lunner \"speech intelligibility background noise ideal binary time-frequency masking\" acoust. soc. vol. wang d.l. wang \"exploring monaural features classification-based speech segregation\" ieee trans. audio speech lang. proc. vol. wang narayanan d.l. wang training targets supervised speech separation\" ieee/acm trans. audio speech lang. proc. vol. p.j. werbos \"backpropagation time proc. ieee vol. d.s. williamson wang d.l. wang \"complex ratio masking monaural speech separation\" ieee/acm trans. audio speech lang. proc. vol.", "year": 2017}