{"title": "Polynomial Neural Networks Learnt to Classify EEG Signals", "tag": ["cs.NE", "cs.AI"], "abstract": "A neural network based technique is presented, which is able to successfully extract polynomial classification rules from labeled electroencephalogram (EEG) signals. To represent the classification rules in an analytical form, we use the polynomial neural networks trained by a modified Group Method of Data Handling (GMDH). The classification rules were extracted from clinical EEG data that were recorded from an Alzheimer patient and the sudden death risk patients. The third data is EEG recordings that include the normal and artifact segments. These EEG data were visually identified by medical experts. The extracted polynomial rules verified on the testing EEG data allow to correctly classify 72% of the risk group patients and 96.5% of the segments. These rules performs slightly better than standard feedforward neural networks.", "text": "nimia-sc nato advanced study institute neural networks instrumentation measurement related industrial applications study cases crema italy october abstract neural network based technique presented able successfully extract polynomial classification rules labeled electroencephalogram signals. represent classification rules analytical form polynomial neural networks trained modified group method data handling classification rules extracted clinical data recorded alzheimer patient sudden death risk patients. third data recordings include normal artifact segments. data visually identified medical experts. extracted polynomial rules verified testing data allow correctly classify risk group patients segments. rules performs slightly better standard feedforward neural networks. neural network based techniques applied electroencephalogram signals classify mental tasks provide clinical interpretation general eegs non-stationary signals vary patient large range amplitudes frequency bands. moreover eegs distorted artifacts noise recording. reasons medical experts cannot objectively interpret recordings. learn classification rules data used unsupervised well supervised learning. first uses kohonen’s neural networks second feed-forward networks trained back-propagation algorithm. improve medical diagnostics authors suggested identify type corruptions. features characterize ocular muscle artifacts include spectral shapes. extraction features carried using techniques parametric modeling cross correlation. three multi-layer neural networks subsequently features perform blink movement muscle detection respectively. detector trained real data artifacts. final stage development classification enhanced incorporating heuristic criteria spatial distribution electrodes scalp. heuristics implemented rules rule-based system. performance system tested segments test data experts preclassified. segments correctly classified. cluster analysis data also performed feed-forward neural network includes input neurons output neurons sigmoid activation function learning algorithm suggested maximizes euclidean distance output vectors belonging different classes. visualizing outputs correlation clusters output data risk groups found. authors found sub-clusters belong different classes strongly overlapped. order find classification rule used neural network input nodes corresponding relevant features. applying pre-processing technique authors finally selected significant features initial features. conclude firstly sub-clusters overlapped medical experts cannot certainly assign patients risk groups. secondly neural network classifiers must trained representative datasets consisting well-defined patterns. system presented consists artificial neural networks developed assess dementia alzheimer type first network divides patients non-dat second estimates severity dat. eegs recorded electrodes power spectrums calculated frequency bands. additionally relative power values computed. trained neural network correctly classified non-dat patients. average error severity score fully connected neural networks learn classify eegs well. however corruptions eegs artifacts still produce misleading results. particular blink movement muscle artifacts similar brain activity characterizing wave shape frequencies. note also standard neural network techniques require preset suitable structures networks users must properly define input nodes well number hidden neurons networks. moreover users must apply training methods able prevent neural networks over-fitting which know reduces generalization ability trained networks. addition classification rules fully connected neural networks learnt data represented readable form large number connections input hidden output neurons. group method data handling based polynomial theory complex systems invented ivakhnenko gmdh require preset neural network structure allows comprehensively present classification rule concise short-term polynomials. improve generalization ability gmdh-type networks authors used genetic inductive approach exploits short-term polynomials fitness function penalize large network topology. however gmdh-type training algorithms performed well noise distortions training data distributed variables neuron weights polynomial coefficients. gmdh-type training algorithms based evolutionary principle performed following. first layer algorithm using possible combinations inputs generates first population neuron-candidates. since neuron-candidates different inputs number combinations size population first layer equal first layer outputs neuron-candidates algorithm selects population neurons best ones selection best neurons performed accordance predefined fitness function whose value depends classification accuracy neurons-candidates. predefine criterion value decreased classification accuracy neuron increased. second next layers size popula. generation tion defined number i.e. selection neurons performed. layers created criterion value decreased. fig. depicted example polynomial network consisting layers gmdh algorithm grew inputs neuron-candidates selected layers provides best depicted grew boxes. neuron classification accuracy assigns output neuron. resulting polynomial network layer network consisting neurons input nodes. network described following polynomials gauss low. presence many irrelevant features training algorithms often over-fit polynomial networks. paper consider tasks fist develop training algorithm able effectively prevent over-fitting polynomial networks real data second compare standard gmdh-type neural network techniques real data. algorithm developed train polynomial networks based projection method require hypothesize statistics noise distortions. polynomial rules extracted real data allow correctly classify risk group patients segments. rules perform slightly better standard feed-forward neural networks section describe neural network classifiers compared real data. sections describe detail training algorithm section describe real data used compare performance trained neural networks. finally sections discuss main results. number neural network approaches used learn classification rules data. describe firstly feed-forward neural network classifiers trained standard back-propagation algorithm. secondly describe gmdh-type polynomial networks. feed-forward neural networks used contain hidden layer output neuron. transfer function neurons standard sigmoid i-th input variable output neuron bias term synaptic weights neuron respectively number input variables. note neuron weights initialized random values. structure fully connected fnns know defined users. users must assign input nodes preset number hidden neurons. increasing step-by-step numbers users experimentally find best classification accuracy. experiments varied numbers trained times different initial weights. training fnns exploited fast levenbergmarquardt algorithm provided matlab. prevent over-fitting fnns used standard earlier stopping technique requires divide dataset training testing validating subsets. contrast fnns gmdh-type neural networks need preset structures comprehensively described concise polynomials gmdh-type networks multi-layered ones consisted selecting best neurons gmdh uses exterior criterion calculated unseen examples used fitting weights neurons. unseen examples reserved dividing dataset nono) intersecting subsets note produce effective evaluations weights distortions data distributed gauss law. assume found desirable weight vector minimizes error subset neuroncandidates layer calculate values exterior criterion subset used weights calculated value depends behavior i-th neuron-candidate unseen examples subset therefore expect value calculated entire high neurons small generalization ability. values calculated r-th layer arranged ascending order first neurons best. layer defined minimal value corresponding best neuron i.e. cri. first best neurons used next layer. outputs selected neurons accordance feed neuron-candidates layer. training selection neurons layer performed equations repeated. step-by-step decreased value number layers increased network grows. once value reaches minimal point starts increase examples fig. fig. value decreased three layers polynomial network i.e. layer value becomes minimal. next layer value increased therefore accordance exterior criterion polynomial network over-fitted. minimum reached previous layer stop training algorithm conclude desirable polynomial network grown layer. realize selection procedure dataset beforehand divided least subsets. first used train neuron weights second evaluate classification accuracy neuron. thus value selection criterion depends behavior neuron examples included training subset. kind criteria called exterior allows prevent gmdhtype networks over-fitting transfer function number neurons selected well selection criterion predefined users. setting parameters users experimentally search best polynomial network. next section discuss detail gmdh-type algorithms used training polynomial networks. section firstly describe idea gmdh-type neural networks. networks evolved training would also gmdh-type algorithm grows networks. secondly discuss distortion real data describe modified training algorithm. matrix input data includes training examples presented features target vector calculate output neuron matrix input data ||u|| euclidian norm matrix ||u|| explain learning rule depicted fig. space weight components vector assume space desirable region inequality satisfied vector however step vector beyond ecall gmdh training algorithm assumes gauss distribution distortions data real-world data unknown distribution function distortions describe modified gmdh-type training algorithm. many real-world tasks difficult hypothesize distribution function including gauss suitable training data. example depicted fig. distribution function variable real power theta distribution function asymmetrical right tail much longer left. addition note distribution functions variables different. required hypothesize distribution function variable individually. avoid problems suggest neuron weights using recurrent algorithm based projection method describe algorithm detail. input vectors correspond training examining examples subsets respectively defined given transfer function. using notations describe basic steps algorithm following. initially algorithm initiates weight vector random values example distributed normalized gauss function. then step algorithm used eegs made healthy young person alzheimer patient opened eyes. data consist columns corresponding standard channels. sampling rate total time seconds. digital conversion measured signal done bits. band-pass filter used well. medical viewer visually cleaned recordings eliminating segments contain muscular artifacts. spatial behavior examined accumulating data various scalp locations. standard system electrode placement used provide simultaneous measurements. channel calculated spectral power density value four standard frequency bands delta theta alpha beta thus number input features equal following divided -second recording second segments overlap second. thus segment includes samples eeg. first seconds recording assigned training rest seconds testing set. therefore training testing sets used consist labeled segments. recordings made standard channels sample rate recordings patients assigned medical expert three clinical groups healthy patients patients frequent apnea patients frequent pathological apnea. following power spectral densities calculated -second segments frequency bands sub-delta delta theta alpha beta beta addition relative absolute power values well amplitude variances calculated. eegviewer removed data artifact segments. thus features. features enhanced clinical features. using features composed training testing sets consisting patients belonging risk groups relevant features performed standard principle component analysis matlab provides. produced principle components contribute variance training dataset. first second components calculated training dataset depicted fig. obviously value proportional distance given vector region next vector updated rule orthogonal projection vector hyperplane located vector region orthogonal projection denoted vector closer previous since vector closer desirable region previous value conclude obtained different learning curves depicted fig. decreased maximal speed learning rate cases level noise data unknown instead stopping rule preset constant defines minimal decrement calculated step respectively. goal training algorithm achieved difference step less thus steps algorithm provides desired weight vector correspondingly value fitness function given experiments best performance obtained case number usually exceed steps example fig. additionally varied configurations subsets well number order experimentally find minimal rse. contrast case normal artifact examples depicted respectively circles asterisks slightly overlapped. therefore classification segments expected accurate previous case. standard calculated alzheimer data characterized features produced principle components. features used input nodes fnn. best fnns trained back-propagation algorithm found experimentally includes hidden neurons. polynomial neural network trained algorithm learnt classification rule described polynomials beta inputs normalized. polynomial rule includes features selected input variables. fig. depicts appropriate structure classification rule. consists input nodes neurons whose transfer function described experiment best grown gmdh algorithm selects neuron layer i.e. general case grown simplest polynomial networks. first layer algorithm forms input vectors described however next layers input vectors risk group examples signed circles asterisks substantially overlapped plane first principle components. impossible find classification rule could correctly distinguish labeled examples used recordings made patients train neural-network automatically recognize artifact segments. expert using information additional channels reflect respiratory control muscular activity blinks visually detected artifact segments. expert labeled artifacts recordings. experiment training testing sets consisted examples respectively. following example represented input features calculated -second intervals. training testing sets included artifacts respectively. fig. depicts distribution artifacts normal segments space principle components. found best made errors training misclassified testing examples. appropriate classification rule learnt training data described following polynomials absolute power sub-delta absolute power sub-delta real power alpha absolute power beta absolute variance theta absolute variance sub-delta absolute variance sub-delta variables normalized. note extracted polynomial rule uses input variables. fig. depicts appropriate structure trained consists input nodes neurons whose transfer function described features characterizing sudden death risk data produced components contribute variance data accordingly trained fnns input nodes. number hidden neurons varied found best contains hidden neurons. misclassified testing examples standard gmdh-type neural network grown inputs respectively. note experiments gmdh produced over-fitted polynomial networks. table depicted errors training testing datasets three types trained neural networks. note denoted polynomial network trained algorithm. artifact data described features produced principle components contribute variance data correspondingly trained fnns input nodes. number hidden neurons varied best trained hidden neurons misclassified testing examples. gmdh-type network grown inputs respectively. table depicts errors trained fnns gmdh-type neural networks pnns training testing sets. note table depicted errors neural networks best training set. example anderson devulapalli stolz. signal classification different signal representations. girosi makhoul manolakos wilson neural networks signal processing ieee service center piscataway galicki witte dörschel doering eiselt grießbach. common optimization adaptive preprocessing units neural network learning period. application pattern recognition. neural networks riddington ifeachor allen hudson mapps. fuzzy expert system interpretation. e.c. ifeachor k.g. rosén proc. conference neural networks expert systems medicine healthcare nnesmed' plymouth england university plymouth james kobayashi gotman. seizure detection self-organizing feature map. malmgren borga niklasson proc. international conference artificial neural network medicine biology annimab’ goteborg sweden springer-verlag n.l. nikolaev iba. automated discovery polynomials inductive genetic programming. j.m. zutkow ranch principles data mining knowledge discovery third european conference pkdd’ lnai- springer berlin common feature fully connected feed-forward neural networks trained real-world data network structures must well predefined. reason users experimentally search best structures neural networks include minimal number input nodes hidden neurons. contrast fnns gmdh-type neural networks need predefine structures training algorithm grows appropriate neural network. resulting network described concise short-term polynomials. however gmdh-type training algorithms cannot effectively neuron weights real data distorted non-gauss noise. result polynomial networks often over-fitted. training algorithm developed able effectively neuron weights real data. algorithm require hypothesize structure noise distorts training data. experiments used three types real data recordings alzheimer healthy patients data sudden death risk patients recordings including normal artifact segments. data medical experts manually cleared labeled used compare classification accuracy trained fnns gmdh-type networks. first data three networks accuracy. complex case polynomial classification rules algorithm extracted performed slightly better standard fnns gmdh-type networks. extracted rules correctly classify risk group patients segments testing datasets. thus conclude neural network based technique developed able successfully extract polynomial classification rules data. hope technique also applied real-world problems.", "year": 2005}