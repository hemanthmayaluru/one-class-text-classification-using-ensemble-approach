{"title": "Interpreting DNN output layer activations: A strategy to cope with  unseen data in speech recognition", "tag": "eess", "abstract": " Unseen data can degrade performance of deep neural net acoustic models. To cope with unseen data, adaptation techniques are deployed. For unlabeled unseen data, one must generate some hypothesis given an existing model, which is used as the label for model adaptation. However, assessing the goodness of the hypothesis can be difficult, and an erroneous hypothesis can lead to poorly trained models. In such cases, a strategy to select data having reliable hypothesis can ensure better model adaptation. This work proposes a data-selection strategy for DNN model adaptation, where DNN output layer activations are used to ascertain the goodness of a generated hypothesis. In a DNN acoustic model, the output layer activations are used to generate target class probabilities. Under unseen data conditions, the difference between the most probable target and the next most probable target is decreased compared to the same for seen data, indicating that the model may be uncertain while generating its hypothesis. This work proposes a strategy to assess a model's performance by analyzing the output layer activations by using a distance measure between the most likely target and the next most likely target, which is used for data selection for performing unsupervised adaptation. ", "text": "generally used expose acoustic model wider range background acoustic variations data augmentation expose model anticipated acoustic variations; reality acoustic variations difficult anticipate. real-world applications encounter diverse acoustic conditions mostly unique hence difficult anticipate. systems trained several thousands hours data collected different realistic conditions typically found quite robust background conditions expected contain many variations; however data contain possible variations found world. recently several open speech recognition evaluations shown vulnerable acoustic models realistic varying unseen acoustic conditions. celebrated resource-constrained approaches coping unseen data conditions performing unsupervised adaptation necessity data. reliable adaptation technique supervised adaptation assumes annotated target-domain data; however annotated data often unavailable real-world scenarios. constraint often makes unsupervised adaptation practical. unsupervised speaker adaptation dnns explored adaptation based maximum likelihood linear regression transforms ivectors etc. showing impressive performance gains un-adapted models. kullback-leibler divergence based regularization proposed model parameter adaptation. feature-space mllr transform found improve acoustic model performance mismatched cases confidence adaptation demonstrated improvements recognition performance wall street journal verbmobil speech recognition tasks. semi-supervised acoustic model training investigated trained small dataset adapted larger data leveraging data selection using confidence measure. work focus understanding acousticcondition mismatch training testing data impacts output decision. similar efforts pursued researchers earlier investigated entropy measure ascertain level uncertainty translate measure quantify decision reliability. paper focuses data mismatch impacts output layer activations proposes measure predicts dnnâ€™s decision less accurate. proposed approach relies fact seen conditions likely nseen data degrade performance deep neural acoustic models. cope unseen data adaptation techniques deployed. unlabeled unseen data must generate hypothesis given existing model used label model adaptation. however assessing goodness hypothesis difficult erroneous hypothesis lead poorly trained models. cases strategy select data reliable hypothesis ensure better model adaptation. work proposes data-selection strategy model adaptation output layer activations used ascertain goodness generated hypothesis. acoustic model output layer activations used generate target class probabilities. unseen data conditions difference probable target next probable target decreased compared seen data indicating model uncertain generating hypothesis. work proposes strategy assess modelâ€™s performance analyzing output layer activations using distance measure likely target next likely target used data selection performing unsupervised adaptation. ndex termsâ€”automatic speech recognition robust speech recognition unsupervised adaptation output layer activations deep neural networks confidence measures. learning technologies revolutionized automatic speech recognition systems demonstrating impressive performance almost tried languages. interestingly deep neural network -based systems data hungry data sensitive performance model found improve additional diverse training data. unfortunately annotated training data expensive. although large volumes data becoming available every properly transcribed reflective varying acoustic conditions systems expected tackle. limited data conditions acoustic models quite sensitive acoustic-condition mismatches subtle variation background acoustic conditions significantly degrade recognition performance. cope problem unseen data multicondition training accompanied data augmentation ___________________________________________________________ *the author performed work international currently working apple inc. used gammatone filterbank energies acoustic features experiments. gfbs generated using bank gammatone filters equally spaced equivalent rectangular bandwidth scale. analysis window frame rate gammatone subband powers dynamic-range compressed using root. gfbs used experiment robustness background distortions compared mel-scale features case unknown acoustic variations dnn-based acoustic models fail generalize well consequence propagate distortion input feature space resulting distorted outputs represent relevant aspects input grossly mismatched situations detecting cases cause system completely fail versus generate reasonable output quite useful. generate detection confidence measure generally indicative trustworthy hypothesis test files. fully connected network interpreted cascade several feature-transformation steps goal making target class discriminative possible respect other. hence cases model fails generate reasonable performance transformations fail generate reliable features therefore model decision impacted. expected model decision impaired model uncertain decision thus multiple output activations generating similar posterior probabilities. natural indicator close neural activation producing maximum value respect activations producing second third maxima respectively. case distance likely target next likely target less model expected uncertain making decision relative model greater distance values. note distance measure absolute measure work pose relative measure based distances obtained training set. name distance measure confusion distance figure shows distribution training unseen dataset. assume target classes indicating neurons output layer generating activations given instant time neuron output layer. vector time instant define vector obtained sorting elements targetâ€™s probability substantially higher next likely targetâ€™s probability whereas unseen conditions difference target probabilities large happens consequence uncertain making decision unseen condition. similar observation impact unseen data winning neuronâ€™s activation respect next best activation cited work output layer neural activations compute distance measure likely target likely targets respectively. name measure confusion distance show higher seen data compared unseen data. compute averaged distance measure utterance select data unsupervised adaptation. note proposed strategy restricted speech recognition used applications involve probabilistic processing. acoustic models work trained using multi-conditioned noisechannel-degraded training data aurora- noisy corpus. aurora- contains total additive noise types channel-matched mismatched conditions. created standard database contains training utterances approximately -hours duration test utterances. test data includes test sets different channel conditions different added noises. signal-to-noise ratio test sets varied audio data test sets recorded sennheiser microphone test sets recorded using second microphone randomly selected different microphones. results test sets presented follows clean matched-channel noisy matched-channel clean varying-channels noisy varying-channels treated reverberation unseen data condition experiments trained models using aurora- corpus evaluated performance realworld reverberated data. adaptation optimization evaluation purposes used training development evaluation sets distributed reverb- challenge. reverb- dataset contains singlespeaker utterances single-microphone part dataset used experiments reported paper. reverb- training consists clean wsjcam data convolved room impulse responses corrupted background noise. note reverb- training used unsupervised adaptation transcriptions used experiments. evaluation development data contain real recordings simulated data real data borrowed mcwsj-av corpus consists utterances recorded noisy reverberant room. simulated evaluation perform time convolution filters perform frequency frequency convolution eight bands used followed maxpooling five three samples respectively. feature maps convolution operations concatenated fully connected -hidden layer neural containing neurons. baseline acoustic model trained aurora- multi-condition training dataset held-out cross-validation used train tfcnn acoustic models. reverberated acoustic condition treated unseen data condition experiments experimental analysis performed using development test data reverb- challenge dataset. baseline unsupervised adapted system used hypothesis whole adaptation adapt tfcnnbaseline model. note adaptation unsupervised adaptation dataset used addition original aurora- training dataset update acoustic model parameters. adaptation model parameters updated norm initial learning rate learning rate halved every iteration adaptation set. early stopping performed based cross-validation error. tables show word error rate obtained baseline model unsupervised adapted baseline model table show performance degradation occurred noisy condition model initially trained degradation substantial consequence adding original training part adaptation set. tables show using entire adaptation improved modelâ€™s performance unseen reverberation condition test sets reverb- reducing point question remains adaptation first term determines average hypothesis second term determines average competing hypothesis time instant utterance consisting frames overall averaged measure computed taking mean estimated frames. ğ¶ğ·ğ‘ğ‘£ğ‘” work cdavg estimated file training unseen dataset cdavg computed training unsupervised adaptation denoted ğ‘¢ğ‘›ğ‘ ğ‘¢ğ‘_ğ‘ğ‘‘ğ‘ğ‘ğ‘¡ respectively. data selection ğ¶ğ·ğ‘ğ‘£ğ‘” unsupervised adaptation performed ğ‘¢ğ‘›ğ‘ ğ‘¢ğ‘_ğ‘ğ‘‘ğ‘ğ‘ğ‘¡ threshold thresholding ğ¶ğ·ğ‘ğ‘£ğ‘” determined ğ¶ğ·ğ‘ğ‘£ğ‘” figure distribution estimated output layer acoustic model. green estimated training data blue estimated unseen data vertical dotted lines indicate respective means. work used time-frequency acoustic models based reliable performance aurora- speech recognition task. generate alignments necessary training acoustic model gaussian mixture model -hmm model used produce senone labels. altogether gmm-hmm system produced context-dependent states aurora- training data. input features acoustic models formed using context window frames acoustic models trained using cross-entropy alignments gmm-hmm system. -layered neurons layer trained using alignments gmm-hmm system turn used generate alignments training subsequent tfcnn acoustic model used paper. tfcnn acoustic models input acoustic features formed using context window frames. tfcnns filters table shows data selection followed model adaptation resulted better performance using entire adaptation data relative reduction respectively obtained data selection compared using whole data. indicates data-selection process helps filter hypotheses used adaptation. table shows unsupervised adaptation using data selection helped reduce relative improvement simulated real test data. respectively. substantial improvement simulated reverberation condition extent expected adaptation used case reverb- training consists simulated reverberation only; hence helped model learn condition real reverberation condition. work investigated using output layer activations predict reliability neural netâ€™s decision using information perform data selection unsupervised model adaptation. proposed metric confusion distance used perform data selection performing unsupervised adaptation. lower reflects confusion neural hypothesis stemming reduced distance winning target next probable target. used data resulted higher values model adaptation demonstrated filtering data hypotheses resulted relative improvement work used summary measure utterance; however measure also obtained individual frame level providing framelevel confusion information. future studies explore using framelevel confidence measure selecting data segments performing unsupervised adaptation. material based upon work partly supported defense advanced research projects agency contract hr--c-. views opinions and/or findings contained article authors interpreted representing official views policies department defense u.s. government. step suffered inaccurate hypothesis generated adaptation data. inaccurate hypothesis filtered performing data selection using ğ‘¢ğ‘›ğ‘ ğ‘¢ğ‘_ğ‘ğ‘‘ğ‘ğ‘ğ‘¡. initial experiment assess values ğ¶ğ·ğ‘ğ‘£ğ‘” rank-sorted ğ‘¢ğ‘›ğ‘ ğ‘¢ğ‘_ğ‘ğ‘‘ğ‘ğ‘ğ‘¡ values estimated adaptation ğ¶ğ·ğ‘ğ‘£ğ‘” selected files performing adaptation results shown table adapted model data selection represented tfcnnuv_ds subscript stands data selection. able shows selecting gave best adaptation performance point tfcnnuv_ds acoustic model outperforms tfcnnuv model. finally explored data selection using threshold learned training list. ï­train_cd train_cd mean ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› data. select variance computed ğ¶ğ·ğ‘ğ‘£ğ‘” data data using ğ‘¢ğ‘›ğ‘ ğ‘¢ğ‘_ğ‘ğ‘‘ğ‘ğ‘ğ‘¡ selected performing ğ¶ğ·ğ‘ğ‘£ğ‘” unsupervised model adaptation. table presents results adaptation using data selected different thresholds evaluated reverb- set. table indicates optimal threshold ï­train_cd-ï³train_cd. tables present wers baseline models adapted models aurora- reverb eval. sets. veselÃ½ hannemann burget \"semisupervised training deep neural networks\" ieee workshop understanding hermansky burget cohen dupoux feldman godfrey khudanpur maciejewski s.h. mallidi menon ogawa peddinti rose stern wiesner veselÃ½ towards machines know know summary work done frederick jelinek memorial workshop proc. icassp zaragoza dâ€™a-b. florence confidence measures neural network classifiers proc. int. conf. information processing management uncertainty knowledge based systems mitra franco leveraging deep neural network activation entropy cope unseen data speech recognition arxiv preprint arxiv.. moosavi computing contextual numbers arxiv. hirsch experimental framework performance evaluation speech recognition front-ends large vocabulary task robinson fransen foote renals wsjcam british english speech corpus large vocabulary continuous speech recognition proc. icassp lincoln mccowan vepa h.k. maganti multi-channel wall street journal audio visual corpus specification initial experiments proc. ieee workshop automatic speech recognition understanding mitra franco time-frequency convolution networks robust speech recognition proc. asru mitra franco coping unseen data conditions investigating neural architectures robust features robust speech recognition proc. interspeech mitra wang franco bartels graciarena. evaluating robust features deep neural networks speech recognition noisy channel mismatched conditions proc. interspeech mohamed g.e. dahl hinton acoustic modeling using deep belief networks ieee trans. aslp vol. seide conversational speech transcription using context-dependent deep neural networks proc. interspeech grÃ©zl egorova karafiÃ¡t further investigation multilingual training adaptation stacked bottle-neck neural network structure proc. karafiÃ¡t grÃ©zl burget szÃ¶ke cernockÃ½ three ways adapt recognizer unseen reverberated speech system aspire challenge proc. interspeech bell m.j.f. gales hain kilgour lanchantin mcparland renals wester p.c. woodland challenge evaluating multi-genre broadcast media recognition proc. asru barker marxer vincent watanabe third â€˜chimeâ€™ speech separation recognition challenge dataset task baselines proc. asru harper automatic speech recognition reverberant environments challenge proc. asru kinoshita delcroix yoshioka nakatani habets haeb-umbach leutnant sehr kellermann maas gannot reverb challenge dereverberation recognition reverberant speech proc. ieee workshop applications signal processing audio acoustics yoshioka ragni m.j.f. gales investigation unsupervised adaptation acoustic models filterbank input proc. icassp saon soltau nahamoo picheny speaker adaptation neural network acoustic models using i-vectors proc. asru s.h.k. parthasarathi hoffmeister matsoukas mandal strom garimella fmllr based featurespace speaker adaptation acoustic models proc. interspeech gupta kenny ouellet i-vector-based speaker adaptation deep neural networks french broadcast audio transcription proc. icassp seide kl-divergence regularized deep neural network adaptation improved large vocabulary speech recognition proc. icassp seide chen \"feature engineering context-dependent deep neural networks conversational speech transcription\" proc asru anastasakos s.v. balakrishnan confidence measures unsupervised adaptation speech recognizers proc. int. conf. spoken language processing sydney australia pitz wessel improved mllr speaker adaptation using confidence measures conversational speech recognition proc. int. conf. spoken language processing beijing china", "year": "2018"}